{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "24.Neural Networks2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO3q3AJVim5d/urN3CNxO45",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "874c85315f9345e3820fe4de2fbf77bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f75e5513b4984d72a317c2bdb588e37c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8dfe4e66c8ad4cdb99a19c6f821e7760",
              "IPY_MODEL_d583c30c5977496f93a607c5c1e74f5e",
              "IPY_MODEL_fe8fd10cf73e48cd901d64a86ccf407d"
            ]
          }
        },
        "f75e5513b4984d72a317c2bdb588e37c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8dfe4e66c8ad4cdb99a19c6f821e7760": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_79043ce4d6bb4311bda4be7d1a05c11c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_57c6b17770354d16b8fb813d7cd1bfdf"
          }
        },
        "d583c30c5977496f93a607c5c1e74f5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_843aa3280c9448028f56285ab4a8df48",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 10,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 10,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5c7b4e55e7c14f0aa57b1f7798d1dd59"
          }
        },
        "fe8fd10cf73e48cd901d64a86ccf407d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_28b7519abff44d66ae75b00ce4301c6b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 10/10 [00:10&lt;00:00,  1.00s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_25801e01a4be4b28adcd98686997c0fd"
          }
        },
        "79043ce4d6bb4311bda4be7d1a05c11c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "57c6b17770354d16b8fb813d7cd1bfdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "843aa3280c9448028f56285ab4a8df48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5c7b4e55e7c14f0aa57b1f7798d1dd59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "28b7519abff44d66ae75b00ce4301c6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "25801e01a4be4b28adcd98686997c0fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yexf308/MAT592/blob/main/24_Neural_Networks2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uwk8QJauj1EH"
      },
      "source": [
        "Part of the notebook is based on \"Neural Networks and Deep Learning\" and Prof Guangliang Chen's notes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGEppQkFaVSx",
        "outputId": "82a12432-ef76-4ffc-f2ee-c955cfe3769f"
      },
      "source": [
        "%pylab inline \n",
        "import numpy.linalg as LA\n",
        "from IPython.display import Image"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gin4mWZdcYOg"
      },
      "source": [
        "## Tip of the day - Progress Bar\n",
        "\n",
        "When running a long calculation, we would usually want to have a progress bar to track the progress of our process. One great python package for creating such a progress bar is [**tqdm**](https://github.com/tqdm/tqdm). This package is easy to use and offers a highly customizable progress bar. \n",
        "\n",
        "For example, to add a progress bar to an existing loop, simply surrounding the iterable which the loops run over with the **tqdm_notebook** command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219,
          "referenced_widgets": [
            "874c85315f9345e3820fe4de2fbf77bf",
            "f75e5513b4984d72a317c2bdb588e37c",
            "8dfe4e66c8ad4cdb99a19c6f821e7760",
            "d583c30c5977496f93a607c5c1e74f5e",
            "fe8fd10cf73e48cd901d64a86ccf407d",
            "79043ce4d6bb4311bda4be7d1a05c11c",
            "57c6b17770354d16b8fb813d7cd1bfdf",
            "843aa3280c9448028f56285ab4a8df48",
            "5c7b4e55e7c14f0aa57b1f7798d1dd59",
            "28b7519abff44d66ae75b00ce4301c6b",
            "25801e01a4be4b28adcd98686997c0fd"
          ]
        },
        "id": "oqBzrhpRb8QM",
        "outputId": "17ef3b8b-0927-48b5-d552-4218cd9bf3c5"
      },
      "source": [
        "import tqdm\n",
        "import time\n",
        "for i in tqdm.notebook.tqdm(range(10)):\n",
        "    print('Step {}'.format(i))\n",
        "    time.sleep(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "874c85315f9345e3820fe4de2fbf77bf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0\n",
            "Step 1\n",
            "Step 2\n",
            "Step 3\n",
            "Step 4\n",
            "Step 5\n",
            "Step 6\n",
            "Step 7\n",
            "Step 8\n",
            "Step 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWhO7QvyfwIj"
      },
      "source": [
        "# How do we train a neural network?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "4dD1j9WojrvD",
        "outputId": "34350f52-1f43-46d8-8047-af6a39eb3879"
      },
      "source": [
        "display(Image(url='https://github.com/yexf308/MAT592/blob/main/image/DNN_notation.png?raw=true', width=500))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/DNN_notation.png?raw=true\" width=\"500\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5-njcGSEuBf"
      },
      "source": [
        "Here we will use the **sigmoid function** $\\sigma$ and **logistic loss** in the output layer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7FUOoLCSaDO"
      },
      "source": [
        "### Notation\n",
        "For each $l = 1,\\dots, L$:   \n",
        "\n",
        "- $w_{jk}^{(l)}$: layer $l$, $j$ back to $k$ weight. \n",
        "  - $\\mathbf{W}^{(l)}=\\left(w_{jk}^{(l)}\\right)_{j,k}$, matrix of all weights between layers $l-1$ and $l$. \n",
        "\n",
        "- $b_j^{(l)}$: layer $l$, neuron $j$ bias. \n",
        "  - $\\mathbf{b}^{(l)}=\\left(b_j^{(l)}\\right)_j$: vector of biases in layer $l$. \n",
        "\n",
        "- $a_j^{(l)}$: layer $l$, neuron $j$ output.\n",
        "   - $\\mathbf{a}^{(l)}=\\left(a_j^{(l)}\\right)_j$: vector of outputs from neurons in layer $l$. \n",
        "\n",
        "- $z_j^{(l)}=\\sum_k w_{jk}^{(l)} a_k^{(l-1)}+b_j^{(l)}$: weighted input to neuron $j$ in layer $l$.\n",
        "   - $\\mathbf{z}^{(l)}=\\left(z_j^{(l)}\\right)_j$vector of weighted inputs to neurons in layer $l$. \n",
        "\n",
        "- $a_j^{(l)}=g(z_j^{(l)})$ and $\\mathbf{a}^{(l)}=g(\\mathbf{z}^{(l)})$ (componentwise), where $g$ is the activation function. \n",
        "\n",
        "- $\\theta = \\{\\mathbf{W}^{(l)}, \\mathbf{b}^{(l)}\\}_{l=1}^L$ are parameters. The whole dataset $\\mathcal{D}=\\{\\mathbf{x}^{(i)},y^{(i)}\\}_{i=1}^N$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqV2UwT7DVbs"
      },
      "source": [
        "### Forward propagation\n",
        "- The input layer is indexed by $l=0$ so that $\\mathbf{a}^{(0)}=\\mathbf{x}$. The output layer is defined as $\\mathbf{a}^{(L)}(\\mathbf{x}) = \\vec f(\\mathbf{x}; \\theta)$. \n",
        "\n",
        "- For each $1\\le l \\le L$, \n",
        "\\begin{align}\n",
        "&\\mathbf{z}^{(l)}=\\mathbf{W}^{(l)}\\mathbf{a}^{(l-1)}+\\mathbf{b}^{(l)} \\\\\n",
        "&\\mathbf{a}^{(l)}=g(\\mathbf{z}^{(l)})\n",
        "\\end{align}\n",
        "\n",
        "- At each layer, we first apply linear transformation and then apply sigmoid function componentwise. So $\\mathbf{a}^{(L)}(\\mathbf{x})$ is ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "e9kYnvEHGiTp",
        "outputId": "e9ccb41c-b90f-4053-fc99-02ab76e35b60"
      },
      "source": [
        "display(Image(url='https://github.com/yexf308/MAT592/blob/main/image/ANN_forward.png?raw=true', width=500))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/ANN_forward.png?raw=true\" width=\"500\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lizf8c98IYTT"
      },
      "source": [
        "### Loss function\n",
        "\n",
        "To tune the weights and biases of a network of sigmoid neurons, we need to select a loss function.\n",
        "\n",
        "For simplicity, we first consider the square loss\n",
        "$$L(\\theta; \\mathcal{D}) =\\frac{1}{2N}\\sum_{i=1}^N \\ell(\\vec f(\\mathbf{x}^{(i)};\\theta),y^{(i)})=\\frac{1}{2N}\\sum_{i=1}^N\\|\\mathbf{a}^{(L)}(\\mathbf{x}^{(i)})-\\mathbf{y}^{(i)}\\|^2 $$\n",
        "\n",
        "Here $\\mathbf{y}^{(i)}$ is the one-hot vector of the training label $y^{(i)}$, i.e. in MNIST dataset, the labels are coded as follows:\n",
        "$$\\text{digit 0} = \\begin{bmatrix}1 \\\\0 \\\\ \\vdots \\\\0 \\end{bmatrix}, \\text{digit 1} = \\begin{bmatrix}0 \\\\1 \\\\ \\vdots \\\\0 \\end{bmatrix}, \\dots,\\text{digit 9} = \\begin{bmatrix}0 \\\\0 \\\\ \\vdots \\\\1 \\end{bmatrix}$$\n",
        "\n",
        "Therefore, by varying the weights and biases, we try to minimize the difference between each network output $\\mathbf{a}^{(L)}(\\mathbf{x}^{(i)})$ and one of the vectors above (associated\n",
        "to the training class that $\\mathbf{x}^{(i)}$ belongs to).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUWlhALRHNgP"
      },
      "source": [
        "## The backpropagation algorithm\n",
        "The goal here is to compute all the\n",
        "partial derivatives $\\frac{\\partial L(\\theta; \\mathcal{D})}{\\partial \\mathbf{W}^{(l)}}, \\frac{\\partial L(\\theta; \\mathcal{D})}{\\partial \\mathbf{b}^{(l)}}$, which are non-trivial at all. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "evBs4e1AH8mu",
        "outputId": "4e26994c-ed55-4b39-9e2c-ab1881058766"
      },
      "source": [
        "display(Image(url='https://github.com/yexf308/MAT592/blob/main/image/ANN_output.png?raw=true', width=500))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/ANN_output.png?raw=true\" width=\"500\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHGW_aXsYHfz"
      },
      "source": [
        "To simplify the task a bit, we consider the sameple error for only $\\mathbf{x}^{(i)}$, $C_i\\triangleq\\ell(\\vec f(\\mathbf{x}^{(i)};\\theta),y^{(i)})=\\|\\mathbf{a}^{(L)}(\\mathbf{x}^{(i)})-\\mathbf{y}^{(i)}\\|^2 $\n",
        "\n",
        "Therefore, these derivatives are summation of each individual derivative,\n",
        "\\begin{align}\n",
        "&\\frac{\\partial L(\\theta; \\mathcal{D})}{\\partial \\mathbf{W}^{(l)}}=\\frac{1}{N}\\sum_{i=1}^N \\frac{\\partial C_i}{\\partial \\mathbf{W}^{(l)}}  \\\\\n",
        "&\\frac{\\partial L(\\theta; \\mathcal{D})}{\\partial \\mathbf{b}^{(l)}} =\\frac{1}{N}\\sum_{i=1}^N \\frac{\\partial C_i}{\\partial \\mathbf{b}^{(l)}}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UquREm4Layay"
      },
      "source": [
        "### Output layer first\n",
        "Let's start with the output layer, \n",
        "By chain rule, we have\n",
        "\\begin{align}\n",
        "\\frac{\\partial C_i}{\\partial w^{(\\color{red}L)}_{jk}} &= \\frac{\\partial C_i}{\\partial a_j^{(\\color{red}L)}}\\cdot \\frac{\\partial a_j^{(\\color{red}L)}}{\\partial z_j^{(\\color{red} L)}}\\cdot \\frac{\\partial z_j^{(\\color{red}L)}}{\\partial w^{(\\color{red}L)}_{jk}} \\\\\n",
        "&= \\left(a_j^{(L)} - y_j^{(i)}\\right) \\cdot g'(z_j^{(L)})\\cdot a_k^{(L-1)}\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial C_i}{\\partial b^{(\\color{red}L)}_{j}} &= \\frac{\\partial C_i}{\\partial a_j^{(\\color{red}L)}}\\cdot \\frac{\\partial a_j^{(\\color{red}L)}}{\\partial z_j^{(\\color{red} L)}}\\cdot \\frac{\\partial z_j^{(\\color{red}L)}}{\\partial b^{(\\color{red}L)}_{j}} \\\\\n",
        "&= \\left(a_j^{(L)} - y_j^{(i)}\\right) \\cdot g'(z_j^{(L)})\n",
        "\\end{align}\n",
        "\n",
        "where $a_j^{(L)}=g(z_j^{(L)})$ and $z_j^{(L)}=\\sum_{k'}w^{(L)}_{jk'}a_{k'}^{(L-1)}+b_j^{(L)}$. \n",
        "\n",
        "Here we want to interpret the formula above. \n",
        "The rate of change of $C_i=\\ell(\\vec f(\\mathbf{x}^{(i)};\\theta),y^{(i)})$ depends on three factors,\n",
        "\n",
        "- $\\left(a_j^{(L)} - y_j^{(i)}\\right)$, how much current\n",
        "output is off from desired output. \n",
        "\n",
        "- $g'(z_j^{(L)})$, how fast the neuron reacts\n",
        "to changes of its input.\n",
        "\n",
        "- $a_k^{(L-1)}$, contribution from neuron $k$ from $L-1$. \n",
        "\n",
        "If the activation function $g$ is the sigmoid function, even the current output is far away from the desired output, $ w^{(L)}_{jk}$ may learn slowly if the input\n",
        "neuron is in low-activation ($a_k^{(L-1)}\\approx 0$) or the output neuron has “saturated”,\n",
        "i.e., is in either high- or low-activation since in both cases $\\sigma'(z_j^{(L)})\\approx 0$.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The beauty of this algorithm is it can be vectorized. \n",
        "\\begin{align}\n",
        "&\\frac{\\partial C_i}{\\partial \\mathbf{W}^{(\\color{red}L)}} = \\left(\\underbrace{\\left(\\mathbf{a}^{(L)}-\\mathbf{y}^{(i)}\\right)}_{\\in \\mathbb{R}^{k\\times1}}\\circ \\underbrace{g'(\\mathbf{z}^{(L)})}_{\\in \\mathbb{R}^{k\\times 1}}\\right)\\cdot \\underbrace{(\\mathbf{a}^{(L-1)})^\\top}_{\\mathbb{R}^{1\\times h^{(L-1)}}} \\in \\mathbb{R}^{k\\times h^{(L-1)}} \\\\ \n",
        "& \\frac{\\partial C_i}{\\partial \\mathbf{b}^{(\\color{red}L)}} = \\underbrace{\\left(\\mathbf{a}^{(L)}-\\mathbf{y}^{(i)}\\right)}_{\\in \\mathbb{R}^{k\\times1}}\\circ \\underbrace{g'(\\mathbf{z}^{(L)})}_{\\in \\mathbb{R}^{k\\times 1}} \\in \\mathbb{R}^{k\\times 1}\n",
        "\\end{align}\n",
        "\n",
        "For convenience, define the auxiliary quantity $\\delta^{(L)}$,\n",
        "\\begin{align}\n",
        "\\delta^{(L)} \\triangleq \\left(\\mathbf{a}^{(L)}-\\mathbf{y}^{(i)}\\right)\\circ g'(\\mathbf{z}^{(L)})\n",
        "\\end{align}\n",
        "\n",
        "Then \n",
        "\\begin{align}\n",
        "\\frac{\\partial C_i}{\\partial \\mathbf{W}^{(L)}} =\\delta^{(L)} \\cdot (\\mathbf{a}^{(L-1)})^\\top\n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGaFuUBTmoir"
      },
      "source": [
        "### What about layer L − 1 (and further inside)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "U6e7C5h5lZ5J",
        "outputId": "06d628ed-1025-4f08-fa3f-3456b82e1f89"
      },
      "source": [
        "display(Image(url='https://github.com/yexf308/MAT592/blob/main/image/ANN_L_1.png?raw=true', width=500))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/ANN_L_1.png?raw=true\" width=\"500\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ye30Neq9mcVV"
      },
      "source": [
        "By chain rule again, \n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial C_i}{\\partial w_{kq}^{(L-1)}} &=\\sum_j\\frac{\\partial C_i}{\\partial a^{(L)}_j}\\cdot \\frac{\\partial a^{(L)}_j}{\\partial w_{kq}^{(L-1)}}\\\\ \n",
        "&= \\sum_j\\frac{\\partial C_i}{\\partial a^{(L)}_j}\\cdot \\frac{\\partial a^{(L)}_j}{\\partial a^{(L-1)}_k}\\cdot \\frac{\\partial a^{(L-1)}_k}{\\partial w_{kq}^{(L-1)}} \\\\ \n",
        "&= \\sum_j \\left(a_j^{(L)} - y_j^{(i)}\\right) \\cdot g'(z_j^{(L)})w_{jk}^{(L)} \\cdot g'(z_k^{(L-1)})a_q^{(L-2)}\n",
        "\\end{align}\n",
        "The middle term $\\frac{\\partial a^{(L)}_j}{\\partial a^{(L-1)}_k}$ is the link between layers $L$ and $L − 1$. \n",
        "\n",
        "---\n",
        "Similarly in vector form, \n",
        "\\begin{align}\n",
        "\\frac{\\partial C_i}{\\partial \\mathbf{W}^{(L-1)}}= \\left(\\underbrace{(\\mathbf{W}^{(L)})^\\top \\cdot\\delta^{(L)}}_{\\in \\mathbb{R}^{h^{(L-1)}\\times 1}} \\circ \\underbrace{g'(\\mathbf{z}^{(L-1)})}_{\\in \\mathbb{R}^{h^{(L-1)}\\times 1}}\\right)\\cdot \\underbrace{(\\mathbf{a}^{(L-2)})^\\top}_{\\in \\mathbb{R}^{1\\times h^{(L-2)}}} \\in \\mathbb{R}^{h^{(L-1)} \\times h^{(L-2)}}\n",
        "\\end{align}\n",
        "\n",
        "For convenience, define the auxiliary quantity $\\delta^{(L-1)}$,\n",
        "\n",
        "\\begin{align}\n",
        "\\delta^{(L-1)} \\triangleq \\left((\\mathbf{W}^{(L)})^\\top \\cdot \\delta^{(L)}\\right)\\circ g'(\\mathbf{z}^{(L-1)})\n",
        "\\end{align}\n",
        "\n",
        "Then \n",
        "\\begin{align}\n",
        "\\frac{\\partial C_i}{\\partial \\mathbf{W}^{(L-1)}} = \\delta^{(L-1)} \\cdot (\\mathbf{a}^{(L-2)})^\\top\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "A16VsXdHzsKa",
        "outputId": "e17aa34c-1dff-4b20-f265-d4dc1c946fb5"
      },
      "source": [
        "display(Image(url='https://github.com/yexf308/MAT592/blob/main/image/ANN_l.png?raw=true', width=700))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/ANN_l.png?raw=true\" width=\"700\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1edbde-lzrwt"
      },
      "source": [
        "### Layer $l$\n",
        "As we move further inside the network (from the output layer), we will need to compute more and more links between layers, \n",
        "\\begin{align}\n",
        "\\frac{\\partial C_i}{\\partial w_{qr}^{(l)}}  = \\sum_{j,k,\\dots,p}\\frac{\\partial C_i}{\\partial a^{(L)}_j}\\cdot \\frac{\\partial a^{(L)}_j}{\\partial a_{k}^{(L-1)}}\\dots \\frac{\\partial a^{(l+1)}_p}{\\partial a_{q}^{(l)}}\\frac{\\partial a^{(l)}_q}{\\partial w_{qr}^{(l)}}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "Similarly, \n",
        "we can define $\\delta^{(l)}$,\n",
        "\\begin{align}\n",
        "\\delta^{(l)}= \\left((\\mathbf{W}^{(l+1)})^\\top \\cdot \\delta^{(l+1)}\\right)\\circ g'(\\mathbf{z}^{(l)})\n",
        "\\end{align}\n",
        "so can calculate $\\delta^{(l)}$ iteratively. \n",
        "\n",
        "Similarly in vector form, \n",
        "\\begin{align}\n",
        "\\frac{\\partial C_i}{\\partial \\mathbf{W}^{(l)}}= \\delta^{(l)}\\cdot (\\mathbf{a}^{(l-1)})^\\top.\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9diOEzt3pRh"
      },
      "source": [
        "### To summerize backpropagation algorithm\n",
        "\n",
        "The products of the link terms may be computed iteratively from right to left. It leads to an efficient algorithm for computing these derivatives $\\frac{\\partial C_i}{\\partial w^{(l)}_{qr}}, \\frac{\\partial C_i}{\\partial b^{(l)}_{q}}$ at layer $l$\n",
        "\n",
        "- Forward propagation: Feedforward $\\mathbf{x}^{(i)}$ to obtain all neuron inputs and outputs,\n",
        "\n",
        "   \\begin{align}\n",
        "    &\\mathbf{a}^{(0)}=\\mathbf{x}^{(i)}, \\\\  \n",
        "    &\\mathbf{a}^{(l)}=g(\\mathbf{W}^{(l)}\\mathbf{a}^{(l-1)}+\\mathbf{b}^{(l)})  \\ \\text{for } l =1, \\dots, L\n",
        "    \\end{align}\n",
        "\n",
        "- Backpropagate the network to compute,\n",
        "\n",
        "\\begin{align}\n",
        "&\\delta^{(L)}=\\left(\\mathbf{a}^{(L)}-\\mathbf{y}^{(i)}\\right)\\circ g'(\\mathbf{z}^{(L)}) \\\\ \n",
        "&\\delta^{(l)}= \\left((\\mathbf{W}^{(l+1)})^\\top \\cdot \\delta^{(l+1)}\\right)\\circ g'(\\mathbf{z}^{(l)}), \\ \\text{for } l =L-1, \\dots, 1\n",
        "\\end{align}\n",
        "\n",
        "- Compute $\\frac{\\partial C_i}{\\partial \\mathbf{W}^{(l)}}, \\frac{\\partial C_i}{\\partial \\mathbf{b}^{(l)}}$ for every layer $l$ ,\n",
        "\\begin{align}\n",
        "&\\frac{\\partial C_i}{\\partial \\mathbf{W}^{(l)}} = \\delta^{(l)}\\cdot (\\mathbf{a}^{(l-1)})^\\top \\\\ \n",
        "& \\frac{\\partial C_i}{\\partial \\mathbf{b}^{(l)}}  = \\delta^{(l)}\n",
        "\\end{align}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9Sk4xRMWDkg"
      },
      "source": [
        "## Training with gradient based method\n",
        "### Stochastic gradient descent revisited. \n",
        "\n",
        "- Initialize all the weights $\\mathbf{W}^{(l)}$ and biases $\\mathbf{b}^{(l)}$, where $\\theta = \\left\\{\\mathbf{W}^{(l)}, \\mathbf{b}^{(l)}\\right\\}_{l=1,\\dots,L}$.\n",
        "\n",
        "- Randomly shuffle training dataset and for each training sample $(\\mathbf{x}^{(i)}, y^{(i)})$, \n",
        "   \n",
        "    - Use backpropagation to compute these gradient $\\frac{\\partial C_i}{\\partial \\theta}$.\n",
        "\n",
        "    - Update the weights and biases using learning rate $\\eta>0$ \n",
        "    $$ \\theta\\leftarrow \\theta - \\eta \\cdot \\frac{\\partial C_i}{\\partial \\theta}$$\n",
        "\n",
        "  \n",
        "  This completes one epoch in the training process.\n",
        "\n",
        "- Repeat the preceding step until convergence.  \n",
        "\n",
        "### Mini-batch \n",
        "\n",
        "- Initialize all the weights $w_{jk}^{(l)}$ and biases $b_j^{(l)}$. where $\\theta = \\left\\{\\mathbf{W}^{(l)}, \\mathbf{b}^{(l)}\\right\\}_{l=1,\\dots,L}$.\n",
        "\n",
        "- Randomly partition the training dataset $\\mathcal{D}$ into $M$ batches, $\\mathcal{B}_1, \\dots \\mathcal{B}_M$ with size $B$. So $M=\\left \\lceil{\\frac{N}{B}}\\right \\rceil $. \n",
        "\n",
        "- For each iteration $s=1,\\dots,M$, \n",
        "   - compute the gradient of loss functions restricted to batch $\\mathcal{B}_j$, \n",
        "   \\begin{align}\n",
        "   &\\frac{\\partial L(\\theta; \\mathcal{B}_s)}{\\partial \\theta} = \\frac{1}{B}\\sum_{i\\in \\mathcal{B}_s}\\frac{\\partial C_i}{\\partial \\theta}\\approx  \\frac{\\partial L(\\theta; \\mathcal{D})}{\\partial \\theta} \n",
        "   \\end{align}\n",
        "   \n",
        "   -  Update the weights and biases using learning rate $\\eta>0$\n",
        "  $$ \\theta\\leftarrow \\theta - \\eta \\cdot \\frac{\\partial L(\\theta; \\mathcal{B}_s)}{\\partial \\theta}$$\n",
        "\n",
        "  \n",
        "\n",
        "  This completes one epoch in the training process.\n",
        "\n",
        "- Repeat the preceding step until convergence.   \n",
        "\n",
        "Note \n",
        "\n",
        "1. An epoch means training the neural network with all the training data for one cycle. In an epoch, we use all of the data exactly once.\n",
        "\n",
        "2. For each complete epoch, we have several iterations. Iteration is the number of batches or steps through the randomly partitioned training data, needed to complete one epoch.\n",
        "\n",
        "3. choice of learning rate $\\eta$ is crucial; we have talked about it in Lecture7. \n",
        "\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "XtAJTUegYQmV",
        "outputId": "e4e06fb3-8ea3-4a3e-bc52-9e382fbecc47"
      },
      "source": [
        "Image(url='https://github.com/yexf308/MAT592/blob/main/image/SGD_learning.png?raw=true', width=1200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/SGD_learning.png?raw=true\" width=\"1200\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqQxWtIBogl1"
      },
      "source": [
        "SGD/Mini-batch has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another.  In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum. \n",
        "### SGD with momentum\n",
        "\\begin{align}\n",
        "&v^t =\\gamma v^{t-1}+\\eta \\frac{\\partial L(\\theta; \\mathcal{B}_s)}{\\partial \\theta}|_{\\color{red}{\\theta=\\theta^{t-1}}} \\\\ \n",
        "& \\theta^t = \\theta^{t-1} -v^t\n",
        "\\end{align}\n",
        "\n",
        "- the momentum $\\gamma$ is typically set to 0.9. \n",
        "\n",
        "- accelerate the standard SGD and converge faster; also simple to implement\n",
        "\n",
        "The momentum name comes from an analogy to physics, such as ball accelerating down a slope. In the case of weight updates, we can think of the weights as a particle traveling through parameter space which incurs acceleration from the gradient of the loss.\n",
        "\n",
        "### SGD with Nesterov momentum\n",
        "Nesterov Momentum is a slightly different version of the momentum update that has recently been gaining popularity. In this version we’re first looking at a point where current momentum is pointing to and computing gradients from that point.\n",
        "\\begin{align}\n",
        "&v^t =\\gamma v^{t-1}+\\eta \\frac{\\partial L(\\theta; \\mathcal{B}_s)}{\\partial \\theta}|_{\\color{red}{\\theta =\\theta^{t-1}-\\gamma v^{t-1}}} \\\\ \n",
        "& \\theta^t = \\theta^{t-1} -v^t\n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "yGMcFG6XnE2P",
        "outputId": "aec21b26-7280-4892-ea0c-8caf7f03c274"
      },
      "source": [
        "Image(url='https://github.com/yexf308/MAT592/blob/main/image/momentum.png?raw=true', width=900)\n",
        "# from https://ruder.io/. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/momentum.png?raw=true\" width=\"900\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86BKE3nAnzEB"
      },
      "source": [
        "### More gradient-based methods \n",
        "see [this webside](https://ruder.io/optimizing-gradient-descent/), like Adagrad, and Adam. Will discuss these more in more advanced classes."
      ]
    }
  ]
}