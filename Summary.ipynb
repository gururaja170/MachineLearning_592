{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "26.Summary.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPEWm0WE9aBKjMKoPYHm35a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yexf308/MAT592/blob/main/26_Summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RbPXc5f8voF",
        "outputId": "3707d127-71be-4029-d074-cb505f233b5b"
      },
      "source": [
        "%pylab inline \n",
        "from IPython.display import Image\n",
        "import numpy.linalg as LA"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "Zme5xr5R-Vem",
        "outputId": "5bd59d8e-6e72-4aca-b1ed-350e303f329e"
      },
      "source": [
        "Image(url='https://github.com/yexf308/MAT592/blob/main/image/ml_map.png?raw=true', width=900)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/ml_map.png?raw=true\" width=\"900\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oluO2iGg_h9L"
      },
      "source": [
        "# $\\color{red}{\\text{Cross Validation}}$\n",
        "It is extremely important to cross validate your machine learning algorithms to prevent overfitting. \n",
        "\n",
        "- **Holdout method**: In the holdout method, we randomly assign data points to two sets $d_0$ and $d_1$, called the training set and the test set, respectively. The size of each of the sets is arbitrary, typically the test set is smaller than the training set. We then train (build a model) on $d_0$ and test (evaluate its performance) on $d_1$. It is the simplest kind of cross-validation. \n",
        "\n",
        "- **$k$-fold cross-validation**: In $k$-fold cross-validation, the original sample is randomly partitioned into $k$ equal sized subsamples. Of the $k$ subsamples, a single subsample is retained as the validation data for testing the model, and the remaining $k âˆ’ 1$ subsamples are used as training data. The cross-validation process is then repeated $k$ times, with each of the $k$ subsamples used exactly once as the validation data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "-iEkaDbu__EA",
        "outputId": "e99b469d-43be-494a-9e8f-484d168bd308"
      },
      "source": [
        "Image(url='https://github.com/yexf308/MAT592/blob/main/image/generalise.png?raw=true',width=500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/generalise.png?raw=true\" width=\"500\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHU2qFptSv03"
      },
      "source": [
        "# $\\color{red}{\\text{Gradient based methods}}$\n",
        "- Gradient descent method. \n",
        "$$ \\theta \\leftarrow \\theta- \\frac{\\eta}{N} \\sum_{i=1}^N\\nabla L_i(\\theta)$$\n",
        "- Stochastic gradient descent method. \n",
        "\n",
        "$$\\theta \\leftarrow \\theta- \\eta \\nabla L_i(\\theta) $$\n",
        "\n",
        "\n",
        "- Mini-batch gradient descent method \n",
        "$$\\theta \\leftarrow \\theta- \\frac{\\eta}{|\\mathcal{B}|}\\sum_{i\\in\\mathcal{B}} \\nabla L_i(\\theta) $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CCINzxq_b_0"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# $\\color{blue}{\\text{Regression}}$\n",
        "**Given:** the training data $\\mathcal{D}=\\{\\mathbf{x}^{(i)}, y^{(i)}\\}_{i=1}^N$, where  $\\mathbf{x}^{(i)}\\in \\mathbb{R}^d$ and $y^{(i)}\\in \\mathbb{R}$ are both continuous. \n",
        "\n",
        "**Goal:** Predict the target variable $y$ with the feature $\\mathbf{x}$. The model is \n",
        "$$p(y|\\mathbf{x}, \\theta) = \\mathcal{N}(y|\\mathbf{w}^\\top \\phi(\\mathbf{x}), \\sigma^2) $$\n",
        "where $\\phi(\\mathbf{x})$  is the nonlinear transformation. \n",
        "\n",
        "## 1. Linear regression \n",
        "- The cost function is $L(\\mathbf{w})= \\frac{1}{2}\\|\\mathbf{X}\\mathbf{w}-\\mathbf{y}\\|_2^2$. \n",
        "\n",
        "- $\\hat{\\mathbf{w}}= \\arg\\min_{\\mathbf{w}}L(\\mathbf{w})=  (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{y}$.\n",
        "\n",
        "## 2. Ridge regression \n",
        "-  Penalize the magnitude of the regression coefficients to prevent overfitting. Introduce $L_2$ regularization. \n",
        "\n",
        "- The cost function is $L(\\mathbf{w})= \\frac{1}{2}\\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_2^2 +\\frac{1}{2}\\lambda \\|\\mathbf{w}\\|_2^2$ where $\\lambda>0$. \n",
        "\n",
        "- $\\hat{\\mathbf{w}}= \\arg\\min_{\\mathbf{w}}L(\\mathbf{w})= (\\mathbf{X}^\\top\\mathbf{X}+\\lambda \\mathbf{I}_d)^{-1}\\mathbf{X}^\\top \\mathbf{y} $.\n",
        "\n",
        "\n",
        "## 3. LASSO\n",
        "\n",
        "- Want to have the sparse solution. Introduce $L_1$ regularization. \n",
        "\n",
        "-  The cost function is $L(\\mathbf{w})= \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_2^2 +\\lambda \\|\\mathbf{w}\\|_1$ where $\\lambda>0$. \n",
        "\n",
        "- Use Pathwise Coordinate Descent algorithm to solve numerically. See [Lect6](https://github.com/yexf308/MAT592/blob/main/Module1/6_regression2.ipynb) for derivation.  This method is particularly appealing if each one-dimensional optimization problem can be solved analytically. \n",
        "\n",
        "- LASSO is for feature selection. You still need to perform linear regression with these new features. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e89jXV6WKc1M"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# $\\color{blue}{\\text{Classification}}$ \n",
        "**Given:** the training data $\\mathcal{D}=\\{\\mathbf{x}^{(i)}, y^{(i)}\\}_{i=1}^N$, where  $\\mathbf{x}^{(i)}\\in \\mathbb{R}^d$ and $y\\in\\{1,\\dots, C\\}$ in the **discrete** class label. If $C=2$, this is known as **binary logistic regression** , and if $C>2$, it is **multiclass logistic regression**. \n",
        "\n",
        "**Goal:** Predict the target variable $y$ with the feature $\\mathbf{x}$.\n",
        "\n",
        "## 1. Logistic regression\n",
        "\n",
        "### A. Perceptron \n",
        "- The classifier $f_\\theta(\\mathbf{x})= \\phi(\\mathbf{w}^\\top\\mathbf{x})=\\mathbb{1}_{(\\mathbf{w}^\\top \\mathbf{x}>0)}$. \n",
        "\n",
        "- The cost function is $L(\\mathbf{w})= \\sum_{i=1}^N L_i(\\mathbf{w})$ and $L_i(\\mathbf{w})= (y^{(i)} - \\phi(\\mathbf{w}^\\top\\mathbf{x}^{(i)}) )^2$. \n",
        "\n",
        "- Use the \"fake\" gradient, $\\nabla L_i(\\mathbf{w})=(\\phi(\\mathbf{w}^\\top\\mathbf{x}^{(i)})-y^{(i)})\\cdot \\phi'(\\mathbf{w}^\\top\\mathbf{x}^{(i)})\\cdot \\mathbf{x}^{(i)}$ and apply stochastic gradient descent algorithm. \n",
        "\n",
        "\n",
        "### B. Binary logistic regression\n",
        "\n",
        "- Given an input $\\mathbf{x}$, the logistic model outputs $\\sigma(\\mathbf{w}^\\top\\mathbf{x})$ can be interpreted as the probability that $\\mathbf{x}$ belongs to Class 1, i.e., $ p(y=1| \\mathbf{x;w})=\\sigma(\\mathbf{w}^\\top \\mathbf{x}),\\ \\  p(y=0| \\mathbf{x;w}) = 1-\\sigma(\\mathbf{w}^\\top \\mathbf{x})$, where $ \\sigma(z)=\\frac{1}{1+\\exp(-z)}$. \n",
        "\n",
        "- The classifier $f_\\theta(\\mathbf{x})= \\begin{cases} \\text{ 1} & \\text{if } \\sigma(\\mathbf{w}^\\top\\mathbf{x})>1/2 \\\\\n",
        " \\text{ 0} & \\text{Otherwise }\\end{cases}$\n",
        "\n",
        "- The cost function is the negative log likelihood $L(\\mathbf{w})=\\sum_{i=1}^N L_i(\\mathbf{w})$ and $L_i(\\mathbf{w})=-y^{(i)} \\log\\sigma(\\mathbf{w}^\\top \\mathbf{x}^{(i)}) + (1-y^{(i)})\\log(1-\\sigma(\\mathbf{w}^\\top \\mathbf{x}^{(i)}))\n",
        "$\n",
        "\n",
        "- The gradient of the cost function, $\\nabla_{\\mathbf{w}} L(\\mathbf{w})=(\\mathbb{1}_N^\\top \\text{diag}(\\mathbf{\\sigma}- \\mathbf{y})\\mathbf{X})^\\top$ and apply stochastic gradient descent algorithm. \n",
        "\n",
        "## 2. Support Vector Machines\n",
        "IN SVM, we move the interception out and the label is $\\pm 1$. The classifier $f_\\theta(\\mathbf{x})=\\phi(\\mathbf{w}^\\top\\mathbf{x}+w_0)=\\text{sign}(\\mathbf{w}^\\top\\mathbf{x}+w_0)$, where $\\mathbf{w}^\\top\\mathbf{x}+w_0=0$ is the decision boundry. \n",
        "\n",
        "### A. Hard Margin Classification\n",
        "- Find the best separator such that maximize the margin. This only works when the data is linearly separable. \n",
        "\n",
        "- What is **support vector** and **margin boundary**?\n",
        "\n",
        "- The constraint optimization problem is $\\min_{\\mathbf{w}, w_0} \\frac{1}{2}\\|\\mathbf{w}\\|_2^2 \\text{, subject to } \\mathbf{y}^{(i)}  (\\mathbf{w}^\\top \\mathbf{x}^{(i)} +w_0 )\\geq 1, \\; \\forall \\, i\n",
        "$.\n",
        "\n",
        "- Solve the dual problem numerically with SMO algorithm. \n",
        "\n",
        "### B. Soft Margin classification\n",
        "\n",
        "- Relax constraint to allow misclassification for outliers. \n",
        "\n",
        "- The constraint optimization problem is $\\min_{\\mathbf{w}\\in\\mathbb{R}^d, w_0\\in \\mathbb{R},\\xi\\in \\mathbb{R}^N}\\frac{1}{2}\\|\\mathbf{w}\\|^2+ \\lambda \\sum_{i=1}^N \\xi_i$, subject to $\\mathbf{y}^{(i)}(\\mathbf{w}^\\top \\mathbf{x}^{(i)}+w_0)\\ge 1 -\\xi_i, \n",
        "\\xi_i\\ge 0, \\forall i=1, \\dots, N$\n",
        "\n",
        "- Find the dual problem for it. When $\\lambda=\\infty$, it becomes hard margin classification. \n",
        "\n",
        "- Rewrite it with the hinge loss, the cost function is $L(\\mathbf{w}, w_0)=\\lambda\\sum_{i=1}^N \\max\\left\\{0, 1-\\mathbf{y}^{(i)}(\\mathbf{w}^\\top \\mathbf{x}^{(i)}+w_0)\\right\\} +\\frac{1}{2} \\|\\mathbf{w}\\|_2^2$\n",
        "\n",
        "- The gradient of the loss function $\\nabla_{\\mathbf{w}} L(\\mathbf{w},w_0) =\\begin{cases} \\mathbf{w} & \\text{if } \\mathbf{y}^{(i)}(\\mathbf{w}^\\top \\mathbf{x}^{(i)}+w_0)>1\\\\ \\mathbf{w} -\\lambda y^{(i)} \\mathbf{x}^{(i)} & \\text{Otherwise}\n",
        "\\end{cases}  $ and $\\nabla_{w_0} L(\\mathbf{w},w_0) =\\begin{cases} 0 & \\text{if } \\mathbf{y}^{(i)}(\\mathbf{w}^\\top \\mathbf{x}^{(i)}+w_0)>1\\\\  -\\lambda y^{(i)}  & \\text{Otherwise}\n",
        "\\end{cases}  $. Apply stochastic gradient descent algorithm. \n",
        "\n",
        "### C. The kernel trick\n",
        "\n",
        "- Map the training data into a higher dimensional feature space, where the data is separable in feature space. \n",
        "\n",
        "\n",
        "- Define the kernel $\\mathcal{K}(\\mathbf{x}^{(i)},\\mathbf{x}^{(j)})=\\phi(\\mathbf{x}^{(i)})^\\top\\phi(\\mathbf{x}^{(j)})$. The Mercer's condition for the kernel. \n",
        "\n",
        "- The classifier: $f_\\theta(\\mathbf{x})=\\mathbf{w}^\\top \\phi(\\mathbf{x})+w_0=\\sum_{i=1}^N\\alpha_iy^{(i)}\\mathcal{K}(\\mathbf{x}^{(i)}, \\mathbf{x})+w_0$.\n",
        "\n",
        "- Primal problem cannot be solved easily this time, you have to solve the dual problem. Details in Lect11. \n",
        "\n",
        "### D. Multiclass SVM\n",
        "- One-against-all scheme and One-against-one scheme. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83npVau_VliD"
      },
      "source": [
        "## 3. $k$-nearest neighbors\n",
        "- $k$-NN is a non-parameterized algorithm for classification (majority vote) and regression (averaging).\n",
        "\n",
        "- No explicit training step or optimization process, i.e., use $k$ nearest data samples of $\\mathbf{x}$ to predict. \n",
        "\n",
        "- Distance function and Decision rule. Using euclidean norm may has problem of the curse of dimensionality (can only handle dim<=10).\n",
        "\n",
        "- Learn the distance function (Mahalanobis metric) from the data: Large margin nearest neighbor. \n",
        "\n",
        "\n",
        "\n",
        "## 4. Decision tree\n",
        "- In the training data $\\{\\mathbf{x}^{(i)}, y^{(i)}\\}_{i=1}^N$, the features variables $\\mathbf{x}$ can be continuous or discrete. The dependent variable $y$ is discrete (classification) and could be continuous (regression).\n",
        "\n",
        "- the loss function \n",
        "$ \\mathcal{L}(\\theta)=\\sum_{i=1}^N\\ell(y^{(i)}, f(\\mathbf{x}^{(i)};\\theta)) =\\sum_{j=1}^J \\sum_{\\mathbf{x}^{(i)}\\in R_j}\\ell(y^{(i)},w_j)$\n",
        "\n",
        "- Use a **greedy procedure**, in which we iteratively grow the tree one node at a time.  Highly informative features are placed higher up in the tree. \n",
        "   - Iterative Dichotomiser 3(ID3) uses the information gain as the metric and only works with categorical/discrete features.\n",
        "   -  C4.5 algorithm accepts discrete/continuous features and chooses the feature with highest information gain ratio from among the features whose information gain is average or higher. \n",
        "   - Classification and Regression Trees (CART) accepts discrete/continuous features and target variables and uses gini impurity as the metric and only uses binary tree. \n",
        "\n",
        "\n",
        "## 5. Deep neural network.\n",
        "\n",
        "- Structure: input layer, hidden neuron layers and output layer. The output is $\\vec f(\\mathbf{x},\\theta)$ can be treated as the predicted probability for the $K$ classes. \n",
        "\n",
        "- Activation functions: sigmoid, relu, tanh, softmax (only on the output layer)\n",
        "\n",
        "- Loss function: \n",
        "    - Square loss: $L(\\theta; \\mathcal{D}) = \\frac{1}{2N}\\sum_{i=1}^N ||\\vec f(\\mathbf{x}^{(i)};\\theta)-\\mathbf{y}^{(i)}||^2 $\n",
        "\n",
        " - Logistic loss (prefered):   \n",
        " $ L(\\theta; \\mathcal{D})= -\\frac{1}{N}\\sum_{i=1}^N \\sum_{j=1}^K \\left(\\mathbf{y}^{(i)}_j \\log f_{j}(\\mathbf{x}^{(i)}; \\theta) +(1-\\mathbf{y}^{(i)}_j)\\log (1-f_{j}(\\mathbf{x}^{(i)}; \\theta))\\right) $\n",
        "\n",
        "- Forward propagation: \n",
        "For each $l=1,2,\\dots, L$, \n",
        "\\begin{align}\n",
        "&\\mathbf{z}^{(l)}=\\mathbf{W}^{(l)}\\mathbf{a}^{(l-1)}+\\mathbf{b}^{(l)} \\\\\n",
        "&\\mathbf{a}^{(l)}=g(\\mathbf{z}^{(l)})\n",
        "\\end{align} \n",
        "and $\\vec f(\\mathbf{x}; \\theta)=\\mathbf{a}^{(L)}(\\mathbf{x})$ is the output. \n",
        "\n",
        "- Backward propagation: For each $l=L-1,\\dots, 1$, \n",
        "  \\begin{align}\n",
        "&\\delta^{(L)}=\\frac{\\partial C_i}{\\partial \\mathbf{a}^{(L)}}\\circ g'(\\mathbf{z}^{(L)}) \\\\ \n",
        "&\\delta^{(l)}= \\left((\\mathbf{W}^{(l+1)})^\\top \\cdot \\delta^{(l+1)}\\right)\\circ g'(\\mathbf{z}^{(l)}).\n",
        "    \\end{align}\n",
        "\n",
        "\n",
        "- Compute the gradient for every layer $l=1,\\dots,L$ ,\n",
        "\\begin{align}\n",
        "&\\frac{\\partial C_i}{\\partial \\mathbf{W}^{(l)}} = \\delta^{(l)}\\cdot (\\mathbf{a}^{(l-1)})^\\top \\\\ \n",
        "& \\frac{\\partial C_i}{\\partial \\mathbf{b}^{(l)}}  = \\delta^{(l)}\n",
        "\\end{align}\n",
        "\n",
        "- Update parameters with stochastic gradient descent method. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDREgQ15iJes"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# $\\color{blue}{\\text{Clustering}}$ \n",
        "**Given:** unlabeled data samples $\\{\\mathbf{x}^{(i)}\\}_{i=1}^N$; specify number of clusters/partitions $K$.\n",
        "\n",
        "**Goal:** group the samples into $K$ clusters, $A(\\mathbf{x}^{(i)})\\in \\{1, \\dots, K\\}$.\n",
        "\n",
        "## 1. Kmeans clustering\n",
        "- **Goal**: Find $K$ clusters $\\mathcal{C}_{1, \\dots, K}$ and their centroids $\\mathbf{\\mu}= \\{\\mu_1, \\dots, \\mu_K\\}$ that minimize the total within cluster variance. \n",
        "\n",
        "- The cost function $L(\\mathcal{C},\\mu )= \\sum_{j=1}^K \\sum_{\\mathbf{x}^{(i)}\\in \\mathcal{C}_j}\\|\\mathbf{x}^{(i)}-\\mu_j\\|^2$, where  $\\mathbf{x}^{(i)}$ is assigned to the cluster with the closest centroid, $\\text{arg}\\min_{1\\le c\\le K}\\|\\mathbf{x}^{(i)} -\\mu_c \\|_2^2$\n",
        "\n",
        "- Use **Lloyd's algorithm**, which is alternative optimization method to minimize the cost function. This a special case of EM algorithm. \n",
        "\n",
        "## 2. Gaussian Mixture Models\n",
        "- Assume data is generated by two-step process: first generate its cluster membership $z^{(i)}\\in \\{1, \\dots, K\\}$ from the multinomial distribution $\\pi =(\\pi_1, \\dots, \\pi_K)$ and second given the Cluster $z^{(i)}=c$, generate a point $\\mathbf{x}^{(i)}$ from the associated multivariate Gaussian distribution $\\mathcal{N}(\\mu_c, \\Sigma_c)$. \n",
        "\n",
        "- **Goal:** infer parameters $\\{\\pi_c, \\mu_c, \\Sigma_c\\}_{c=1}^K$ from the data $\\{\\mathbf{x}^{(i)}\\}_{i=1}^N$ and recover\n",
        "the clusters, i.e. $\\{z^{(i)}\\}_{i=1}^N$. \n",
        "\n",
        "- Log-likelihood function for GMM is $\\log \\ell(\\theta) =\\sum_{i=1}^N\\log\\sum_{z^{(i)}=1}^K p(\\mathbf{x}^{(i)}, z^{(i)}|\\theta). $ \n",
        "\n",
        "- Use Expectation-Maximization(EM) algorithm to maximize the Log-likelihood function. Why EM works? \n",
        "\n",
        "## 3. Hidden Markov Models\n",
        "\n",
        "- To be continued  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0fMNlbtqJhG"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# $\\color{blue}{\\text{Dimensional Reduction}}$ \n",
        "**Goal:** to learning a mapping from the high dimensional visible space $\\mathbf{x}\\in \\mathbb{R}^D$, to a low-dimensional latent space $\\mathbf{z}\\in \\mathbb{R}^d$\n",
        "\n",
        "## 1.Principal Component Analysis  \n",
        "- **Goal:** find a proper projection onto $d$ dimension subspace $\\mathcal{V}\\subset \\mathbb{R}^D$ with orthonormal basis $\\{\\mathbf{w}_1,\\dots, \\mathbf{w}_d\\}$, i.e., $\\|\\mathbf{w}_j\\|=1$ and $\\mathbf{w}_i^\\top\\mathbf{w}_j=0$ for $i\\ne j$.\n",
        "\n",
        "-  Find orthonormal basis of $\\mathcal{V}$ by solving the maximum variance problem, $\\max_{\\mathbf{w}_1,\\dots, \\mathbf{w}_d\\in\\mathbf{R}^D} \\sum_{j=1}^d\\mathbf{w}_j^\\top \\mathbf{X}^\\top \\mathbf{X}\\mathbf{w}_j  $,\n",
        "subject to \n",
        "$\\mathbf{w}_i^\\top\\mathbf{w}_j=\\delta_{ij} $.\n",
        "\n",
        "- Solved analytically, $\\mathbf{w}_j^*=\\mathbf{v}_j: j =1,\\dots, d$ are the first $d$ columns of $\\mathbf{V}$, where the SVD of data matrix is $\\mathbf{X}=\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^\\top$.\n",
        "\n",
        "- low-dimensional data representation: \n",
        "$ \\mathbf{z}^{(i)}=[\\mathbf{v}_1^\\top \\mathbf{x}^{(i)}, \\dots, \\mathbf{v}_d^\\top \\mathbf{x}^{(i)}]^\\top=\\mathbf{V}_d^\\top\\mathbf{x}^{(i)} \\in \\mathbb{R}^d$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMrRokWmV6tT"
      },
      "source": [
        "# New Section"
      ]
    }
  ]
}