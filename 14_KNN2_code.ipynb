{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "14.KNN2_code.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOpBcaUnJ7Buv0NQaO3ii0A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yexf308/MAT592/blob/main/14_KNN2_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4iMik6qq5Up"
      },
      "source": [
        "This code is from https://github.com/johny-c/pylmnn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYwTTfOrrpIg",
        "outputId": "20e1c808-5395-4c96-fccf-a534f4528039"
      },
      "source": [
        "%pylab inline \n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "x7cykkLqnXxt"
      },
      "source": [
        "#@title uility function\n",
        "import numpy as np\n",
        "from sklearn.utils.extmath import row_norms, safe_sparse_dot\n",
        "\n",
        "\n",
        "def _euclidean_distances_without_checks(X, Y=None, Y_norm_squared=None,\n",
        "                                        squared=False, X_norm_squared=None,\n",
        "                                        clip=True):\n",
        "    \"\"\"sklearn.pairwise.euclidean_distances without checks with optional clip.\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : {array-like, sparse matrix}, shape (n_samples_1, n_features)\n",
        "    Y : {array-like, sparse matrix}, shape (n_samples_2, n_features)\n",
        "    Y_norm_squared : array-like, shape (n_samples_2, ), optional\n",
        "        Pre-computed dot-products of vectors in Y (e.g.,\n",
        "        ``(Y**2).sum(axis=1)``)\n",
        "    squared : boolean, optional\n",
        "        Return squared Euclidean distances.\n",
        "    X_norm_squared : array-like, shape = [n_samples_1], optional\n",
        "        Pre-computed dot-products of vectors in X (e.g.,\n",
        "        ``(X**2).sum(axis=1)``)\n",
        "    clip : bool, optional (default=True)\n",
        "        Whether to explicitly enforce computed distances to be non-negative.\n",
        "        Some algorithms, such as LargeMarginNearestNeighbor, compare distances\n",
        "        to strictly positive values (distances to farthest target neighbors\n",
        "        + margin) only to make a binary decision (if a sample is an impostor\n",
        "        or not). In such cases, it does not matter if the distance is zero\n",
        "        or negative, since it is definitely smaller than a strictly positive\n",
        "        value.\n",
        "    Returns\n",
        "    -------\n",
        "    distances : array, shape (n_samples_1, n_samples_2)\n",
        "    \"\"\"\n",
        "\n",
        "    if Y is None:\n",
        "        Y = X\n",
        "\n",
        "    if X_norm_squared is not None:\n",
        "        XX = X_norm_squared\n",
        "        if XX.shape == (1, X.shape[0]):\n",
        "            XX = XX.T\n",
        "    else:\n",
        "        XX = row_norms(X, squared=True)[:, np.newaxis]\n",
        "\n",
        "    if X is Y:  # shortcut in the common case euclidean_distances(X, X)\n",
        "        YY = XX.T\n",
        "    elif Y_norm_squared is not None:\n",
        "        YY = np.atleast_2d(Y_norm_squared)\n",
        "    else:\n",
        "        YY = row_norms(Y, squared=True)[np.newaxis, :]\n",
        "\n",
        "    distances = safe_sparse_dot(X, Y.T, dense_output=True)\n",
        "    distances *= -2\n",
        "    distances += XX\n",
        "    distances += YY\n",
        "\n",
        "    if clip:\n",
        "        np.maximum(distances, 0, out=distances)\n",
        "\n",
        "    if X is Y:\n",
        "        # Ensure that distances between vectors and themselves are set to 0.0.\n",
        "        # This may not be the case due to floating point rounding errors.\n",
        "        distances.flat[::distances.shape[0] + 1] = 0.0\n",
        "\n",
        "    return distances if squared else np.sqrt(distances, out=distances)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_comparison(components, X, y, dim_pref=2, t_sne=False):\n",
        "    \"\"\"Draw a scatter plot of points, colored by their labels, before and after applying a learned transformation\n",
        "    Parameters\n",
        "    ----------\n",
        "    components : array_like\n",
        "        The learned transformation in an array with shape (n_components, n_features).\n",
        "    X : array_like\n",
        "        An array of data samples with shape (n_samples, n_features).\n",
        "    y : array_like\n",
        "        An array of data labels with shape (n_samples,).\n",
        "    dim_pref : int\n",
        "        The preferred number of dimensions to plot (default: 2).\n",
        "    t_sne : bool\n",
        "        Whether to use t-SNE to produce the plot or just use the first two dimensions\n",
        "        of the inputs (default: False).\n",
        "    \"\"\"\n",
        "    if dim_pref < 2 or dim_pref > 3:\n",
        "        print('Preferred plot dimensionality must be 2 or 3, setting to 2!')\n",
        "        dim_pref = 2\n",
        "\n",
        "    if t_sne:\n",
        "        print(\"Computing t-SNE embedding\")\n",
        "        tsne = TSNE(n_components=dim_pref, init='pca', random_state=0)\n",
        "        X_tsne = tsne.fit_transform(X)\n",
        "        Lx_tsne = tsne.fit_transform(X.dot(components.T))\n",
        "        X = X_tsne\n",
        "        Lx = Lx_tsne\n",
        "    else:\n",
        "        Lx = X.dot(components.T)\n",
        "\n",
        "    fig = plt.figure()\n",
        "    if X.shape[1] > 2 and dim_pref == 3:\n",
        "        ax = fig.add_subplot(121, projection='3d')\n",
        "        ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y)\n",
        "        ax.set_title('Original Data')\n",
        "        ax = fig.add_subplot(122, projection='3d')\n",
        "        ax.scatter(Lx[:, 0], Lx[:, 1], Lx[:, 2], c=y)\n",
        "        ax.set_title('Transformed Data')\n",
        "    elif X.shape[1] >= 2:\n",
        "        ax = fig.add_subplot(121)\n",
        "        ax.scatter(X[:, 0], X[:, 1], c=y)\n",
        "        ax.set_title('Original Data')\n",
        "        ax = fig.add_subplot(122)\n",
        "        ax.scatter(Lx[:, 0], Lx[:, 1], c=y)\n",
        "        ax.set_title('Transformed Data')\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "GVuHdle2mAyA"
      },
      "source": [
        "#@title Large Margin Nearest Neighbor implementation\n",
        "# coding: utf-8\n",
        "\"\"\"\n",
        "Large Margin Nearest Neighbor Classification\n",
        "\"\"\"\n",
        "\n",
        "# Author: John Chiotellis <johnyc.code@gmail.com>\n",
        "# License: BSD 3 clause\n",
        "\n",
        "from __future__ import print_function\n",
        "from warnings import warn\n",
        "\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "from scipy.sparse import csr_matrix, csc_matrix, coo_matrix\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.utils import gen_batches\n",
        "from sklearn.utils.extmath import row_norms, safe_sparse_dot\n",
        "from sklearn.utils.random import check_random_state\n",
        "from sklearn.utils.multiclass import check_classification_targets\n",
        "from sklearn.utils.validation import check_is_fitted, check_array, check_X_y\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "try:\n",
        "    from six import integer_types, string_types\n",
        "except ImportError:\n",
        "    try:\n",
        "        from sklearn.externals.six import integer_types, string_types\n",
        "    except ImportError:\n",
        "        raise ImportError(\"The module six must be installed or the version of scikit-learn version must be < 0.23\")\n",
        "\n",
        "\n",
        "class LargeMarginNearestNeighbor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Distance metric learning for large margin classification.\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_neighbors : int, optional (default=3)\n",
        "        Number of neighbors to use as target neighbors for each sample.\n",
        "    n_components : int, optional (default=None)\n",
        "        Preferred dimensionality of the embedding.\n",
        "        If None it is inferred from ``init``.\n",
        "    init : string or numpy array, optional (default='pca')\n",
        "        Initialization of the linear transformation. Possible options are\n",
        "        'pca', 'identity' and a numpy array of shape (n_features_a,\n",
        "        n_features_b).\n",
        "        pca:\n",
        "            ``n_components`` many principal components of the inputs passed\n",
        "            to :meth:`fit` will be used to initialize the transformation.\n",
        "        identity:\n",
        "            If ``n_components`` is strictly smaller than the\n",
        "            dimensionality of the inputs passed to :meth:`fit`, the identity\n",
        "            matrix will be truncated to the first ``n_components`` rows.\n",
        "        numpy array:\n",
        "            n_features_b must match the dimensionality of the inputs passed to\n",
        "            :meth:`fit` and n_features_a must be less than or equal to that.\n",
        "            If ``n_components`` is not None, n_features_a must match it.\n",
        "    warm_start : bool, optional, (default=False)\n",
        "        If True and :meth:`fit` has been called before, the solution of the\n",
        "        previous call to :meth:`fit` is used as the initial linear\n",
        "        transformation (``n_components`` and ``init`` will be ignored).\n",
        "    max_impostors : int, optional (default=500000)\n",
        "        Maximum number of impostors to consider per iteration. In the worst\n",
        "        case this will allow ``max_impostors * n_neighbors`` constraints to be\n",
        "        active.\n",
        "    neighbors_params : dict, optional (default=None)\n",
        "        Parameters to pass to a :class:`neighbors.NearestNeighbors` instance -\n",
        "        apart from ``n_neighbors`` - that will be used to select the target\n",
        "        neighbors.\n",
        "    weight_push_loss : float, optional (default=0.5)\n",
        "        A float in (0, 1], weighting the push loss. This is parameter ``μ``\n",
        "        in the journal paper (See references below). In practice, the objective\n",
        "        function will be normalized so that the push loss has weight 1 and\n",
        "        hence the pull loss has weight ``(1 - μ)/μ``.\n",
        "    impostor_store : str ['auto'|'list'|'sparse'], optional\n",
        "        list :\n",
        "            Three lists will be used to store the indices of reference\n",
        "            samples, the indices of their impostors and the (squared)\n",
        "            distances between the (sample, impostor) pairs.\n",
        "        sparse :\n",
        "            A sparse indicator matrix will be used to store the (sample,\n",
        "            impostor) pairs. The (squared) distances to the impostors will be\n",
        "            computed twice (once to determine the impostors and once to be\n",
        "            stored), but this option tends to be faster than 'list' as the\n",
        "            size of the data set increases.\n",
        "        auto :\n",
        "            Will attempt to decide the most appropriate choice of data\n",
        "            structure based on the values passed to :meth:`fit`.\n",
        "    max_iter : int, optional (default=50)\n",
        "        Maximum number of iterations in the optimization.\n",
        "    tol : float, optional (default=1e-5)\n",
        "        Convergence tolerance for the optimization.\n",
        "    callback : callable, optional (default=None)\n",
        "        If not None, this function is called after every iteration of the\n",
        "        optimizer, taking as arguments the current solution (transformation)\n",
        "        and the number of iterations. This might be useful in case one wants\n",
        "        to examine or store the transformation found after each iteration.\n",
        "    store_opt_result : bool, optional (default=False)\n",
        "        If True, the :class:`scipy.optimize.OptimizeResult` object returned by\n",
        "        :meth:`minimize` of `scipy.optimize` will be stored as attribute\n",
        "        ``opt_result_``.\n",
        "    verbose : int, optional (default=0)\n",
        "        If 0, no progress messages will be printed.\n",
        "        If 1, progress messages will be printed to stdout.\n",
        "        If > 1, progress messages will be printed and the ``iprint``\n",
        "        parameter of :meth:`_minimize_lbfgsb` of `scipy.optimize` will be set\n",
        "        to ``verbose - 2``.\n",
        "    random_state : int or numpy.RandomState or None, optional (default=None)\n",
        "        A pseudo random number generator object or a seed for it if int.\n",
        "    n_jobs : int, optional (default=1)\n",
        "        The number of parallel jobs to run for neighbors search.\n",
        "        If ``-1``, then the number of jobs is set to the number of CPU cores.\n",
        "        Doesn't affect :meth:`fit` method.\n",
        "    Attributes\n",
        "    ----------\n",
        "    components_ : array, shape (n_components, n_features)\n",
        "        The linear transformation learned during fitting.\n",
        "    n_neighbors_ : int\n",
        "        The provided ``n_neighbors`` is decreased if it is greater than or\n",
        "        equal to  min(number of elements in each class).\n",
        "    n_iter_ : int\n",
        "        Counts the number of iterations performed by the optimizer.\n",
        "    opt_result_ : scipy.optimize.OptimizeResult (optional)\n",
        "        A dictionary of information representing the optimization result.\n",
        "        This is stored only if ``store_opt_result`` is True. It contains the\n",
        "        following attributes:\n",
        "        x : ndarray\n",
        "            The solution of the optimization.\n",
        "        success : bool\n",
        "            Whether or not the optimizer exited successfully.\n",
        "        status : int\n",
        "            Termination status of the optimizer.\n",
        "        message : str\n",
        "            Description of the cause of the termination.\n",
        "        fun, jac : ndarray\n",
        "            Values of objective function and its Jacobian.\n",
        "        hess_inv : scipy.sparse.linalg.LinearOperator\n",
        "            the product of a vector with the approximate inverse of the\n",
        "            Hessian of the objective function..\n",
        "        nfev : int\n",
        "            Number of evaluations of the objective function..\n",
        "        nit : int\n",
        "            Number of iterations performed by the optimizer.\n",
        "    Examples\n",
        "    --------\n",
        "    >>> from pylmnn import LargeMarginNearestNeighbor\n",
        "    >>> from sklearn.neighbors import KNeighborsClassifier\n",
        "    >>> from sklearn.datasets import load_iris\n",
        "    >>> from sklearn.model_selection import train_test_split\n",
        "    >>> X, y = load_iris(return_X_y=True)\n",
        "    >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "    ... stratify=y, test_size=0.7, random_state=42)\n",
        "    >>> lmnn = LargeMarginNearestNeighbor(n_neighbors=3, random_state=42)\n",
        "    >>> lmnn.fit(X_train, y_train) # doctest: +ELLIPSIS\n",
        "    LargeMarginNearestNeighbor(...)\n",
        "    >>> # Fit and evaluate a simple nearest neighbor classifier for comparison\n",
        "    >>> knn = KNeighborsClassifier(n_neighbors=3)\n",
        "    >>> knn.fit(X_train, y_train) # doctest: +ELLIPSIS\n",
        "    KNeighborsClassifier(...)\n",
        "    >>> print(knn.score(X_test, y_test))\n",
        "    0.933333333333\n",
        "    >>> # Now fit on the data transformed by the learned transformation\n",
        "    >>> knn.fit(lmnn.transform(X_train), y_train) # doctest: +ELLIPSIS\n",
        "    KNeighborsClassifier(...)\n",
        "    >>> print(knn.score(lmnn.transform(X_test), y_test))\n",
        "    0.971428571429\n",
        "    .. warning::\n",
        "        Exact floating-point reproducibility is generally not guaranteed\n",
        "        (unless special care is taken with library and compiler options). As\n",
        "        a consequence, the transformations computed in 2 identical runs of\n",
        "        LargeMarginNearestNeighbor can differ from each other. This can\n",
        "        happen even before the optimizer is called if initialization with\n",
        "        PCA is used (init='pca').\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] Weinberger, Kilian Q., and Lawrence K. Saul.\n",
        "           \"Distance Metric Learning for Large Margin Nearest Neighbor\n",
        "           Classification.\"\n",
        "           Journal of Machine Learning Research, Vol. 10, Feb. 2009,\n",
        "           pp. 207-244.\n",
        "           http://jmlr.csail.mit.edu/papers/volume10/weinberger09a/weinberger09a.pdf\n",
        "    .. [2] Wikipedia entry on Large Margin Nearest Neighbor\n",
        "           https://en.wikipedia.org/wiki/Large_margin_nearest_neighbor\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_neighbors=3, n_components=None, init='pca',\n",
        "                 warm_start=False, max_impostors=500000, neighbors_params=None,\n",
        "                 weight_push_loss=0.5, impostor_store='auto', max_iter=50,\n",
        "                 tol=1e-5, callback=None, store_opt_result=False, verbose=0,\n",
        "                 random_state=None, n_jobs=1):\n",
        "\n",
        "        # Parameters\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.n_components = n_components\n",
        "        self.init = init\n",
        "        self.warm_start = warm_start\n",
        "        self.max_impostors = max_impostors\n",
        "        self.neighbors_params = neighbors_params\n",
        "        self.weight_push_loss = weight_push_loss\n",
        "        self.impostor_store = impostor_store\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.callback = callback\n",
        "        self.store_opt_result = store_opt_result\n",
        "        self.verbose = verbose\n",
        "        self.random_state = random_state\n",
        "        self.n_jobs = n_jobs\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit the model according to the given training data.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            The training samples.\n",
        "        y : array-like, shape (n_samples,)\n",
        "            The corresponding training labels.\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "            returns a trained LargeMarginNearestNeighbor model.\n",
        "        \"\"\"\n",
        "\n",
        "        # Validate the inputs\n",
        "        X, y = check_X_y(X, y, ensure_min_samples=2)\n",
        "        check_classification_targets(y)\n",
        "\n",
        "        # Check that the inputs are consistent with the parameters\n",
        "        X_valid, y_valid, classes, init = self._validate_params(X, y)\n",
        "\n",
        "        # Initialize the random generator\n",
        "        self.random_state_ = check_random_state(self.random_state)\n",
        "\n",
        "        # Measure the total training time\n",
        "        t_train = time.time()\n",
        "\n",
        "        # Initialize the linear transformation\n",
        "        transformation = self._initialize(X_valid, init)\n",
        "\n",
        "        # Find the target neighbors\n",
        "        target_neighbors = self._select_target_neighbors_wrapper(\n",
        "            X_valid, y_valid, classes)\n",
        "\n",
        "        # Compute the gradient part contributed by the target neighbors\n",
        "        grad_static = self._compute_grad_static(X_valid, target_neighbors)\n",
        "\n",
        "        # Compute the pull loss coefficient\n",
        "        pull_loss_coef = (1. - self.weight_push_loss) / self.weight_push_loss\n",
        "        grad_static *= pull_loss_coef\n",
        "\n",
        "        # Decide how to store the impostors\n",
        "        if self.impostor_store == 'sparse':\n",
        "            use_sparse = True\n",
        "        elif self.impostor_store == 'list':\n",
        "            use_sparse = False\n",
        "        else:\n",
        "            # auto: Use a heuristic based on the data set size\n",
        "            use_sparse = X_valid.shape[0] > 6500\n",
        "\n",
        "        # Create a dictionary of parameters to be passed to the optimizer\n",
        "        disp = self.verbose - 2 if self.verbose > 1 else -1\n",
        "        optimizer_params = {'method': 'L-BFGS-B',\n",
        "                            'fun': self._loss_grad_lbfgs,\n",
        "                            'jac': True,\n",
        "                            'args': (X_valid, y_valid, classes,\n",
        "                                     target_neighbors, grad_static,\n",
        "                                     use_sparse),\n",
        "                            'x0': transformation,\n",
        "                            'tol': self.tol,\n",
        "                            'options': dict(maxiter=self.max_iter, disp=disp),\n",
        "                            'callback': self._callback\n",
        "                            }\n",
        "\n",
        "        # Call the optimizer\n",
        "        self.n_iter_ = 0\n",
        "        opt_result = minimize(**optimizer_params)\n",
        "\n",
        "        # Reshape the solution found by the optimizer\n",
        "        self.components_ = opt_result.x.reshape(-1, X_valid.shape[1])\n",
        "\n",
        "        # Stop timer\n",
        "        t_train = time.time() - t_train\n",
        "        if self.verbose:\n",
        "            cls_name = self.__class__.__name__\n",
        "\n",
        "            # Warn the user if the algorithm did not converge\n",
        "            if not opt_result.success:\n",
        "                warn('[{}] LMNN did not converge: {}'.format(\n",
        "                    cls_name, opt_result.message),\n",
        "                     ConvergenceWarning)\n",
        "\n",
        "            print('[{}] Training took {:8.2f}s.'.format(cls_name, t_train))\n",
        "\n",
        "        # Optionally store information returned by the optimizer\n",
        "        if self.store_opt_result:\n",
        "            self.opt_result_ = opt_result\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Applies the learned transformation to the given data.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            Data samples.\n",
        "        Returns\n",
        "        -------\n",
        "        X_embedded: array, shape (n_samples, n_components)\n",
        "            The data samples transformed.\n",
        "        Raises\n",
        "        ------\n",
        "        NotFittedError\n",
        "            If :meth:`fit` has not been called before.\n",
        "        \"\"\"\n",
        "\n",
        "        check_is_fitted(self, ['components_'])\n",
        "        X = check_array(X)\n",
        "\n",
        "        return np.dot(X, self.components_.T)\n",
        "\n",
        "    def _transform_without_checks(self, X):\n",
        "        \"\"\"Same as transform but without validating the inputs.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array, shape (n_samples, n_features)\n",
        "            Data samples.\n",
        "        Returns\n",
        "        -------\n",
        "        X_embedded: array, shape (n_samples, n_components)\n",
        "            The data samples transformed.\n",
        "        \"\"\"\n",
        "        return np.dot(X, self.components_.T)\n",
        "\n",
        "    def _validate_params(self, X, y):\n",
        "        \"\"\"Validate parameters as soon as :meth:`fit` is called.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            The training samples.\n",
        "        y : array-like, shape (n_samples,)\n",
        "            The corresponding training labels.\n",
        "        Returns\n",
        "        -------\n",
        "        X : array, shape (n_samples, n_features)\n",
        "            The validated training samples.\n",
        "        y_inverse : array, shape (n_samples,)\n",
        "            The validated training labels, encoded to be integers in\n",
        "            the range(0, n_classes).\n",
        "        classes_inverse_non_singleton : array, shape (n_classes_non_singleton,)\n",
        "            The non-singleton classes, encoded as integers in [0, n_classes).\n",
        "        init : string or numpy array of shape (n_features_a, n_features_b)\n",
        "            The validated initialization of the linear transformation.\n",
        "        Raises\n",
        "        -------\n",
        "        TypeError\n",
        "            If a parameter is not an instance of the desired type.\n",
        "        ValueError\n",
        "            If a parameter's value violates its legal value range or if the\n",
        "            combination of two or more given parameters is incompatible.\n",
        "        \"\"\"\n",
        "\n",
        "        # Find the appearing classes and the class index for each sample\n",
        "        classes, y_inverse = np.unique(y, return_inverse=True)\n",
        "        classes_inverse = np.arange(len(classes))\n",
        "\n",
        "        # Ignore classes that have less than 2 samples (singleton classes)\n",
        "        class_sizes = np.bincount(y_inverse)\n",
        "        mask_singleton_class = class_sizes == 1\n",
        "        singleton_classes, = np.where(mask_singleton_class)\n",
        "        if len(singleton_classes):\n",
        "            warn('There are {} singleton classes that will be ignored during '\n",
        "                 'training. A copy of the inputs `X` and `y` will be made.'\n",
        "                 .format(len(singleton_classes)))\n",
        "            mask_singleton_sample = np.asarray([yi in singleton_classes for\n",
        "                                                yi in y_inverse])\n",
        "            X = X[~mask_singleton_sample].copy()\n",
        "            y_inverse = y_inverse[~mask_singleton_sample].copy()\n",
        "\n",
        "        # Check that there are at least 2 non-singleton classes\n",
        "        n_classes_non_singleton = len(classes) - len(singleton_classes)\n",
        "        if n_classes_non_singleton < 2:\n",
        "            raise ValueError('LargeMarginNearestNeighbor needs at least 2 '\n",
        "                             'non-singleton classes, got {}.'\n",
        "                             .format(n_classes_non_singleton))\n",
        "\n",
        "        classes_inverse_non_singleton = classes_inverse[~mask_singleton_class]\n",
        "\n",
        "        # Check the preferred embedding dimensionality\n",
        "        if self.n_components is not None:\n",
        "            _check_scalar(self.n_components, 'n_components',\n",
        "                          integer_types, 1)\n",
        "\n",
        "            if self.n_components > X.shape[1]:\n",
        "                raise ValueError('The preferred embedding dimensionality '\n",
        "                                 '`n_components` ({}) cannot be greater '\n",
        "                                 'than the given data dimensionality ({})!'\n",
        "                                 .format(self.n_components, X.shape[1]))\n",
        "\n",
        "        # If warm_start is enabled, check that the inputs are consistent\n",
        "        _check_scalar(self.warm_start, 'warm_start', bool)\n",
        "        if self.warm_start and hasattr(self, 'components_'):\n",
        "            if self.components_.shape[1] != X.shape[1]:\n",
        "                raise ValueError('The new inputs dimensionality ({}) does not '\n",
        "                                 'match the input dimensionality of the '\n",
        "                                 'previously learned transformation ({}).'\n",
        "                                 .format(X.shape[1],\n",
        "                                         self.components_.shape[1]))\n",
        "\n",
        "        _check_scalar(self.n_neighbors, 'n_neighbors', integer_types, 1,\n",
        "                      X.shape[0] - 1)\n",
        "        _check_scalar(self.max_iter, 'max_iter', integer_types, 1)\n",
        "        _check_scalar(self.tol, 'tol', float, 0.)\n",
        "        _check_scalar(self.weight_push_loss, 'weight_push_loss', float, 0., 1.)\n",
        "        if self.weight_push_loss == 0:\n",
        "            raise ValueError('`weight_push_loss` cannot be zero.')\n",
        "\n",
        "        _check_scalar(self.max_impostors, 'max_impostors', integer_types, 1)\n",
        "        _check_scalar(self.impostor_store, 'impostor_store', string_types)\n",
        "        _check_scalar(self.n_jobs, 'n_jobs', integer_types)\n",
        "        _check_scalar(self.verbose, 'verbose', integer_types, 0)\n",
        "\n",
        "        if self.impostor_store not in ['auto', 'sparse', 'list']:\n",
        "            raise ValueError(\"`impostor_store` must be 'auto', 'sparse' or \"\n",
        "                             \"'list'.\")\n",
        "\n",
        "        if self.callback is not None:\n",
        "            if not callable(self.callback):\n",
        "                raise ValueError('`callback` is not callable.')\n",
        "\n",
        "        # Check how the linear transformation should be initialized\n",
        "        init = self.init\n",
        "        if isinstance(init, np.ndarray):\n",
        "            init = check_array(init)\n",
        "\n",
        "            # Assert that init.shape[1] = X.shape[1]\n",
        "            if init.shape[1] != X.shape[1]:\n",
        "                raise ValueError('The input dimensionality ({}) of the given '\n",
        "                                 'linear transformation `init` must match the '\n",
        "                                 'dimensionality of the given inputs `X` ({}).'\n",
        "                                 .format(init.shape[1], X.shape[1]))\n",
        "\n",
        "            # Assert that init.shape[0] <= init.shape[1]\n",
        "            if init.shape[0] > init.shape[1]:\n",
        "                raise ValueError('The output dimensionality ({}) of the given '\n",
        "                                 'linear transformation `init` cannot be '\n",
        "                                 'greater than its input dimensionality ({}).'\n",
        "                                 .format(init.shape[0], init.shape[1]))\n",
        "\n",
        "            if self.n_components is not None:\n",
        "                # Assert that self.n_components = init.shape[0]\n",
        "                if self.n_components != init.shape[0]:\n",
        "                    raise ValueError('The preferred embedding dimensionality '\n",
        "                                     '`n_components` ({}) does not match '\n",
        "                                     'the output dimensionality of the given '\n",
        "                                     'linear transformation `init` ({})!'\n",
        "                                     .format(self.n_components,\n",
        "                                             init.shape[0]))\n",
        "        elif init in ['pca', 'identity']:\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(\"`init` must be 'pca', 'identity', or a numpy \"\n",
        "                             \"array of shape (n_components, n_features).\")\n",
        "\n",
        "        # Check the preferred number of neighbors\n",
        "        min_non_singleton_size = class_sizes[~mask_singleton_class].min()\n",
        "        if self.n_neighbors >= min_non_singleton_size:\n",
        "            warn('`n_neighbors` (={}) is not less than the number of '\n",
        "                 'samples in the smallest non-singleton class (={}). '\n",
        "                 '`n_neighbors_` will be set to {} for estimation.'\n",
        "                 .format(self.n_neighbors, min_non_singleton_size,\n",
        "                         min_non_singleton_size - 1))\n",
        "\n",
        "        self.n_neighbors_ = min(self.n_neighbors, min_non_singleton_size - 1)\n",
        "\n",
        "        neighbors_params = self.neighbors_params\n",
        "        if neighbors_params is not None:\n",
        "            _check_scalar(neighbors_params, 'neighbors_params', dict)\n",
        "            neighbors_params.setdefault('n_jobs', self.n_jobs)\n",
        "            # Attempt to instantiate a NearestNeighbors instance here to\n",
        "            # raise any errors before actually fitting\n",
        "            NearestNeighbors(n_neighbors=self.n_neighbors_, **neighbors_params)\n",
        "\n",
        "        return X, y_inverse, classes_inverse_non_singleton, init\n",
        "\n",
        "    def _initialize(self, X, init):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array, shape (n_samples, n_features)\n",
        "            The training samples.\n",
        "        init : string or numpy array of shape (n_features_a, n_features)\n",
        "            The initialization of the linear transformation.\n",
        "        Returns\n",
        "        -------\n",
        "        transformation : array, shape (n_components, n_features)\n",
        "            The initialized linear transformation.\n",
        "        \"\"\"\n",
        "\n",
        "        transformation = init\n",
        "        if self.warm_start and hasattr(self, 'components_'):\n",
        "            transformation = self.components_\n",
        "\n",
        "        elif isinstance(init, np.ndarray):\n",
        "            pass\n",
        "\n",
        "        elif init == 'pca':\n",
        "            pca = PCA(n_components=self.n_components,\n",
        "                      random_state=self.random_state_)\n",
        "            t_pca = time.time()\n",
        "            if self.verbose:\n",
        "                print('[{}] Finding principal components...'.format(\n",
        "                    self.__class__.__name__))\n",
        "                sys.stdout.flush()\n",
        "\n",
        "            pca.fit(X)\n",
        "            if self.verbose:\n",
        "                t_pca = time.time() - t_pca\n",
        "                print('[{}] Found principal components in {:5.2f}s.'.format(\n",
        "                    self.__class__.__name__, t_pca))\n",
        "\n",
        "            transformation = pca.components_\n",
        "\n",
        "        elif init == 'identity':\n",
        "            if self.n_components is None:\n",
        "                transformation = np.eye(X.shape[1])\n",
        "            else:\n",
        "                transformation = np.eye(self.n_components, X.shape[1])\n",
        "\n",
        "        return transformation\n",
        "\n",
        "    def _select_target_neighbors_wrapper(self, X, y, classes=None):\n",
        "        \"\"\"Find the target neighbors of each data sample.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array, shape (n_samples, n_features)\n",
        "            The training samples.\n",
        "        y : array, shape (n_samples,)\n",
        "            The corresponding training labels indices.\n",
        "        classes : array, shape (n_classes,), optional (default=None)\n",
        "            The non-singleton classes, encoded as integers in [0, n_classes).\n",
        "            If None (default), they will be inferred from ``y``.\n",
        "        Returns\n",
        "        -------\n",
        "        target_neighbors: array, shape (n_samples, n_neighbors)\n",
        "            An array of neighbors indices for each sample.\n",
        "        \"\"\"\n",
        "\n",
        "        t_start = time.time()\n",
        "        if self.verbose:\n",
        "            print('[{}] Finding the target neighbors...'.format(\n",
        "                self.__class__.__name__))\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        neighbors_params = self.neighbors_params\n",
        "        if neighbors_params is None:\n",
        "            neighbors_params = {}\n",
        "\n",
        "        neighbors_params.setdefault('n_jobs', self.n_jobs)\n",
        "        target_neighbors = _select_target_neighbors(\n",
        "            X, y, self.n_neighbors_, classes=classes, **neighbors_params)\n",
        "\n",
        "        if self.verbose:\n",
        "            print('[{}] Found the target neighbors in {:5.2f}s.'.format(\n",
        "                self.__class__.__name__, time.time() - t_start))\n",
        "\n",
        "        return target_neighbors\n",
        "\n",
        "    def _compute_grad_static(self, X, target_neighbors):\n",
        "        \"\"\"Compute the gradient contributed by the target neighbors.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array, shape (n_samples, n_features)\n",
        "            The training samples.\n",
        "        target_neighbors : array, shape (n_samples, n_neighbors)\n",
        "            The k nearest neighbors of each sample from the same class.\n",
        "        Returns\n",
        "        -------\n",
        "        grad_target_neighbors, shape (n_features, n_features)\n",
        "            An array with the sum of all outer products of\n",
        "            (sample, target_neighbor) pairs.\n",
        "        \"\"\"\n",
        "\n",
        "        t_grad_static = time.time()\n",
        "        if self.verbose:\n",
        "            print('[{}] Computing static part of the gradient...'.format(\n",
        "                self.__class__.__name__))\n",
        "\n",
        "        n_samples, n_neighbors = target_neighbors.shape\n",
        "        row = np.repeat(range(n_samples), n_neighbors)\n",
        "        col = target_neighbors.ravel()\n",
        "        tn_graph = csr_matrix((np.ones(target_neighbors.size), (row, col)),\n",
        "                              shape=(n_samples, n_samples))\n",
        "        grad_target_neighbors = _sum_weighted_outer_differences(X, tn_graph)\n",
        "\n",
        "        if self.verbose:\n",
        "            t_grad_static = time.time() - t_grad_static\n",
        "            print('[{}] Computed static part of the gradient in {:5.2f}s.'\n",
        "                  .format(self.__class__.__name__, t_grad_static))\n",
        "\n",
        "        return grad_target_neighbors\n",
        "\n",
        "    def _callback(self, transformation):\n",
        "        \"\"\"Called after each iteration of the optimizer.\n",
        "        Parameters\n",
        "        ----------\n",
        "        transformation : array, shape(n_components, n_features)\n",
        "            The solution computed by the optimizer in this iteration.\n",
        "        \"\"\"\n",
        "        if self.callback is not None:\n",
        "            self.callback(transformation, self.n_iter_)\n",
        "\n",
        "        self.n_iter_ += 1\n",
        "\n",
        "    def _loss_grad_lbfgs(self, transformation, X, y, classes, target_neighbors,\n",
        "                         grad_static, use_sparse):\n",
        "        \"\"\"Compute the loss and the loss gradient w.r.t. ``transformation``.\n",
        "        Parameters\n",
        "        ----------\n",
        "        transformation : array, shape (n_components * n_features,)\n",
        "            The current (flattened) linear transformation.\n",
        "        X : array, shape (n_samples, n_features)\n",
        "            The training samples.\n",
        "        y : array, shape (n_samples,)\n",
        "            The corresponding training labels.\n",
        "        classes : array, shape (n_classes,)\n",
        "            The non-singleton classes, encoded as integers in [0, n_classes).\n",
        "        target_neighbors : array, shape (n_samples, n_neighbors)\n",
        "            The target neighbors of each sample.\n",
        "        grad_static : array, shape (n_features, n_features)\n",
        "            The (weighted) gradient component caused by target neighbors,\n",
        "            that stays fixed throughout the algorithm.\n",
        "        use_sparse : bool\n",
        "            Whether to use a sparse matrix to store the impostors.\n",
        "        Returns\n",
        "        -------\n",
        "        loss: float\n",
        "            The loss based on the given transformation.\n",
        "        grad: array, shape (n_components * n_features,)\n",
        "            The new (flattened) gradient of the loss.\n",
        "        \"\"\"\n",
        "\n",
        "        n_samples, n_features = X.shape\n",
        "        transformation = transformation.reshape(-1, n_features)\n",
        "        self.components_ = transformation\n",
        "\n",
        "        if self.n_iter_ == 0:\n",
        "            self.n_iter_ += 1\n",
        "            if self.verbose:\n",
        "                header_fields = ['Iteration', 'Objective Value',\n",
        "                                 '#Active Triplets', 'Time(s)']\n",
        "                header_fmt = '{:>10} {:>20} {:>20} {:>10}'\n",
        "                header = header_fmt.format(*header_fields)\n",
        "                cls_name = self.__class__.__name__\n",
        "                print('[{}]'.format(cls_name))\n",
        "                print('[{}] {}\\n[{}] {}'.format(cls_name, header,\n",
        "                                                cls_name, '-' * len(header)))\n",
        "\n",
        "        t_funcall = time.time()\n",
        "        X_embedded = self._transform_without_checks(X)\n",
        "\n",
        "        # Compute (squared) distances to the target neighbors\n",
        "        n_neighbors = target_neighbors.shape[1]\n",
        "        dist_tn = np.zeros((n_samples, n_neighbors))\n",
        "        for k in range(n_neighbors):\n",
        "            dist_tn[:, k] = row_norms(X_embedded -\n",
        "                                      X_embedded[target_neighbors[:, k]],\n",
        "                                      squared=True)\n",
        "\n",
        "        # Add the margin to all (squared) distances to target neighbors\n",
        "        dist_tn += 1\n",
        "\n",
        "        # Find the impostors and compute (squared) distances to them\n",
        "        impostors_graph = self._find_impostors(\n",
        "            X_embedded, y, classes, dist_tn[:, -1], use_sparse)\n",
        "\n",
        "        # Compute the push loss and its gradient\n",
        "        loss, grad_new, n_active_triplets = \\\n",
        "            _compute_push_loss(X, target_neighbors, dist_tn, impostors_graph)\n",
        "\n",
        "        # Compute the total gradient\n",
        "        grad = np.dot(transformation, grad_static + grad_new)\n",
        "        grad *= 2\n",
        "\n",
        "        # Add the (weighted) pull loss to the total loss\n",
        "        metric = np.dot(transformation.T, transformation)\n",
        "        loss += np.dot(grad_static.ravel(), metric.ravel())\n",
        "\n",
        "        if self.verbose:\n",
        "            t_funcall = time.time() - t_funcall\n",
        "            values_fmt = '[{}] {:>10} {:>20.6e} {:>20,} {:>10.2f}'\n",
        "            print(values_fmt.format(self.__class__.__name__, self.n_iter_,\n",
        "                                    loss, n_active_triplets, t_funcall))\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        return loss, grad.ravel()\n",
        "\n",
        "    def _find_impostors(self, X_embedded, y, classes, margin_radii,\n",
        "                        use_sparse=True):\n",
        "        \"\"\"Compute the (sample, impostor) pairs exactly.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X_embedded : array, shape (n_samples, n_components)\n",
        "            An array of transformed samples.\n",
        "        y : array, shape (n_samples,)\n",
        "            The corresponding (possibly encoded) class labels.\n",
        "        classes : array, shape (n_classes,)\n",
        "            The non-singleton classes, encoded as integers in [0, n_classes).\n",
        "        margin_radii : array, shape (n_samples,)\n",
        "            (Squared) distances of samples to their farthest target\n",
        "            neighbors plus margin.\n",
        "        use_sparse : bool, optional (default=True)\n",
        "            Whether to use a sparse matrix to store the (sample, impostor)\n",
        "            pairs.\n",
        "        Returns\n",
        "        -------\n",
        "        impostors_graph : coo_matrix, shape (n_samples, n_samples)\n",
        "            Element (i, j) is the distance between samples i and j if j is an\n",
        "            impostor to i, otherwise zero.\n",
        "        \"\"\"\n",
        "        n_samples = X_embedded.shape[0]\n",
        "\n",
        "        if use_sparse:\n",
        "            # Initialize a sparse (indicator) matrix for impostors storage\n",
        "            impostors_sp = csr_matrix((n_samples, n_samples), dtype=np.int8)\n",
        "            for class_id in classes[:-1]:\n",
        "                ind_in, = np.where(y == class_id)\n",
        "                ind_out, = np.where(y > class_id)\n",
        "\n",
        "                # Split ind_out x ind_in into chunks of a size that fits\n",
        "                # in memory\n",
        "                imp_ind = _find_impostors_blockwise(\n",
        "                    X_embedded[ind_out], X_embedded[ind_in],\n",
        "                    margin_radii[ind_out], margin_radii[ind_in])\n",
        "\n",
        "                if len(imp_ind):\n",
        "                    # sample impostors if they are too many\n",
        "                    if len(imp_ind) > self.max_impostors:\n",
        "                        imp_ind = self.random_state_.choice(\n",
        "                            imp_ind, self.max_impostors, replace=False)\n",
        "\n",
        "                    dims = (len(ind_out), len(ind_in))\n",
        "                    ii, jj = np.unravel_index(imp_ind, shape=dims)\n",
        "                    # Convert indices to refer to the original data matrix\n",
        "                    imp_row = ind_out[ii]\n",
        "                    imp_col = ind_in[jj]\n",
        "                    new_imp = csr_matrix((np.ones(len(imp_row), dtype=np.int8),\n",
        "                                          (imp_row, imp_col)), dtype=np.int8,\n",
        "                                         shape=(n_samples, n_samples))\n",
        "                    impostors_sp = impostors_sp + new_imp\n",
        "\n",
        "            impostors_sp = impostors_sp.tocoo(copy=False)\n",
        "            imp_row = impostors_sp.row\n",
        "            imp_col = impostors_sp.col\n",
        "\n",
        "            # Make sure we do not exceed max_impostors\n",
        "            n_impostors = len(imp_row)\n",
        "            if n_impostors > self.max_impostors:\n",
        "                ind_sampled = self.random_state_.choice(\n",
        "                    n_impostors, self.max_impostors, replace=False)\n",
        "                imp_row = imp_row[ind_sampled]\n",
        "                imp_col = imp_col[ind_sampled]\n",
        "\n",
        "            imp_dist = _paired_distances_blockwise(X_embedded, imp_row,\n",
        "                                                   imp_col)\n",
        "        else:\n",
        "            # Initialize lists for impostors storage\n",
        "            imp_row, imp_col, imp_dist = [], [], []\n",
        "            for class_id in classes[:-1]:\n",
        "                ind_in, = np.where(y == class_id)\n",
        "                ind_out, = np.where(y > class_id)\n",
        "\n",
        "                # Split ind_out x ind_in into chunks of a size that fits in\n",
        "                # memory\n",
        "                imp_ind, dist_batch = _find_impostors_blockwise(\n",
        "                    X_embedded[ind_out], X_embedded[ind_in],\n",
        "                    margin_radii[ind_out], margin_radii[ind_in],\n",
        "                    return_distance=True)\n",
        "\n",
        "                if len(imp_ind):\n",
        "                    # sample impostors if they are too many\n",
        "                    if len(imp_ind) > self.max_impostors:\n",
        "                        ind_sampled = self.random_state_.choice(\n",
        "                            len(imp_ind), self.max_impostors, replace=False)\n",
        "                        imp_ind = imp_ind[ind_sampled]\n",
        "                        dist_batch = dist_batch[ind_sampled]\n",
        "\n",
        "                    dims = (len(ind_out), len(ind_in))\n",
        "                    ii, jj = np.unravel_index(imp_ind, shape=dims)\n",
        "                    # Convert indices to refer to the original data matrix\n",
        "                    imp_row.extend(ind_out[ii])\n",
        "                    imp_col.extend(ind_in[jj])\n",
        "                    imp_dist.extend(dist_batch)\n",
        "\n",
        "            imp_row = np.asarray(imp_row, dtype=np.intp)\n",
        "            imp_col = np.asarray(imp_col, dtype=np.intp)\n",
        "            imp_dist = np.asarray(imp_dist)\n",
        "\n",
        "            # Make sure we do not exceed max_impostors\n",
        "            n_impostors = len(imp_row)\n",
        "            if n_impostors > self.max_impostors:\n",
        "                ind_sampled = self.random_state_.choice(\n",
        "                    n_impostors, self.max_impostors, replace=False)\n",
        "                imp_row = imp_row[ind_sampled]\n",
        "                imp_col = imp_col[ind_sampled]\n",
        "                imp_dist = imp_dist[ind_sampled]\n",
        "\n",
        "        impostors_graph = coo_matrix((imp_dist, (imp_row, imp_col)),\n",
        "                                     shape=(n_samples, n_samples))\n",
        "\n",
        "        return impostors_graph\n",
        "\n",
        "\n",
        "########################\n",
        "# Some core functions #\n",
        "#######################\n",
        "\n",
        "\n",
        "def _select_target_neighbors(X, y, n_neighbors, classes=None, **nn_kwargs):\n",
        "    \"\"\"Find the target neighbors of each data sample.\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array, shape (n_samples, n_features)\n",
        "        The training samples.\n",
        "    y : array, shape (n_samples,)\n",
        "        The corresponding (encoded) training labels.\n",
        "    n_neighbors : int\n",
        "        The number of target neighbors to select for each sample in X.\n",
        "    classes : array, shape (n_classes,), optional (default=None)\n",
        "        The non-singleton classes, encoded as integers in [0, n_classes).\n",
        "        If None (default), they will be inferred from ``y``.\n",
        "    **nn_kwargs : keyword arguments\n",
        "        Parameters to be passed to a :class:`neighbors.NearestNeighbors`\n",
        "        instance except from ``n_neighbors``.\n",
        "    Returns\n",
        "    -------\n",
        "    target_neighbors: array, shape (n_samples, n_neighbors)\n",
        "        The indices of the target neighbors of each sample.\n",
        "    \"\"\"\n",
        "\n",
        "    target_neighbors = np.zeros((X.shape[0], n_neighbors), dtype=np.intp)\n",
        "\n",
        "    nn = NearestNeighbors(n_neighbors=n_neighbors, **nn_kwargs)\n",
        "\n",
        "    if classes is None:\n",
        "        classes = np.unique(y)\n",
        "\n",
        "    for class_id in classes:\n",
        "        ind_class, = np.where(y == class_id)\n",
        "        nn.fit(X[ind_class])\n",
        "        neigh_ind = nn.kneighbors(return_distance=False)\n",
        "        target_neighbors[ind_class] = ind_class[neigh_ind]\n",
        "\n",
        "    return target_neighbors\n",
        "\n",
        "\n",
        "def _find_impostors_blockwise(X_a, X_b, radii_a, radii_b,\n",
        "                              return_distance=False, block_size=8):\n",
        "    \"\"\"Find (sample, impostor) pairs in blocks to avoid large memory usage.\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_a : array, shape (n_samples_a, n_components)\n",
        "        Transformed data samples from class A.\n",
        "    X_b : array, shape (n_samples_b, n_components)\n",
        "        Transformed data samples from class B.\n",
        "    radii_a : array, shape (n_samples_a,)\n",
        "        Squared distances of the samples in ``X_a`` to their margins.\n",
        "    radii_b : array, shape (n_samples_b,)\n",
        "        Squared distances of the samples in ``X_b`` to their margins.\n",
        "    block_size : int, optional (default=8)\n",
        "        The maximum number of mebibytes (MiB) of memory to use at a time for\n",
        "        calculating paired squared distances.\n",
        "    return_distance : bool, optional (default=False)\n",
        "        Whether to return the squared distances to the impostors.\n",
        "    Returns\n",
        "    -------\n",
        "    imp_indices : array, shape (n_impostors,)\n",
        "        Unraveled indices of (sample, impostor) pairs referring to a matrix\n",
        "        of shape (n_samples_a, n_samples_b).\n",
        "    imp_distances : array, shape (n_impostors,), optional\n",
        "        imp_distances[i] is the squared distance between samples imp_row[i] and\n",
        "        imp_col[i], where\n",
        "        imp_row, imp_col = np.unravel_index(imp_indices, shape=(n_samples_a,\n",
        "        n_samples_b))\n",
        "    \"\"\"\n",
        "\n",
        "    n_samples_a = X_a.shape[0]\n",
        "    bytes_per_row = X_b.shape[0] * X_b.itemsize\n",
        "    block_n_rows = int(block_size*1024*1024 // bytes_per_row)\n",
        "\n",
        "    imp_indices, imp_distances = [], []\n",
        "\n",
        "    # X_b squared norm stays constant, so pre-compute it to get a speed-up\n",
        "    X_b_norm_squared = row_norms(X_b, squared=True)[np.newaxis, :]\n",
        "    for chunk in gen_batches(n_samples_a, block_n_rows):\n",
        "        # The function `sklearn.metrics.pairwise.euclidean_distances` would\n",
        "        # add an extra ~8% time of computation due to input validation on\n",
        "        # every chunk and another ~8% due to clipping of negative values.\n",
        "        distances_ab = _euclidean_distances_without_checks(\n",
        "            X_a[chunk], X_b, squared=True, Y_norm_squared=X_b_norm_squared,\n",
        "            clip=False)\n",
        "\n",
        "        ind_b, = np.where((distances_ab < radii_a[chunk, None]).ravel())\n",
        "        ind_a, = np.where((distances_ab < radii_b[None, :]).ravel())\n",
        "        ind = np.unique(np.concatenate((ind_a, ind_b)))\n",
        "\n",
        "        if len(ind):\n",
        "            ind_plus_offset = ind + chunk.start * X_b.shape[0]\n",
        "            imp_indices.extend(ind_plus_offset)\n",
        "\n",
        "            if return_distance:\n",
        "                # We only need to do clipping if we return the distances.\n",
        "                distances_chunk = distances_ab.ravel()[ind]\n",
        "                # Clip only the indexed (unique) distances\n",
        "                np.maximum(distances_chunk, 0, out=distances_chunk)\n",
        "                imp_distances.extend(distances_chunk)\n",
        "\n",
        "    imp_indices = np.asarray(imp_indices)\n",
        "\n",
        "    if return_distance:\n",
        "        return imp_indices, np.asarray(imp_distances)\n",
        "    else:\n",
        "        return imp_indices\n",
        "\n",
        "\n",
        "def _compute_push_loss(X, target_neighbors, dist_tn, impostors_graph):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array, shape (n_samples, n_features)\n",
        "        The training input samples.\n",
        "    target_neighbors : array, shape (n_samples, n_neighbors)\n",
        "        Indices of target neighbors of each sample.\n",
        "    dist_tn : array, shape (n_samples, n_neighbors)\n",
        "        (Squared) distances of samples to their target neighbors.\n",
        "    impostors_graph : coo_matrix, shape (n_samples, n_samples)\n",
        "        Element (i, j) is the distance between sample i and j if j is an\n",
        "        impostor to i, otherwise zero.\n",
        "    Returns\n",
        "    -------\n",
        "    loss : float\n",
        "        The push loss caused by the given target neighbors and impostors.\n",
        "    grad : array, shape (n_features, n_features)\n",
        "        The gradient of the push loss.\n",
        "    n_active_triplets : int\n",
        "        The number of active triplet constraints.\n",
        "    \"\"\"\n",
        "\n",
        "    n_samples, n_neighbors = dist_tn.shape\n",
        "    imp_row = impostors_graph.row\n",
        "    imp_col = impostors_graph.col\n",
        "    dist_impostors = impostors_graph.data\n",
        "\n",
        "    loss = 0\n",
        "    shape = (n_samples, n_samples)\n",
        "    A0 = csr_matrix(shape)\n",
        "    sample_range = range(n_samples)\n",
        "    n_active_triplets = 0\n",
        "    for k in range(n_neighbors - 1, -1, -1):\n",
        "        loss1 = np.maximum(dist_tn[imp_row, k] - dist_impostors, 0)\n",
        "        ac, = np.where(loss1 > 0)\n",
        "        n_active_triplets += len(ac)\n",
        "        A1 = csr_matrix((2 * loss1[ac], (imp_row[ac], imp_col[ac])), shape)\n",
        "\n",
        "        loss2 = np.maximum(dist_tn[imp_col, k] - dist_impostors, 0)\n",
        "        ac, = np.where(loss2 > 0)\n",
        "        n_active_triplets += len(ac)\n",
        "        A2 = csc_matrix((2 * loss2[ac], (imp_row[ac], imp_col[ac])), shape)\n",
        "\n",
        "        val = (A1.sum(1).ravel() + A2.sum(0)).getA1()\n",
        "        A3 = csr_matrix((val, (sample_range, target_neighbors[:, k])), shape)\n",
        "        A0 = A0 - A1 - A2 + A3\n",
        "        loss += np.dot(loss1, loss1) + np.dot(loss2, loss2)\n",
        "\n",
        "    grad = _sum_weighted_outer_differences(X, A0)\n",
        "\n",
        "    return loss, grad, n_active_triplets\n",
        "\n",
        "\n",
        "##########################\n",
        "# Some helper functions #\n",
        "#########################\n",
        "\n",
        "def _paired_distances_blockwise(X, ind_a, ind_b, squared=True, block_size=8):\n",
        "    \"\"\"Equivalent to row_norms(X[ind_a] - X[ind_b], squared=squared).\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array, shape (n_samples, n_features)\n",
        "        An array of data samples.\n",
        "    ind_a : array, shape (n_indices,)\n",
        "        An array of sample indices.\n",
        "    ind_b : array, shape (n_indices,)\n",
        "        Another array of sample indices.\n",
        "    squared : bool (default=True)\n",
        "        Whether to return the squared distances.\n",
        "    block_size : int, optional (default=8)\n",
        "        The maximum number of mebibytes (MiB) of memory to use at a time for\n",
        "        calculating paired (squared) distances.\n",
        "    Returns\n",
        "    -------\n",
        "    distances: array, shape (n_indices,)\n",
        "        An array of pairwise, optionally squared, distances.\n",
        "    \"\"\"\n",
        "\n",
        "    bytes_per_row = X.shape[1] * X.itemsize\n",
        "    batch_size = int(block_size*1024*1024 // bytes_per_row)\n",
        "\n",
        "    n_pairs = len(ind_a)\n",
        "    distances = np.zeros(n_pairs)\n",
        "    for chunk in gen_batches(n_pairs, batch_size):\n",
        "        distances[chunk] = row_norms(X[ind_a[chunk]] - X[ind_b[chunk]], True)\n",
        "\n",
        "    return distances if squared else np.sqrt(distances, out=distances)\n",
        "\n",
        "\n",
        "def _sum_weighted_outer_differences(X, weights):\n",
        "    \"\"\"Compute the sum of weighted outer pairwise differences.\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array, shape (n_samples, n_features)\n",
        "        An array of data samples.\n",
        "    weights : csr_matrix, shape (n_samples, n_samples)\n",
        "        A sparse weights matrix.\n",
        "    Returns\n",
        "    -------\n",
        "    sum_weighted_outer_diffs : array, shape (n_features, n_features)\n",
        "        The sum of all outer weighted differences.\n",
        "    \"\"\"\n",
        "\n",
        "    weights_sym = weights + weights.T\n",
        "    diagonal = weights_sym.sum(1).getA()\n",
        "    laplacian_dot_X = diagonal * X - safe_sparse_dot(weights_sym, X,\n",
        "                                                     dense_output=True)\n",
        "    result = np.dot(X.T, laplacian_dot_X)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def _check_scalar(x, name, target_type, min_val=None, max_val=None):\n",
        "    \"\"\"Validate scalar parameters type and value.\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : object\n",
        "        The scalar parameter to validate.\n",
        "    name : str\n",
        "        The name of the parameter to be printed in error messages.\n",
        "    target_type : type or tuple\n",
        "        Acceptable data types for the parameter.\n",
        "    min_val : float or int, optional (default=None)\n",
        "        The minimum value value the parameter can take. If None (default) it\n",
        "        is implied that the parameter does not have a lower bound.\n",
        "    max_val: float or int, optional (default=None)\n",
        "        The maximum valid value the parameter can take. If None (default) it\n",
        "        is implied that the parameter does not have an upper bound.\n",
        "    Raises\n",
        "    -------\n",
        "    TypeError\n",
        "        If the parameter's type does not match the desired type.\n",
        "    ValueError\n",
        "        If the parameter's value violates the given bounds.\n",
        "    \"\"\"\n",
        "\n",
        "    if not isinstance(x, target_type):\n",
        "        raise TypeError('`{}` must be an instance of {}, not {}.'\n",
        "                        .format(name, target_type, type(x)))\n",
        "\n",
        "    if min_val is not None and x < min_val:\n",
        "        raise ValueError('`{}`= {}, must be >= {}.'.format(name, x, min_val))\n",
        "\n",
        "    if max_val is not None and x > max_val:\n",
        "        raise ValueError('`{}`= {}, must be <= {}.'.format(name, x, max_val))\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "# Convenience function to construct the trivial LMNN - KNN pipeline #\n",
        "#####################################################################\n",
        "\n",
        "def make_lmnn_pipeline(\n",
        "        n_neighbors=3, n_components=None, init='pca', warm_start=False,\n",
        "        max_impostors=500000, neighbors_params=None, weight_push_loss=0.5,\n",
        "        impostor_store='auto', max_iter=50, tol=1e-5, callback=None,\n",
        "        store_opt_result=False, verbose=0, random_state=None, n_jobs=1,\n",
        "        n_neighbors_predict=None, weights='uniform', algorithm='auto',\n",
        "        leaf_size=30, n_jobs_predict=None, **kwargs):\n",
        "    \"\"\"Constructs a LargeMarginNearestNeighbor - KNeighborsClassifier pipeline.\n",
        "    See LargeMarginNearestNeighbor module documentation for details.\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_neighbors_predict : int, optional (default=None)\n",
        "        The number of neighbors to use during prediction. If None (default)\n",
        "        the value of ``n_neighbors`` used to train the model is used.\n",
        "    weights : str or callable, optional (default = 'uniform')\n",
        "        weight function used in prediction.  Possible values:\n",
        "        - 'uniform' : uniform weights.  All points in each neighborhood\n",
        "          are weighted equally.\n",
        "        - 'distance' : weight points by the inverse of their distance.\n",
        "          in this case, closer neighbors of a query point will have a\n",
        "          greater influence than neighbors which are further away.\n",
        "        - [callable] : a user-defined function which accepts an\n",
        "          array of distances, and returns an array of the same shape\n",
        "          containing the weights.\n",
        "    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional\n",
        "        Algorithm used to compute the nearest neighbors:\n",
        "        - 'ball_tree' will use :class:`BallTree`\n",
        "        - 'kd_tree' will use :class:`KDTree`\n",
        "        - 'brute' will use a brute-force search.\n",
        "        - 'auto' will attempt to decide the most appropriate algorithm\n",
        "          based on the values passed to :meth:`fit` method.\n",
        "        Note: fitting on sparse input will override the setting of\n",
        "        this parameter, using brute force.\n",
        "    leaf_size : int, optional (default = 30)\n",
        "        Leaf size passed to BallTree or KDTree.  This can affect the\n",
        "        speed of the construction and query, as well as the memory\n",
        "        required to store the tree.  The optimal value depends on the\n",
        "        nature of the problem.\n",
        "    n_jobs_predict : int, optional (default=None)\n",
        "        The number of parallel jobs to run for neighbors search during\n",
        "        prediction. If None (default), then the value of ``n_jobs`` is used.\n",
        "    memory : None, str or object with the joblib.Memory interface, optional\n",
        "        Used to cache the fitted transformers of the pipeline. By default,\n",
        "        no caching is performed. If a string is given, it is the path to\n",
        "        the caching directory. Enabling caching triggers a clone of\n",
        "        the transformers before fitting. Therefore, the transformer\n",
        "        instance given to the pipeline cannot be inspected\n",
        "        directly. Use the attribute ``named_steps`` or ``steps`` to\n",
        "        inspect estimators within the pipeline. Caching the\n",
        "        transformers is advantageous when fitting is time consuming.\n",
        "    Returns\n",
        "    -------\n",
        "    lmnn_pipe : Pipeline\n",
        "        A Pipeline instance with two steps: a ``LargeMarginNearestNeighbor``\n",
        "        instance that is used to fit the model and a ``KNeighborsClassifier``\n",
        "        instance that is used for prediction.\n",
        "    Examples\n",
        "    --------\n",
        "    >>> from pylmnn import make_lmnn_pipeline\n",
        "    >>> from sklearn.datasets import load_iris\n",
        "    >>> from sklearn.model_selection import train_test_split\n",
        "    >>> X, y = load_iris(return_X_y=True)\n",
        "    >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "    ... stratify=y, test_size=0.7, random_state=42)\n",
        "    >>> lmnn_pipe = make_lmnn_pipeline(n_neighbors=3, n_neighbors_predict=3,\n",
        "    ... random_state=42)\n",
        "    >>> lmnn_pipe.fit(X_train, y_train) # doctest: +ELLIPSIS\n",
        "    Pipeline(...)\n",
        "    >>> print(lmnn_pipe.score(X_test, y_test))\n",
        "    0.971428571429\n",
        "    \"\"\"\n",
        "\n",
        "    memory = kwargs.pop('memory', None)\n",
        "    if kwargs:\n",
        "        raise TypeError('Unknown keyword arguments: \"{}\"'\n",
        "                        .format(list(kwargs.keys())[0]))\n",
        "\n",
        "    lmnn = LargeMarginNearestNeighbor(\n",
        "        n_neighbors=n_neighbors, n_components=n_components, init=init,\n",
        "        warm_start=warm_start, max_impostors=max_impostors,\n",
        "        neighbors_params=neighbors_params, weight_push_loss=weight_push_loss,\n",
        "        impostor_store=impostor_store, max_iter=max_iter, tol=tol,\n",
        "        callback=callback, store_opt_result=store_opt_result, verbose=verbose,\n",
        "        random_state=random_state, n_jobs=n_jobs)\n",
        "\n",
        "    if n_neighbors_predict is None:\n",
        "        n_neighbors_predict = n_neighbors\n",
        "\n",
        "    if n_jobs_predict is None:\n",
        "        n_jobs_predict = n_jobs\n",
        "\n",
        "    knn = KNeighborsClassifier(\n",
        "        n_neighbors=n_neighbors_predict, weights=weights, algorithm=algorithm,\n",
        "        leaf_size=leaf_size, n_jobs=n_jobs_predict)\n",
        "\n",
        "    return Pipeline([('lmnn', lmnn), ('knn', knn)], memory=memory)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "Kc-hoveerEPi",
        "outputId": "19452fcd-5cca-482c-87ba-fe8506d8b4fc"
      },
      "source": [
        "#@title Compare KNN and LMNN with iris data\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def myknn_predict(k, x_train, y_train, x_test):\n",
        "  labelEst = np.zeros(x_test.shape[0])\n",
        "  for i, x in enumerate(x_test):\n",
        "    dist            = norm(x_train - x, axis=1)\n",
        "    dist_sort_index = argsort(dist)\n",
        "    neigh_labels    = y_train[dist_sort_index[0:k]]\n",
        "    labelEst[i]     = argmax(bincount(neigh_labels))\n",
        "  return labelEst\n",
        "\n",
        "# Load a data set\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split in training and testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, stratify=y, random_state=42)\n",
        "\n",
        "# Set up the hyperparameters\n",
        "k_train, k_test, n_components, max_iter = 3, 3, X.shape[1], 180\n",
        "\n",
        "\n",
        "y_test_predicted = myknn_predict(k_train, X_train, y_train, X_test)\n",
        "print('KNN accuracy on test set of {} points:',(1-norm(y_test_predicted - y_test, 0)/X_test.shape[0]))\n",
        "\n",
        "# Instantiate the metric learner\n",
        "lmnn = LargeMarginNearestNeighbor(n_neighbors=k_train, max_iter=max_iter,verbose=1, n_components=n_components)\n",
        "\n",
        "# Train the metric learner\n",
        "lmnn.fit(X_train, y_train)\n",
        "\n",
        "# Fit the nearest neighbors classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=k_test)\n",
        "knn.fit(lmnn.transform(X_train), y_train)\n",
        "\n",
        "# Compute the k-nearest neighbor test accuracy after applying the learned transformation\n",
        "lmnn_acc = knn.score(lmnn.transform(X_test), y_test)\n",
        "print('LMNN accuracy on test set of {} points: {:.4f}'.format(X_test.shape[0], lmnn_acc))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN accuracy on test set of {} points: 0.9333333333333333\n",
            "[LargeMarginNearestNeighbor] Finding principal components...\n",
            "[LargeMarginNearestNeighbor] Found principal components in  0.00s.\n",
            "[LargeMarginNearestNeighbor] Finding the target neighbors...\n",
            "[LargeMarginNearestNeighbor] Found the target neighbors in  0.01s.\n",
            "[LargeMarginNearestNeighbor] Computing static part of the gradient...\n",
            "[LargeMarginNearestNeighbor] Computed static part of the gradient in  0.00s.\n",
            "[LargeMarginNearestNeighbor]\n",
            "[LargeMarginNearestNeighbor]  Iteration      Objective Value     #Active Triplets    Time(s)\n",
            "[LargeMarginNearestNeighbor] ---------------------------------------------------------------\n",
            "[LargeMarginNearestNeighbor]          1         1.769061e+02                  232       0.01\n",
            "[LargeMarginNearestNeighbor]          1         1.379797e+02                  126       0.01\n",
            "[LargeMarginNearestNeighbor]          2         1.233415e+02                  146       0.02\n",
            "[LargeMarginNearestNeighbor]          3         1.134961e+02                  170       0.01\n",
            "[LargeMarginNearestNeighbor]          4         1.071357e+02                  162       0.01\n",
            "[LargeMarginNearestNeighbor]          5         1.050290e+02                  150       0.01\n",
            "[LargeMarginNearestNeighbor]          6         1.046654e+02                  147       0.01\n",
            "[LargeMarginNearestNeighbor]          7         1.042488e+02                  145       0.01\n",
            "[LargeMarginNearestNeighbor]          8         1.034733e+02                  145       0.01\n",
            "[LargeMarginNearestNeighbor]          9         1.035306e+02                  151       0.01\n",
            "[LargeMarginNearestNeighbor]          9         1.033786e+02                  148       0.01\n",
            "[LargeMarginNearestNeighbor]         10         1.033023e+02                  148       0.01\n",
            "[LargeMarginNearestNeighbor]         11         1.032788e+02                  149       0.01\n",
            "[LargeMarginNearestNeighbor]         12         1.032701e+02                  148       0.01\n",
            "[LargeMarginNearestNeighbor]         13         1.032688e+02                  148       0.01\n",
            "[LargeMarginNearestNeighbor]         14         1.032686e+02                  148       0.01\n",
            "[LargeMarginNearestNeighbor] Training took     0.23s.\n",
            "LMNN accuracy on test set of 105 points: 0.9714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "cellView": "form",
        "id": "bhgTkAh7tNH3",
        "outputId": "5d782f13-53d1-440b-f54c-3adbd3531ad7"
      },
      "source": [
        "#@title Draw a comparison plot of the test data before and after applying the learned transformation\n",
        "\n",
        "plot_comparison(lmnn.components_, X_test, y_test, dim_pref=3)\n",
        "plt.show()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "DpUi_VGBAtCC",
        "outputId": "ac0fbabf-39e8-4430-8128-5de148bdfa44"
      },
      "source": [
        "\n",
        "\n",
        "#@title Compare KNN and LMNN with olivetti_faces\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_olivetti_faces\n",
        "\n",
        "\n",
        "data = fetch_olivetti_faces()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "# Train with no transformation (euclidean metric)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Test with euclidean metric\n",
        "acc = knn.score(X_test, y_test)\n",
        "\n",
        "print('KNN accuracy on test set: {}%'.format(acc))\n",
        "\n",
        "\n",
        "# LMNN is no longer a classifier but a transformer\n",
        "lmnn = LargeMarginNearestNeighbor(n_neighbors=3, verbose=1)\n",
        "lmnn.fit(X_train, y_train)\n",
        "\n",
        "# Train with transformation learned by LMNN\n",
        "knn.fit(lmnn.transform(X_train), y_train)\n",
        "\n",
        "# Test with transformation learned by LMNN\n",
        "acc = knn.score(lmnn.transform(X_test), y_test)\n",
        "\n",
        "print('LMNN accuracy on test set: {}%'.format(acc))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN accuracy on test set: 0.8583333333333333%\n",
            "[LargeMarginNearestNeighbor] Finding principal components...\n",
            "[LargeMarginNearestNeighbor] Found principal components in  0.22s.\n",
            "[LargeMarginNearestNeighbor] Finding the target neighbors...\n",
            "[LargeMarginNearestNeighbor] Found the target neighbors in  0.05s.\n",
            "[LargeMarginNearestNeighbor] Computing static part of the gradient...\n",
            "[LargeMarginNearestNeighbor] Computed static part of the gradient in  0.41s.\n",
            "[LargeMarginNearestNeighbor]\n",
            "[LargeMarginNearestNeighbor]  Iteration      Objective Value     #Active Triplets    Time(s)\n",
            "[LargeMarginNearestNeighbor] ---------------------------------------------------------------\n",
            "[LargeMarginNearestNeighbor]          1         2.255708e+06                3,737       1.22\n",
            "[LargeMarginNearestNeighbor]          1         3.039490e+05                1,686       1.18\n",
            "[LargeMarginNearestNeighbor]          2         1.507605e+05                1,132       1.20\n",
            "[LargeMarginNearestNeighbor]          3         1.056940e+05                  871       1.27\n",
            "[LargeMarginNearestNeighbor]          4         6.088940e+04                  522       1.28\n",
            "[LargeMarginNearestNeighbor]          5         4.375553e+04                  319       1.25\n",
            "[LargeMarginNearestNeighbor]          6         3.540135e+04                  174       1.21\n",
            "[LargeMarginNearestNeighbor]          7         3.242531e+04                   92       1.23\n",
            "[LargeMarginNearestNeighbor]          8         3.067151e+04                   28       1.20\n",
            "[LargeMarginNearestNeighbor]          9         2.817301e+04                    4       1.14\n",
            "[LargeMarginNearestNeighbor]         10         3.694714e+04                  337       1.13\n",
            "[LargeMarginNearestNeighbor]         10         2.144376e+04                    4       1.23\n"
          ]
        }
      ]
    }
  ]
}