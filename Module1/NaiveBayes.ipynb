{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NaiveBayes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNg/8fWQmjMSYPrh+iUPOJF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yexf308/MAT592/blob/main/Module1/NaiveBayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wh6v1EOqWwCE"
      },
      "outputs": [],
      "source": [
        "%pylab inline \n",
        "import numpy.linalg as LA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\def\\m#1{\\mathbf{#1}}$\n",
        "$\\def\\mm#1{\\boldsymbol{#1}}$\n",
        "$\\def\\mb#1{\\mathbb{#1}}$\n",
        "$\\def\\c#1{\\mathcal{#1}}$\n",
        "\n",
        "Refer to PML and [StatQuest](https://statquest.gumroad.com/l/wvtmc). \n",
        "\n",
        "# Naive Bayes Classifier (NBC)\n",
        "\n",
        "Naive Bayes assumption: features are **conditionally independent** given the class label. i.e.,\n",
        "the probability can be expressed by a class conditional density, \n",
        "$$ p(\\m{x}|y=c,\\mm{\\theta}) = \\Pi_{d=1}^D p(x_d|y=c, \\theta_{dc})\n",
        "$$\n",
        "where $\\theta_{dc}$ are the parameters for the class conditional density for class $c$ and feature $d$. \n",
        "\n",
        "### Features: \n",
        "- \"naive\": this assumption is very naive! We don't expect **features** to be conditional independent on the **class label**. It is not true in reality, but it still performs well. \n",
        "\n",
        "- \"simple\": this model is simple. Note $y\\in \\{1,\\dots, C\\}$.  It only has $O(CD)$ parameters for $C$ classes and $D$ features, compared with $O(CD^2)$ in QDA. It may not likely overfit!\n",
        "\n",
        "- Connection to QDA: later! Naiver Bayes is much more broader than QDA since conditional density is not necessarily the Gaussian distribution. \n",
        "\n",
        "The posterior distribution over the class labels is \n",
        "$$\n",
        "p(y=c|\\m{x}, \\theta) = \\frac{p(y=c|\\mm{\\pi}) p(\\m{x}|y=c,\\mm{\\theta}) }{\\sum_{c'}p(y=c'|\\mm{\\pi}) p(\\m{x}|y=c',\\mm{\\theta})} = \\frac{p(y=c|\\mm\\pi)  \\Pi_{d=1}^D p(x_d|y=c, \\theta_{dc})}{\\sum_{c'}p(y=c'|\\mm\\pi) \\Pi_{d=1}^D p(x_d|y=c', \\theta_{dc'})}\n",
        "$$\n",
        "where $\\mm\\pi$ is the prior probability of class $c$ and $\\theta=\\{\\mm\\pi, {\\theta_{dc}}\\}$ are the parameters. \n"
      ],
      "metadata": {
        "id": "CnsCEkJZW3I7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Examples\n",
        "\n",
        "- $x_d\\in\\{0,1\\}$: Binary feature. \n",
        "   - Model: **multivariate Bernoulli naive Bayes**.\n",
        "   - Bernoulli distribution: $ p(\\m{x}|y=c,\\mm{\\theta}) = \\Pi_{d=1}^D \\text{Ber}(x_d|\\theta_{dc})$, where $\\theta_{dc} = p(x_d = 1 |y=c)$. Note $1-\\theta_{dc} = p(x_d = 0 |y=c)$.\n",
        "\n",
        "- $x_d\\in\\{1, \\dots, K\\}$: Categorical feature.\n",
        "   - Model:  **multivariate Categorical naive Bayes**.\n",
        "   - Categorical distribution: $ p(\\m{x}|y=c,\\mm{\\theta}) = \\Pi_{d=1}^D \\text{Cat}(x_d|\\mm{\\theta}_{dc})$ where $\\theta_{dck}=p(x_d=k | y=c)$. Note $\\sum_{k=1}^K \\theta_{dck}=1$. \n",
        "\n",
        "\n",
        "- $x_d\\in \\mb{R}$: Real-valued feature.\n",
        "   - Model: **GDA with diagonal variance**.\n",
        "   - Univariate Gaussian distribution: $ p(\\m{x}|y=c,\\mm{\\theta})=\\Pi_{d=1}^D \\c{N}(x_d| \\mu_{dc}, \\sigma_{dc}^2)$, where $\\mu_{dc}$ is the mean of feature $d$ when the class label is $c$ and $\\sigma_{dc}^2$ is its variance. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NHkea_lgDjAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model fitting \n",
        "Fit a naive Bayes classifier using MLE. Note $\\c{D}=\\{\\m{x}^{(i)}, y^{(i)}\\}_{i=1}^N$. \n",
        "\\begin{align}\n",
        "p(\\c{D}|\\mm{\\theta}) &= \\Pi_{i=1}^N \\text{Cat}(y^{(i)}|\\mm{\\pi}) \\Pi_{d=1}^D p(x_d^{(i)}|y^{(i)},\\mm{\\theta}_d) \\\\\n",
        "&= \\Pi_{i=1}^N \\text{Cat}(y^{(i)}|\\mm{\\pi}) \\Pi_{d=1}^D \\Pi_{c=1}^C p(x_d^{(i)}|\\mm\\theta_{dc})^{\\mb{1}_{(y^{(i)}=c)}}  \n",
        "\\end{align}\n",
        "Then the log-likelihood is given \n",
        "\\begin{align}\n",
        "\\log p(\\c{D}|\\mm{\\theta}) = \\left[\\sum_{i=1}^N \\sum_{c=1}^C \\mb{1}_{(y^{(i)}=c)}\\log \\pi_c \\right] +\\sum_{c=1}^C \\sum_{d=1}^D \\left[\\sum_{i: y^{(i)}=c}\\log p(x_d^{(i)}|\\mm{\\theta}_{dc})\\right]\n",
        "\\end{align}\n",
        "\n",
        "Decomposes into a term for $\\mm{\\pi}$, $CD$ terms for each $\\mm\\theta_{dc}$:\n",
        "\\begin{align}\n",
        "\\log p(\\c{D}|\\mm{\\theta}) = \\log p(\\c{D}_y|\\mm{\\pi}) + \\sum_c\\sum_d \\log p(\\c{D}_{dc}|\\mm{\\theta}_{dc})\n",
        "\\end{align}\n",
        "\n",
        "where $\\c{D}_y = \\{y^{(i)}: i =1:N\\}$ are all the labels and $\\c{D}_{dc} = \\{x_d^{(i)}: y^{(i)}=c\\}$ are all the values of feature $d$ for examples from class $c$. \n",
        "\n",
        "\n",
        "\n",
        "- The MLE for $\\mm{\\pi}$ is the vector of emirical counts $\\hat{\\pi}_c = \\frac{N_c}{N}$. \n",
        "\n",
        "- The MLE for $\\mm{\\theta}_{dc}$ depend on the class conditional density for features $p(\\m{x}|y=c, \\mm{\\theta})$.\n",
        "\n",
        "  - In Categorial feature, the MLE is given as \n",
        "    \\begin{align}\n",
        "    \\hat{\\theta}_{dck} = \\frac{N_{dck}}{\\sum_{k'=1}^K N_{dck'}}= \\frac{N_{dck}}{N_c}\n",
        "    \\end{align}\n",
        "where $N_{dck} = \\sum_{i=1}^N \\mb{1}_{(x^{(i)}_d=k, y^{(i)}=c)}$ is the number of times that feature $d$ had value $k$ in examples of class $c$. \n",
        "\n",
        "  - In Binary feature, the categorical distribution becomes the Bernoulli, the MLE is given as \n",
        "  \\begin{align}\n",
        "   \\hat{\\theta}_{dc} = \\frac{N_{dc}}{N_c}\n",
        "  \\end{align}\n",
        "  which is the empirical fraction of times that feature $d$ on in examples of class $c$. \n",
        "\n",
        "  - In real-valued feature, use Gaussian distribution. Similar as QDA, \n",
        "  \\begin{align}\n",
        "  &\\hat\\mu_{dc} = \\frac{1}{N_c} \\sum_{i: y^{(i)}=c} x_d^{(i)} \\\\\n",
        "  &\\hat\\sigma^2_{dc} = \\frac{1}{N_c} \\sum_{i: y^{(i)}=c}(x^{(i)}_d - \\hat\\mu_{dc})^2\n",
        "  \\end{align}\n",
        "\n",
        "So NBC is very efficient and simple. \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jGKh_b-dLW4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 1: Spam email "
      ],
      "metadata": {
        "id": "60StSkAlSjay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connection to logistic regression\n",
        "Assime all features are discrete and have $K$ states, i.e., $x_d=\\{1,2,\\dots, K\\}$. \n",
        "\n",
        "Define $x_{dk}=\\mb{1}_{(x_d=k)}$, then $\\m{x}_d$ is the one-hot vector of feature $d$. Then the class conditional density can be written as follows: \n",
        "\\begin{align}\n",
        "p(\\m{x}|y=c, \\mm{\\theta})=\\Pi_{d=1}^D \\text{Cat}(x_d |y=c, \\mm{\\theta}) = \\Pi_{d=1}^D\\Pi_{k=1}^K \\theta_{dck}^{x_{dk}}\n",
        "\\end{align}\n",
        "Hence the posterior over classes is given by\n",
        "\\begin{align}\n",
        "p(y=c | \\m{x}, \\mm \\theta) &= \\frac{\\pi_c\\Pi_d\\Pi_k  \\theta_{dck}^{x_{dk}} }{\\sum_{c'} \\pi_{c'}\\Pi_d\\Pi_k  \\theta_{dc'k}^{x_{dk}}} \\\\\n",
        "& = \\frac{\\exp(\\log \\pi_c +\\sum_d \\sum_k x_{dk}\\log \\theta_{dck})}{\\sum_{c'}\\exp(\\log \\pi_{c'} +\\sum_d \\sum_k x_{dk}\\log \\theta_{dc'k})} \\\\\n",
        "&=\\frac{\\exp(\\beta_c^\\top \\m{x}+\\gamma_c)}{\\sum_{c'}\\exp(\\beta_{c'}^\\top \\m{x}+\\gamma_{c'})}\n",
        "\\end{align}\n",
        "with the suitably $\\beta_c$ and $\\gamma_c$. This has exactly the same form as multinomial logistic regression with softmax output. \n",
        "\n",
        "**Note:** the result holds for arbitrary feature distributions in the exponential family.\n",
        "\n",
        "**Difference:**\n",
        "- In Naive Bayes, we maximize the joint likelihood $p(\\c{D}|\\mm\\theta)= \\Pi_{i=1}^N p(\\m{x}^{(i)}, y^{(i)}|\\mm\\theta)$. \n",
        "\n",
        "- In logistic regression, we maximize the conditional likelihood $\\Pi_{i=1}^N p(y^{(i)}|\\m{x}^{(i)}, \\mm\\theta)$.\n",
        "\n",
        "So both methods will give different results. \n",
        "\n"
      ],
      "metadata": {
        "id": "ShC2A-os1nXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generative classifier vs Discriminative classifier\n",
        "\n",
        "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/GvsD.png?raw=true\" width=\"400\" />\n",
        "\n",
        "\n",
        "### Generative classifier\n",
        "Def: A model of the form $p(\\m{x}, y) = p(y)p(\\m{x}|y)$. It can use each class $y$ to generate examples $\\m{x}$. \n",
        "\n",
        "Examples: \n",
        " - LDA and QDA.\n",
        " - Naive Bayes.\n",
        " - Gaussian mixture and other mixture models\n",
        " - Variational autoencoder\n",
        " - Generative adversarial network\n",
        "\n",
        "Advantages:\n",
        "\n",
        "- **Easy to fit.** From LDA and Naive Bayes, we fit classifers by counting and averaging. We don't need solve a convex or nonconvex optimization problems numerically in the discriminative classifier which will be time consuming. \n",
        "\n",
        "- **Can handle missing input features**: will discuss it in detail. In a discriminative classifier, it assumes the feature $\\m{x}$ is always available to be conditioned on. \n",
        "\n",
        "- **Can handle unlabeled training data**: For semi-supervised learning, in which we combine labeled data $\\{\\m{x}^{(i)}, y^{(i)}\\}$ and unlabeled data $\\{\\m{x}^{(i)}\\}$. It is harder and not natural way to process in discriminative classifier. \n",
        "\n",
        "- **Can fit classes separately:** We estimate the parameters of each class conditional density independently, so we do not have to retrain the model when we add more classes. In a discriminative classifier, the whole model has to be retrained. \n",
        "\n",
        "\n",
        "\n",
        "### Discriminative classifier\n",
        "Def: A model of the form $p(y|\\m{x}) $. It can only be used to discriminate between different classes.\n",
        "\n",
        "Examples:\n",
        "\n",
        "- KNN\n",
        "- Logistic regression\n",
        "- SVM\n",
        "- Decision tree and random forest\n",
        "\n",
        "Advantages: \n",
        "\n",
        "- **Better predictive accuracy:** Discriminative classifiers are often much more accurate than generative classifiers. The reason is that the conditional distribution $p(y|\\m{x})$ is often much simpler (and therefore easier to learn) than the joint distribution $p(y, \\m{x})$. In particular, discriminative models do not need to “waste effort” modeling the distribution of the input features.\n",
        "\n",
        "- **Can handle feature preprocessing:** A big advantage of discriminative methods is that they allow us to preprocess the input in arbitrary ways. For example, we can perform a polynomial expansion of the input features. It is often hard to define a generative model on such pre-processed data, since the new features can be correlated in complex ways which are hard to model.\n",
        "\n",
        "- **Well-calibrated probabilities:**  Some generative classifiers, like NBC, make strong independence assumptions which are often not valid. Discriminative models, such as logistic regression, are often better calibrated in terms of their probability estimates. \n",
        "\n"
      ],
      "metadata": {
        "id": "vyTRkKt5pRz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling missing features\n",
        "In a generative classifier, we can handle this situation by marginalizing out the missing values.\n",
        "\n",
        "Suppose we are missing the value of $x_1$, which is the first feature of $\\m{x}$, we can compute \n",
        "\\begin{align}\n",
        "p(y=c|x_{2:D}, \\mm{\\theta}) &\\propto  p(y=c|\\mm\\pi) p(x_{2:D}|y=c, \\mm\\theta) \\\\\n",
        "&=  p(y=c|\\mm\\pi) \\sum_{x_1}p(x_1,x_{2:D}|y=c, \\mm\\theta) \n",
        "\\end{align}\n",
        "\n",
        "In Gaussian discriminant analysis, we can marginalize out $x_1$ by conditional expectation. More in applied statistics. \n",
        "\n",
        "In NBC, we can ignore the\n",
        "likelihood term for $x_1$,\n",
        "\\begin{align}\n",
        " \\sum_{x_1}p(x_1,x_{2:D}|y=c, \\mm\\theta)  = \\left[\\sum_{x_1} p(x_1| y=c, \\mm\\theta_{1c})\\right] \\Pi_{d=2}^Dp(x_d|y=c, \\mm\\theta_{dc})  =\\Pi_{d=2}^Dp(x_d|y=c, \\mm\\theta_{dc}) \n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GncaLdMt5m3u"
      }
    }
  ]
}