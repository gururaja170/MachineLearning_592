{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12.SVM4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOXsVjY0s7oU2EiKiuZTSBG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yexf308/MAT592/blob/main/12_SVM4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tfey6gHazW7p"
      },
      "source": [
        "## Multiclass SVMs\n",
        "### One-against-all scheme: \n",
        "- Construct $K$ binary SVMs with parameters $(\\mathbf{w}^j, w_0^j),1\\le j\\le K$. Each classifiers the current class $j$ against all others.   \n",
        "\n",
        "- Given a new feature $\\mathbf{x}$, the further the point is from the decision boundary of some binary SVM in the \"positive\" direction, the more likely we think it belongs to that class. \n",
        "\n",
        "- That is, the predicted class is set to \n",
        "  $$ \\text{arg}\\max_{1\\le j\\le K}\\{(\\mathbf{w}^{j})^\\top \\phi(\\mathbf{x})+w_0^j\\}$$\n",
        "\n",
        "### One-against-one scheme:\n",
        "\n",
        "- construct a binary SVMs for each pair of classes; in total $\\frac{K(K-1)}{2}$ binary SVMs for $K$-class classification. \n",
        "\n",
        "- All the binary classifiers are tested; for each of them, a win for one class is\n",
        "a vote for that class. The class with the most votes wins.\n",
        "\n",
        "Example: MNIST classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0M6mR7rh2iNC"
      },
      "source": [
        "# Summary of SVM and logistic regression: a unified framework\n",
        "\n",
        "- Construct a **decision function** (or **classifier**) $f_\\theta(\\mathbf{x}):\\mathbb{R}^d \\rightarrow \\mathbb{R}$, parameterized by $\\theta$. \n",
        "\n",
        "\n",
        "> Linear model: $f_\\theta(\\mathbf{x})=\\mathbf{w}^\\top \\mathbf{x}+w_0$ with $\\theta=(\\mathbf{w}, w_0)$. \n",
        "\n",
        "\n",
        "> Polynomials model up to degree $2$: $f_\\theta(\\mathbf{x})=\\mathbf{w}^\\top \\phi(\\mathbf{x})+w_0$, with $\\phi(\\mathbf{x})=[x_1, \\dots, x_d, x_1^2, \\dots, x_d^2, \\sqrt{2}x_1x_2, \\dots, \\sqrt{2}x_{d-1}x_d]$ and $\\theta=(\\mathbf{w}, w_0)$.\n",
        "\n",
        "- **Decision boundary** is given by the equation $f_{\\theta}(\\mathbf{x})=0$\n",
        "\n",
        "- Magnitude of $f_\\theta(\\mathbf{x})$ or equivanlently, $yf_\\theta(\\mathbf{x})$ reflects how far the sample $\\mathbf{x}$ is from the decision boundary, where label $y\\in\\{\\pm1\\}$.\n",
        "\n",
        "- Sign of $f_\\theta(\\mathbf{x})$ predicts the class. Sign of $yf_\\theta(\\mathbf{x})$ indicates if sample $\\mathbf{x}$ is\n",
        "correctly classified. \n",
        "\n",
        "- Choosing a decreasing **loss function** $\\ell: \\mathbb{R}\\rightarrow \\mathbb{R}$, penalizing upon the\n",
        "discrepancy between model output and the corresponding label. \n",
        "\n",
        "- Inconsistency between the output $f_\\theta(\\mathbf{x})$ and $y$ is measured by $\\ell(f_\\theta(\\mathbf{x}))$; negative $yf_\\theta(\\mathbf{x})$ implies misclassification and incurs a relatively large loss\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX2IiIXBo1RN"
      },
      "source": [
        "Fit the model on training dataset $\\{\\mathbf{x}^{(i)}, y^{(i)}\\}_{i=1}^N$, \n",
        "\n",
        "$$\\boxed{\\min_{\\theta}\\lambda\\sum_{i=1}^N\\ell(y^{(i)}f_\\theta(\\mathbf{x}^{(i)}))+ R(\\theta) } $$\n",
        "Often comes with with a regularizer $R(\\theta)$ like $\\|\\theta\\|_2^2, \\|\\theta\\|_1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpdzzxfKuzkz"
      },
      "source": [
        "\n",
        "\n",
        "### 1.  Perceptron:\n",
        "- Classifier: $f_\\theta(\\mathbf{x})=\\mathbf{w}^\\top \\mathbf{x}+w_0$.\n",
        "\n",
        "- Loss function: 0-1 loss function. $\\ell(z)=\\mathbb{1}_{z<0}$.\n",
        "\n",
        "- Regularizer: None. \n",
        "\n",
        "- Minimize the total number of misclassified training samples\n",
        "\n",
        "- Solver: SGD with the \"fake\" gradient\n",
        "\n",
        "### 2.   Logistic regression:\n",
        "- Classifier: $f_\\theta(\\mathbf{x})=\\mathbf{w}^\\top \\mathbf{x}+w_0$.\n",
        "- Loss function: log-loss function. $\\ell(z)=\\log(1+\\exp(-z))$. If $y\\in\\{\\pm 1\\}$, $p(y|\\mathbf{x},\\theta)=\\ell(yf_\\theta(\\mathbf{x}))$.\n",
        "\n",
        "- Regularizer: None. \n",
        "\n",
        "- Minimize the negative log likelihood when the probability is defined as before. \n",
        "\n",
        "- Solver: SGD with the true gradient\n",
        "\n",
        "### 3. Soft Margin Classification\n",
        "- Classifier: $f_\\theta(\\mathbf{x})=\\mathbf{w}^\\top \\mathbf{x}+w_0$.\n",
        "\n",
        "- Loss function: Hinge loss function. $\\ell(z)=\\max\\{1-z, 0\\}$.\n",
        "\n",
        "- Regularizer: Tikhonov($l_2$) regulation, $\\|\\mathbf{w}\\|_2^2$\n",
        "\n",
        "- Minimize the hinge loss with Tikhonov regularization.\n",
        "\n",
        "- Solver: solve the primal problem directily with SGD or solve the dual problem with SMO. \n",
        "\n",
        "### 4. Hard Margin Classification\n",
        "- Same as Soft Margin Classification with $\\lambda =+\\infty$. \n",
        "\n",
        "- Solver: solve the dual problem with SMO.\n",
        "\n",
        "### 5. Kernel SVM\n",
        "- Classifier: $f_\\theta(\\mathbf{x})=\\mathbf{w}^\\top \\phi(\\mathbf{x})+w_0=\\sum_{i=1}^N\\alpha_iy^{(i)}\\mathcal{K}(\\mathbf{x}^{(i)}, \\mathbf{x})+w_0$.\n",
        "\n",
        "- Loss function: Hinge loss function. $\\ell(z)=\\max\\{1-z, 0\\}$.\n",
        "\n",
        "- Regularizer: Tikhonov($l_2$) regulation, $\\|\\mathbf{w}\\|_2^2$.\n",
        "\n",
        "- Minimize the hinge loss with Tikhonov regularization.\n",
        "\n",
        "- Solver: solve the dual problem with SMO.\n",
        "\n",
        "- > Polynomial kernels: $\\mathcal{K}(\\mathbf{x}, \\mathbf{z}) = (1+\\mathbf{x}^\\top\\mathbf{z})^k $.\n",
        "\n",
        "- > Gaussian kernels: $\\mathcal{K}(\\mathbf{x}, \\mathbf{z}) = \\exp(-\\|\\mathbf{x}-\\mathbf{z}\\|^2/2\\sigma^2)$. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}