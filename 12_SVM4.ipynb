{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12.SVM4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPq8HPSHqeX3qKCCmbI5lGF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yexf308/MAT592/blob/main/12_SVM4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tfey6gHazW7p"
      },
      "source": [
        "## Multiclass SVMs\n",
        "### One-against-all scheme: \n",
        "- Construct $K$ binary SVMs with parameters $(\\mathbf{w}^j, w_0^j),1\\le j\\le K$. Each classifiers the current class $j$ against all others.   \n",
        "\n",
        "- Given a new feature $\\mathbf{x}$, the further the point is from the decision boundary of some binary SVM in the \"positive\" direction, the more likely we think it belongs to that class. \n",
        "\n",
        "- That is, the predicted class is set to \n",
        "  $$ \\text{arg}\\max_{1\\le j\\le K}\\{(\\mathbf{w}^{j})^\\top \\phi(\\mathbf{x})+w_0^j\\}$$\n",
        "\n",
        "### One-against-one scheme:\n",
        "\n",
        "- construct a binary SVMs for each pair of classes; in total $\\frac{K(K-1)}{2}$ binary SVMs for $K$-class classification. \n",
        "\n",
        "- All the binary classifiers are tested; for each of them, a win for one class is\n",
        "a vote for that class. The class with the most votes wins.\n",
        "\n",
        "Example: MNIST classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0M6mR7rh2iNC"
      },
      "source": [
        "# Summary of SVM and logistic regression: a unified framework\n",
        "\n",
        "- Construct a **decision function** (or **classifier**) $f_\\theta(\\mathbf{x}):\\mathbb{R}^d \\rightarrow \\mathbb{R}$, parameterized by $\\theta$. \n",
        "\n",
        "\n",
        "> Linear model: $f_\\theta(\\mathbf{x})=\\mathbf{w}^\\top \\mathbf{x}+w_0$ with $\\theta=(\\mathbf{w}, w_0)$. \n",
        "\n",
        "\n",
        "> Polynomials model up to degree $2$: $f_\\theta(\\mathbf{x})=\\mathbf{w}^\\top \\phi(\\mathbf{x})+w_0$, with $\\phi(\\mathbf{x})=[x_1, \\dots, x_d, x_1^2, \\dots, x_d^2, \\sqrt{2}x_1x_2, \\dots, \\sqrt{2}x_{d-1}x_d]$ and $\\theta=(\\mathbf{w}, w_0)$.\n",
        "\n",
        "- **Decision boundary** is given by the equation $f_{\\theta}(\\mathbf{x})=0$\n",
        "\n",
        "- Magnitude of $f_\\theta(\\mathbf{x})$ or equivanlently, $yf_\\theta(\\mathbf{x})$ reflects how far the sample $\\mathbf{x}$ is from the decision boundary, where label $y\\in\\{\\pm1\\}$.\n",
        "\n",
        "- Sign of $f_\\theta(\\mathbf{x})$ predicts the class. Sign of $yf_\\theta(\\mathbf{x})$ indicates if sample $\\mathbf{x}$ is\n",
        "correctly classified. \n",
        "\n",
        "- Choosing a decreasing **loss function** $\\ell: \\mathbb{R}\\rightarrow \\mathbb{R}$, penalizing upon the\n",
        "discrepancy between model output and the corresponding label. \n",
        "\n",
        "- Inconsistency between the output $f_\\theta(\\mathbf{x})$ and $y$ is measured by $\\ell(f_\\theta(\\mathbf{x}))$; negative $yf_\\theta(\\mathbf{x})$ implies misclassification and incurs a relatively large loss\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX2IiIXBo1RN"
      },
      "source": [
        "Fit the model on training dataset $\\{\\mathbf{x}^{(i)}, y^{(i)}\\}_{i=1}^N$, \n",
        "\n",
        "$$\\boxed{\\min_{\\theta}\\lambda\\sum_{i=1}^N\\ell(y^{(i)}f_\\theta(\\mathbf{x}^{(i)}))+ R(\\theta) } $$\n",
        "Often comes with with a regularizer $R(\\theta)$ like $\\|\\theta\\|_2^2, \\|\\theta\\|_1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpdzzxfKuzkz"
      },
      "source": [
        "\n",
        "\n",
        "### 1.  Perceptron:\n",
        "- Classifier: $f_\\theta(\\mathbf{x})=\\mathbf{w}^\\top \\mathbf{x}+w_0$.\n",
        "\n",
        "- Loss function: 0-1 loss function. $\\ell(z)=\\mathbb{1}_{z<0}$.\n",
        "\n",
        "- Regularizer: None. \n",
        "\n",
        "- Minimize the total number of misclassified training samples\n",
        "\n",
        "- Solver: SGD with the \"fake\" gradient\n",
        "\n",
        "### 2.   Logistic regression:\n",
        "- Classifier: $f_\\theta(\\mathbf{x})=\\mathbf{w}^\\top \\mathbf{x}+w_0$.\n",
        "- Loss function: log-loss function. $\\ell(z)=\\log(1+\\exp(-z))$. If $y\\in\\{\\pm 1\\}$, $p(y|\\mathbf{x},\\theta)=\\ell(yf_\\theta(\\mathbf{x}))$.\n",
        "\n",
        "- Regularizer: None. \n",
        "\n",
        "- Minimize the negative log likelihood when the probability is defined as before. \n",
        "\n",
        "- Solver: SGD with the true gradient\n",
        "\n",
        "### 3. Soft Margin Classification\n",
        "- Classifier: $f_\\theta(\\mathbf{x})=\\mathbf{w}^\\top \\mathbf{x}+w_0$.\n",
        "\n",
        "- Loss function: Hinge loss function. $\\ell(z)=\\max\\{1-z, 0\\}$.\n",
        "\n",
        "- Regularizer: Tikhonov($l_2$) regulation, $\\|\\mathbf{w}\\|_2^2$\n",
        "\n",
        "- Minimize the hinge loss with Tikhonov regularization.\n",
        "\n",
        "- Solver: solve the primal problem directily with SGD or solve the dual problem with SMO. \n",
        "\n",
        "### 4. Hard Margin Classification\n",
        "- Same as Soft Margin Classification with $\\lambda =+\\infty$. \n",
        "\n",
        "- Solver: solve the dual problem with SMO.\n",
        "\n",
        "### 5. Kernel SVM\n",
        "- Classifier: $f_\\theta(\\mathbf{x})=\\mathbf{w}^\\top \\phi(\\mathbf{x})+w_0=\\sum_{i=1}^N\\alpha_iy^{(i)}\\mathcal{K}(\\mathbf{x}^{(i)}, \\mathbf{x})+w_0$.\n",
        "\n",
        "- Loss function: Hinge loss function. $\\ell(z)=\\max\\{1-z, 0\\}$.\n",
        "\n",
        "- Regularizer: Tikhonov($l_2$) regulation, $\\|\\mathbf{w}\\|_2^2$.\n",
        "\n",
        "- Minimize the hinge loss with Tikhonov regularization.\n",
        "\n",
        "- Solver: solve the dual problem with SMO.\n",
        "\n",
        "- > Polynomial kernels: $\\mathcal{K}(\\mathbf{x}, \\mathbf{z}) = (1+\\mathbf{x}^\\top\\mathbf{z})^k $.\n",
        "\n",
        "- > Gaussian kernels: $\\mathcal{K}(\\mathbf{x}, \\mathbf{z}) = \\exp(-\\|\\mathbf{x}-\\mathbf{z}\\|^2/2\\sigma^2)$. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dxZCFy5TnCs",
        "outputId": "fd296a6d-0189-439d-d14b-079fc2065971"
      },
      "source": [
        "%pylab inline \n",
        "from IPython.display import Image"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "PO2iKNRLTq1W",
        "outputId": "1e11c3b1-e315-45e2-b67d-6d8f2c6f055e"
      },
      "source": [
        "Image(url='https://github.com/yexf308/MAT592/blob/main/image/losses.png?raw=true', width=500)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/losses.png?raw=true\" width=\"500\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G--iSgX-UB4J"
      },
      "source": [
        "1. log loss. $\\ell(z)=\\log(1+\\exp(-z))$ \n",
        "2. exp loss. $\\ell(z)=\\exp(-z)$\n",
        "\n",
        "They are both smooth convex relaxation of 0-1 loss function. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vbjH90_WBKu"
      },
      "source": [
        "## SVM for regression\n",
        "Fit the model on training dataset $\\{\\mathbf{x}^{(i)}, y^{(i)}\\}_{i=1}^N$, in regression, we have\n",
        "\n",
        "$$\\boxed{\\min_{\\theta}\\lambda\\sum_{i=1}^N\\ell(y^{(i)}, f(\\mathbf{x}^{(i)},\\mathbf{w}))+ R(\\theta) } $$\n",
        "where $f(\\mathbf{x}^{(i)},\\mathbf{w})$ is the function for regression, i.e, $f(\\mathbf{x}^{(i)},\\mathbf{w})=\\mathbf{w}^\\top \\phi(\\mathbf{x})$. $\\phi(\\mathbf{x})$ can be up to $k$-th order polynomials, or even Gaussian basis functions\n",
        "$$\\phi(\\mathbf{x})=\\left[\\exp\\left(-\\frac{\\|\\mathbf{x}-\\mathbf{x}^{(1)}\\|^2}{2\\sigma^2}\\right), \\dots, \\exp\\left(-\\frac{\\|\\mathbf{x}-\\mathbf{x}^{(N)}\\|^2}{2\\sigma^2}\\right) \\right]\\in \\mathbb{R}^N $$\n",
        "\n",
        "### 1. ridge regression\n",
        "\n",
        "- Loss function: Square loss. $\\ell(y^{(i)}, f(\\mathbf{x}^{(i)},\\mathbf{w})) =\\left(y^{(i)}- f(\\mathbf{x}^{(i)},\\mathbf{w})\\right)^2$.\n",
        "\n",
        "- Regularizer: Tikhonov regularization. $\\|\\mathbf{w}\\|^2_2$\n",
        "\n",
        "### 2. LASSO\n",
        "\n",
        "- Loss function: Square loss. $\\ell(y^{(i)}, f(\\mathbf{x}^{(i)},\\mathbf{w})) =\\left(y^{(i)}- f(\\mathbf{x}^{(i)},\\mathbf{w})\\right)^2$.\n",
        "\n",
        "- Regularizer: $l_1$ regularization. $\\|\\mathbf{w}\\|_1$\n",
        "\n",
        "### 3.  $\\epsilon$-insensitive loss\n",
        "\n",
        "- Loss function: **$\\epsilon$-insensitive loss function**:\n",
        "$$\\ell(y^{(i)}, f(\\mathbf{x}^{(i)},\\mathbf{w}))=\\max(|y^{(i)}- f(\\mathbf{x}^{(i)},\\mathbf{w})|-\\epsilon , 0) $$ \n",
        "\n",
        "- Regularizer: Tikhonov regularization. $\\|\\mathbf{w}\\|^2_2$\n",
        "\n",
        "### 4. Huber loss\n",
        "- Loss function: Huber loss, $\\ell(y^{(i)}, f(\\mathbf{x}^{(i)},\\mathbf{w}))=h(y^{(i)}- f(\\mathbf{x}^{(i)},\\mathbf{w}))$\n",
        "$$h(r) = \\begin{cases} r^2 & \\text{if }|r|\\le c  \\\\ 2c|r|-c^2 & \\text{Otherwise} \\end{cases} $$\n",
        "\n",
        "- Regularizer: Tikhonov regularization. $\\|\\mathbf{w}\\|^2_2$\n",
        "\n",
        "- (mixed quadratic/linear): robustness to outliers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "qP1VFZOA_ycg",
        "outputId": "ec3208f2-b6d2-4a28-c07d-c6597f1f997c"
      },
      "source": [
        "Image(url='https://github.com/yexf308/MAT592/blob/main/image/loss_function.png?raw=true', width=500)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/loss_function.png?raw=true\" width=\"500\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPj410D3AdEL"
      },
      "source": [
        "### Final notes on Loss functions\n",
        "Regressors and classifiers can be constructed by a “mix ‘n’ match” of loss\n",
        "functions and regularizers to obtain a learning machine suited to a\n",
        "particular application. \n",
        "\n",
        "- $l_1$—SVM\n",
        "$$ \\min_{\\mathbf{w}\\in \\mathbb{R}^d, w_0\\in \\mathbb{R}} \\lambda\\sum_{i=1}^N \\max\\left\\{0, 1-\\mathbf{y}^{(i)}(\\mathbf{w}^\\top \\mathbf{x}^{(i)}+w_0)\\right\\} +\\frac{1}{2} \\|\\mathbf{w}\\|_1$$\n",
        "- Least squares SVM\n",
        "\n",
        "$$ \\min_{\\mathbf{w}\\in \\mathbb{R}^d, w_0\\in \\mathbb{R}} \\lambda\\sum_{i=1}^N \\left(\\max\\left\\{0, 1-\\mathbf{y}^{(i)}(\\mathbf{w}^\\top \\mathbf{x}^{(i)}+w_0)\\right\\}\\right)^2 +\\frac{1}{2} \\|\\mathbf{w}\\|_2^2$$"
      ]
    }
  ]
}