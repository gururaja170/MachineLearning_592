{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fa22HW4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMLPUuoFQpYZlPheTrgOWgx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yexf308/MAT592/blob/main/homework/HW4/Fa22HW4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 4\n",
        "## Homework guideline\n",
        "- The deadline is Nov 11th 10:30am. Submission after the deadline will not be graded. \n",
        "\n",
        "- Submit your work(your reasoning and your code) as a SINGLE .ipynb document. Please rename the document as _HW1_YOURNAME.ipynb_ (for example, _HW1_FELIX.ipynb_). You are responsible for checking that you have correctly submitted the correct document. If your code cannot run, you may receive NO point. \n",
        "\n",
        "- Please justify all short answers with a brief explanation. It is highly recommended to use latex in the markdown. \n",
        "\n",
        "- You only use the Python packages included in the following cell. You are not allowed to use other advanced package or modules unless you are permitted to. \n",
        "\n",
        "- In your final submission include the plots produced by the unedited code as presented below, as well as any additional plots produced after editing the code during the course of a problem. You may find it necessary to copy/paste relevant code into additional cells to accomplish this.\n",
        "\n",
        "- Feel free to use the lecture notes and other resources. But you\n",
        "must understand, write, and hand in your own answers. In addition, you must write and submit\n",
        "your own code in the programming part of the assignment (we may run your code).\n",
        "If you copy someone else homework solution, both of you may receive ZERO point. \n",
        "\n",
        "\n",
        "- Colab is preferred. However, if you use Anaconda, please download the .mat file locally and save it to the same folder as this homework file. "
      ],
      "metadata": {
        "id": "7Vd2p9ZDha-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Collaboration:** List the names of all people you collaborated with and for which question(s). This is important!\n"
      ],
      "metadata": {
        "id": "s-fimMHhhwXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pylab inline \n",
        "import numpy.linalg as LA\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "\n",
        "## Set some default values of the the matplotlib plots\n",
        "plt.rcParams['figure.figsize'] = (8.0, 8.0)  # Set default plot's sizes\n",
        "plt.rcParams['axes.grid'] = True  # Show grid by default in figures"
      ],
      "metadata": {
        "id": "xKIBIxLfiFzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Description\n",
        "In this assignment, we will use the various methods for face recognition. Our task here is to be able to predict the correct label (name of the person) given an image of his face.\n",
        "\n",
        "We will use the same dataset in the last homework."
      ],
      "metadata": {
        "id": "mAfdl4yEiLWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_lfw_people\n",
        "dataset = fetch_lfw_people(min_faces_per_person=100)\n",
        "\n",
        "X = dataset.images\n",
        "y = dataset.target\n",
        "label_to_name_mapping = dataset.target_names\n",
        "image_shape = X[0].shape\n",
        "\n",
        "print('Number of images in the dataset: {}'.format(len(X)))\n",
        "print('Number of different persons in the dataset: {}'.format(len(np.unique(y))))\n",
        "print('Each images size is: {}'.format(image_shape))\n",
        "\n",
        "_, images_per_class = np.unique(y, return_counts=True)\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(label_to_name_mapping, images_per_class)\n",
        "ax.set_xticklabels(label_to_name_mapping, rotation=-90);\n",
        "ax.set_title('Images per person')\n",
        "ax.set_ylabel('Number of images')\n",
        "\n",
        "\n",
        "# plots the first 20 images in the dataset. \n",
        "fig, ax_array = plt.subplots(3, 3)\n",
        "for i, ax in enumerate(ax_array.flat):\n",
        "    ax.imshow(X[i], cmap='gray')\n",
        "    ax.set_ylabel(label_to_name_mapping[y[i]])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xticks([])"
      ],
      "metadata": {
        "id": "WDxa4NA6hQqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# split the data into 80% training set and 20% testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "AOhdOiO1hTtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "# Q1: Logistic regression with Softmax (40pt)\n",
        "Let's do multi-class classification with the softmax function. \n",
        "Remember the model is as follows: \n",
        "\n",
        "1. the probability for $K$ classes are  \n",
        "\\begin{align}\n",
        "\\text{Pr}(y=\\ell |\\mathbf{x}, \\mathbf{W}) = \\frac{\\exp(\\mathbf{w}_\\ell \\cdot \\mathbf{x})}{\\sum_{j=0}^{K-1} \\exp(\\mathbf{w}_j \\cdot \\mathbf{x}) }\n",
        "\\end{align}\n",
        "where $\\mathbf{w}_\\ell $ is $\\ell$-th row of $\\mathbf{W}$. Here we ignore the bias vector. \n",
        "\n",
        "2. In prediction, you will take the largest predicted probability among your $K$ predicted probability. \n",
        "\n",
        "3. The negative log-likelihood function on the $N$ training dataset is \n",
        "\\begin{align}\n",
        "L(\\mathbf{W}) = -\\frac{1}{N}\\sum_{i=1}^N \\log \\text{Pr}(y=y^{(i)} | \\mathbf{x}^{(i)}, \\mathbf{W})\n",
        "\\end{align}\n",
        "The following figure illustrates the logistic regression with softmax on the MNIST dataset (of course with the bias vector). \n",
        "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/MNIST_LR.png?raw=true\" width=\"800\" />\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Q2.1 Derivative (10pt)\n",
        "Express the label $y$ into one hot vector, i.e., $y=\\ell$ is represented by $\\vec{y}$ with $\\vec{y}_\\ell =1$ and other indices are zero. \n",
        "\n",
        "Calculate the derivative of the negative log-likelihood with respect to the variable $\\mathbf{W}$. Write this in a natural manner in terms of conditional probability, the data matrix $\\mathbf{X}$. Don't write this expression in terms of exponentials directly. \n",
        "You may refer to results in previous lectures. \n"
      ],
      "metadata": {
        "id": "Not_3zS-ynsb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Answer: "
      ],
      "metadata": {
        "id": "n0tYucQk_MEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Q2.2 Implementation (30pt)\n",
        "(a) Implement softmax classification with SGD. Specify your learning rate (or your learning rate decay scheme if you alter learning rate). \n",
        "\n",
        "(b) For both training dataset and testing dataset, plot the negative log-likelihood as function of the epoch number. \n",
        "\n",
        "(c) For both training dataset and testing dataset, plot the misclassification rate as function of the epoch number.\n",
        "\n",
        "(d) How many iterations did this take you? \n",
        "\n"
      ],
      "metadata": {
        "id": "fKKECmhU_zwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2.2: your code starts here. \n"
      ],
      "metadata": {
        "id": "iGuERadPBZrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Answer: "
      ],
      "metadata": {
        "id": "sgiMYEM3BcPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Q2.3 (optional) Compare with full batch gradient descent \n",
        "\n",
        "(a) Implement the previous algorithm with full batch gradient descent. What learning rate did you use? \n",
        "\n",
        "(b) Please plot the similar plots as you did in the previous question. \n",
        "\n",
        "(c) At last, compare the computational complexity it took you to reach a comparable misclassification rate on your training set. \n",
        "\n",
        "(d) Do you think SGD performs better than full batch gradient descent or not? "
      ],
      "metadata": {
        "id": "B-p5VEKIBjb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2.3: your code starts here. \n"
      ],
      "metadata": {
        "id": "CaKfVDC3CeqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Answer: "
      ],
      "metadata": {
        "id": "q0mtjE9YCgJO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Q2: Deep neural network with Sigmoid function and squared loss (30pt)\n",
        "In class, we showed how to implement DNN with the sigmoid function as the activation function. In particular, the output layer also uses the sigmoid function. \n",
        "\n",
        "(a) Please setup your own deep neural network to classify these faces. Here you can use sigmoid function as activation function and output function, and you can use the square loss function as well. But you have to choose the number of the layer and hidden neurons by yourself. **Happy tuning!**\n",
        "\n",
        "(b) For both training dataset and testing dataset, plot the cost as function of the epoch number. \n",
        "\n",
        "(c) For both training dataset and testing dataset, plot the misclassification rate as function of the epoch number.\n",
        "\n",
        "(d) Did DNN performs better than logistic regression or not? \n"
      ],
      "metadata": {
        "id": "hg0yA3JWzkbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3: your code starts here. \n"
      ],
      "metadata": {
        "id": "peqcx-7gz6kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Answer:"
      ],
      "metadata": {
        "id": "uX3lCBxVDqp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Q3: Deep neural network with Relu function and squared loss (30pt)\n",
        "Another popular choice of the activation function is relu function. \n",
        "\n",
        "(a) Modify your code to use Relu function in these hidden nueron layer and sigmoid/softmax function in the output layer. \n",
        "\n",
        "(b) Compare the performance with Q1 and Q2. "
      ],
      "metadata": {
        "id": "QQFRzZ7pGZ_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3: your code starts here. \n"
      ],
      "metadata": {
        "id": "ZDbdEAeYGidt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Answer:"
      ],
      "metadata": {
        "id": "_2MgjykeGkTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "# Q4: (optional) Deep neural network with logistic loss \n",
        "\n",
        "(a) What about switching to logistic loss? Do you find it is nearly impossible to tune here due to the vanishing gradient and other errors/warnings? \n",
        "\n",
        "(b) Please dig out the root cause and fix it! You should get similar performance as the square loss. \n"
      ],
      "metadata": {
        "id": "F2SGDdzn0C1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4: your code starts here. \n"
      ],
      "metadata": {
        "id": "2PT3VfnT1cxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Answer:"
      ],
      "metadata": {
        "id": "h3Uw_KWuDnbV"
      }
    }
  ]
}