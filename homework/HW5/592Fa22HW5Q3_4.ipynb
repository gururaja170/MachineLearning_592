{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPRqF17XhMkmVrj3F/yQCcn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yexf308/MachineLearning/blob/main/homework/HW5/592Fa22HW5Q3_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMk2oFUIIMGq"
      },
      "outputs": [],
      "source": [
        "%pylab inline \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\def\\m#1{\\mathbf{#1}}$\n",
        "$\\def\\mm#1{\\boldsymbol{#1}}$\n",
        "$\\def\\mb#1{\\mathbb{#1}}$\n",
        "$\\def\\c#1{\\mathcal{#1}}$\n",
        "$\\def\\mr#1{\\mathrm{#1}}$\n",
        "$\\newenvironment{rmat}{\\left[\\begin{array}{rrrrrrrrrrrrr}}{\\end{array}\\right]}$\n",
        "$\\newcommand\\brm{\\begin{rmat}}$\n",
        "$\\newcommand\\erm{\\end{rmat}}$\n",
        "$\\newenvironment{cmat}{\\left[\\begin{array}{ccccccccc}}{\\end{array}\\right]}$\n",
        "$\\newcommand\\bcm{\\begin{cmat}}$\n",
        "$\\newcommand\\ecm{\\end{cmat}}$"
      ],
      "metadata": {
        "id": "ApRwUE5KIXGW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Q3: Image classification on CIFAR-10 (60pt)\n",
        "\n",
        "### Preliminaries information:\n",
        "In this problem we will explore different deep learning architectures for image classification on the CIFAR-10\n",
        "dataset. If you are not comfortable with PyTorch from the previous lecture and discussion materials, use the tutorials at http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html and make sure you\n",
        "are familiar with tensors, two-dimensional convolutions (`nn.Conv2d`) and fully-connected layers (`nn.Linear`),\n",
        "ReLU non-linearities (`F.relu`), pooling (`nn.MaxPool2d`), and tensor reshaping (`view`).\n",
        "\n",
        "For this problem, it is highly recommended that you copy and modify the existing network code produced in\n",
        "the tutorial *Training a classifier*. You should not be coding this network from scratch!\n",
        "\n",
        "\n",
        "\n",
        "- Each network $f$ maps an image $x^{\\rm in} \\in \\mb{R}^{32 \\times 32 \\times 3}$ (3 channels for RGB) to an output $f(x^{\\rm in}) = x^{\\rm out} \\in \\mb{R}^{10}$. The class label is predicted as $\\arg\\max_{i=0,1,\\dots,9} x_{i}^{\\rm out}$.\n",
        "\n",
        "- The network is trained via multiclass cross-entropy loss (log of softmax function).  Specifically, for an input image and label pair $(x^{\\rm in} , c)$ where $c\\in \\{0,\\dots, 9\\}$. If the network’s\n",
        "output layer is $x^{\\rm out} \\in \\mb{R}^{10}$, the loss $-\\log\\left(\\frac{\\exp(x_c^{\\rm out})}{\\sum_{c'} \\exp(x_{c'}^{\\rm out})}\\right)$. \n",
        "\n",
        "- For computational efficiency reasons, this particular network considers mini-batches of images per training\n",
        "step meaning the network actually maps $B=4$ images per feed-forward so that $\\tilde{x}^{\\rm in}\\in\\mb{R}^{B\\times 32 \\times 32 \\times 3}$ and $\\tilde{x}^{\\rm out}\\in\\mb{R}^{B\\times 10}$.  This is ignored in the network descriptions below but it is something to be aware of.\n",
        " \n",
        "- Create a validation dataset by appropriately partitioning the train dataset. **Hint**: look at the documentation for `torch.utils.data.random\\_split`. Make sure to tune hyperparameters like network architecture and step size on the validation dataset. Do **NOT** validate your hyperparameters on the test dataset.\n",
        "\n",
        "- Modify the training code such that at the end of each epoch (one pass over the training data) it computes and prints the training and test classification accuracy.\n",
        "\n",
        "- The cross-entropy loss for a neural network is, in general, non-convex. This means that the optimization\n",
        "method may converge to different local minima based on different hyperparameters of the optimization\n",
        "procedure (e.g., stepsize). Usually one can find a good setting for these hyperparameters by just observing\n",
        "the relative progress of training over the first epoch or two (how fast is it decreasing) but you are warned\n",
        "that early progress is not necessarily indicative of the final convergence value (you may converge quickly to a poor local minimum whereas a different step size could have poor early performance but converge to\n",
        "a better final value).\n",
        "\n",
        "- While one would usually train a network for hundreds of epochs to reach convergence and maximize accuracy, this can be prohibitively time-consuming, so feel free to train for just a a dozen or so epochs. \n",
        "\n",
        "\n",
        "**Your Task:** \n",
        "For all of the following, \n",
        "- Apply a **hyperparameter tuning method** (manually by\n",
        "hand, grid search, random search, etc.) using the\n",
        "validation set\n",
        "\n",
        "- Report the hyperparameter configurations you evaluated and the best set of hyperparameters\n",
        "from this set.  \n",
        "\n",
        "- Plot the training and validation classification accuracy as a function of iteration. Produce\n",
        "a separate line or plot for each hyperparameter configuration evaluated (please try to use multiple lines in a single plot to keep the number of figures minimal). \n",
        "\n",
        "- Finally, evaluate your best set of\n",
        "hyperparameters on the test data and report the accuracy.\n",
        "\n",
        "\n",
        "\n",
        "The number of hyperparameters to tune, combined with the slow training times, will hopefully give\n",
        "you a taste of how difficult it is to construct networks with good generalization performance. It should be emphasized that the\n",
        "networks we constructed are **tiny**. \n",
        "State-of-the-art networks can have dozens of layers, each with their own hyperparameters to tune. Additional\n",
        "hyperparameters you are welcome to play with if you are so inclined, include: changing the activation\n",
        "function, replace max-pool with average-pool, adding more convolutional or fully connected layers, and\n",
        "experimenting with batch normalization or dropout.\n",
        "\n",
        "\n",
        "Here are the network architectures you will construct and compare.\n",
        "Before you jump into tuning, it is better to write  separate train and evaluation functions. \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T-zGHmoZITXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sns.set()\n",
        "torch.manual_seed(592)\n",
        "np.random.seed(592)\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "FXeS8QdNmrkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# it may takes while to download the data, please try this code several times. \n",
        "def prepare_dataset(batch_size=64, train_val_split_ratio=0.9):\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    cifar10_set = datasets.CIFAR10(root='./data', train=True, download=False, transform=transform)\n",
        "\n",
        "    train_size = int(len(cifar10_set) * train_val_split_ratio)\n",
        "    val_size   = len(cifar10_set) - train_size\n",
        "\n",
        "    cifar10_trainset, cifar10_valset = torch.utils.data.random_split(cifar10_set, [train_size, val_size])\n",
        "    cifar10_testset = datasets.CIFAR10(root='./data', train=False, download=False, transform=transform)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(cifar10_trainset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(cifar10_valset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(cifar10_testset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n"
      ],
      "metadata": {
        "id": "eDIJJ4NVlvkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, val_loader, criterion, optimizer, epochs, batch_size):\n",
        "   \"\"\" Trains a model for n epochs using given optimizer, and then records \n",
        "    validation and training accuracies, validation and training losses.  \n",
        "   \"\"\"\n",
        "   # Your code starts here."
      ],
      "metadata": {
        "id": "bRLFZXYuxP27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(model, test_loader, criterion):\n",
        "  \"\"\"Calculate and print test accuracy and test losses.\n",
        "  \"\"\"\n",
        "  # Your code starts here."
      ],
      "metadata": {
        "id": "xmePJ8gby2AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3.1:  Fully-connected output, no hidden layers (logistic regression) (10pt)\n",
        "We begin with the simplest network\n",
        "possible that has no hidden layers and simply linearly maps the input layer to the output layer. That is,\n",
        "conceptually it could be written as\n",
        "\\begin{align*}\n",
        "    x^{\\rm out} &= W \\text{vec}(x^{\\rm in}) +b\n",
        "\\end{align*} \n",
        "where $x^{\\rm out} \\in \\mb{R}^{10}$, $x^{\\rm in} \\in \\mb{R}^{32 \\times 32 \\times 3}$, $W \\in \\mb{R}^{10 \\times 3072}$, $b \\in \\mb{R}^{10}$ since $3072 = 32 \\cdot 32 \\cdot 3$. For a tensor $x \\in \\mb{R}^{a \\times b \\times c}$, we let $\\text{vec}(x) \\in \\mb{R}^{a b c}$ be the reshaped form of the tensor into a vector (in an arbitrary but consistent pattern).   There is no required benchmark testing accuracy for this part."
      ],
      "metadata": {
        "id": "dn3WeaNXIc0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Q3.1 your code starts here"
      ],
      "metadata": {
        "id": "zKsm48L0I6nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Solution:"
      ],
      "metadata": {
        "id": "_5Ajj8csI8LI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Q3.2: Fully-connected output, 1 fully-connected hidden layer (10pt)\n",
        "\n",
        "We will have one hidden layer denoted as $x^{\\rm hidden} \\in \\mb{R}^{M}$ where $M$ will be a hyperparameter you choose ($M$ could be in the hundreds). The non-linearity applied to the hidden layer will be the **relu** ($\\mathrm{relu}(x) = \\max\\{0,x\\}$, elementwise). This network can be written as\n",
        "\n",
        "\\begin{align*}\n",
        "    x^{\\rm out} &= W_2 \\mathrm{relu}(W_1 \\text{vec}(x^{\\rm in}) +b_1) + b_2\n",
        "\\end{align*}\n",
        "\n",
        "where $W_1 \\in \\mb{R}^{M \\times 3072}$, $b_1 \\in \\mb{R}^M$, $W_2 \\in \\mb{R}^{10 \\times M}$, $b_2 \\in \\mb{R}^{10}$.  Tune the different hyperparameters and train for\n",
        "a sufficient number of epochs to achieve a testing accuracy of at least 50%. Provide the hyperparameter\n",
        "configuration used to achieve this performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "O1nkTyNuI-Q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Q3.2 your code starts here"
      ],
      "metadata": {
        "id": "c3FS6gwRJCOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Solution:"
      ],
      "metadata": {
        "id": "ezZkLNswJHLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Q3.3: Convolutional layer with max-pool and fully-connected output (15pt)\n",
        "\n",
        "For a convolutional layer $W_1$ with filters of size $k \\times k \\times 3$, and $M$ filters (reasonable choices are $M=100$, $k=5$), we have that $\\mathrm{Conv2d}(x^{\\rm in}, W_1) \\in \\mb{R}^{(33-k) \\times (33-k) \\times M}$. \n",
        "\n",
        "- Each convolution will have its own offset applied to each of the output pixels of the convolution; we denote this as $\\mathrm{Conv2d}(x^{\\rm in}, W) + b_1$ where $b_1$ is parameterized in $\\mb{R}^M$. Apply a **relu** activation to the result of the convolutional layer. \n",
        "\n",
        "-  Next, use a max-pool of size $N \\times N$ (a reasonable choice is $N=14$ to pool to $2 \\times 2$ with $k=5$) we have that $\\textrm{MaxPool}( \\mathrm{relu}( \\mathrm{Conv2d}(x^{\\rm in}, W_1)+b_1)) \\in \\mb{R}^{\\lfloor\\frac{33-k}{N}\\rfloor \\times \\lfloor\\frac{33-k}{N}\\rfloor \\times M}$.\n",
        "\n",
        "- We will then apply a fully-connected layer to the output to get a final network given as\n",
        "\\begin{align*}\n",
        "          x^{\\rm output} = W_2 \\text{vec}(\\textrm{MaxPool}( \\mathrm{relu}( \\mathrm{Conv2d}(x^{\\rm input}, W_1)+b_1))) + b_2\n",
        "\\end{align*}\n",
        "where $W_2 \\in \\mb{R}^{10 \\times M (\\lfloor\\frac{33-k}{N}\\rfloor)^2}$, $b_2 \\in \\mb{R}^{10}$.\n",
        "\n",
        "\n",
        "The parameters $M, k, N$ (in addition to the step size and momentum) are all hyperparameters, but you\n",
        "can choose a reasonable value. Tune the different hyperparameters (number of convolutional filters, filter\n",
        "sizes, dimensionality of the fully-connected layers, stepsize, etc.) and train for a sufficient number of\n",
        "epochs to achieve a validation accuracy of **at least 70%**. Provide the hyperparameter configuration used\n",
        "to achieve this performance. Make sure to save this model so that you can do the next part.\n"
      ],
      "metadata": {
        "id": "XNkG_qIkJKW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Q3.3 your code starts here"
      ],
      "metadata": {
        "id": "0GwNhUIFJUp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Solution:"
      ],
      "metadata": {
        "id": "c5eGPdKpJXJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Q3.4: More tuning (10pt)\n",
        "\n",
        "Return to the original network you were left with at the end of the tutorial Training\n",
        "a classifier. (Note that this is not the network from Q3.3 above.) Tune the different hyperparameters\n",
        "(number of convolutional filters, filter sizes, dimensionality of the fully-connected layers, stepsize, etc.) and\n",
        "train for a sufficient number of iterations to achieve a *train accuracy* of **at least 87%**. You may not modify\n",
        "the core structure of the model (i.e., adding additional layers). Provide the hyperparameter configuration\n",
        "used to achieve this performance. Make sure to save this model so that you can do the next part (see\n",
        "the Training a classifier tutorial for details on how to do this)."
      ],
      "metadata": {
        "id": "ZxRhwXJjLFoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Q3.4 your code starts here"
      ],
      "metadata": {
        "id": "p0URQB5cLnfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Solution:"
      ],
      "metadata": {
        "id": "nzZ9GTydLpWy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Q3.5: Transfer Learning:  Use AlexNet as a fixed feature extractor (5pt)\n",
        "So far we have trained very small neural networks from scratch. As mentioned in the previous problem,\n",
        "modern neural networks are much larger and more difficult to train and validate. In practice, it is rare to train\n",
        "such large networks from scratch. This is because it is difficult to obtain both the massive datasets and the\n",
        "computational resources required to train such networks. \n",
        "\n",
        "Instead of training a network from scratch, in this problem, we will use a network that has already been trained\n",
        "on a very large dataset (ImageNet) and adjust it for the task at hand. This process of adapting weights in a\n",
        "model trained for another task is known as **transfer learning**.\n",
        "\n",
        "Begin with the pretrained **AlexNet** model from `torchvision.models` for the following tasks below. AlexNet\n",
        "achieved an early breakthrough performance on ImageNet and was instrumental in sparking the deep\n",
        "learning revolution in 2012.\n",
        "\n",
        "Do not modify any module within AlexNet that is not the final classifier layer.\n",
        "\n",
        "- The output of AlexNet comes from the 6-th layer of the classifier. Specifically, `model.classifer[6] =\n",
        "nn.Linear(4096, 1000)`. To use AlexNet with CIFAR-10, we will reinitialize (replace) this layer with\n",
        "`nn.Linear(4096, 10)`. This re-initializes the weights, and changes the output shape to reflect the desired\n",
        "number of target classes in CIFAR-10. \n",
        "\n",
        "- We only adjust the weights of this new layer (keeping the weights of all other layers\n",
        "fixed). When using AlexNet as a fixed feature extractor, make sure to freeze all of the parameters in the network\n",
        "before adding your new linear layer:\n",
        " ```\n",
        "model = torchvision.models.alexnet(pretrained=True)\n",
        "for param in model.parameters():\n",
        "param.requires_grad = False\n",
        "model.classifier[6] = nn.Linear(4096, 10)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v6FqzS3z1hrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Q3.5 your code starts here"
      ],
      "metadata": {
        "id": "3NcS6cNB8gYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Solution:"
      ],
      "metadata": {
        "id": "xWLOXixw8kWC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Q3.6: Transfer Learning: Use AlexNet as initialization (10pt)\n",
        "The second approach to transfer learning is to fine-tune the weights of the pretrained network, in addition to training the new classification layer. In this approach, all network weights\n",
        "are updated at every training iteration; we simply use the existing AlexNet weights as the “initialization”\n",
        "for our network (except for the weights in the new classification layer, which will be initialized using\n",
        "whichever method is specified in the constructor) prior to training on CIFAR-10. \n",
        "\n",
        "**Note**: Fine-tune AlexNet on\n",
        "CPU takes an insame amount of time, so we recommend you to use Google Colab, which has free GPU\n",
        "access. To enable GPU for the notebook: Navigate to Edit→Notebook Settings. select GPU from the\n",
        "Hardware Accelerator drop-down. For information about training on GPU, check the tutorial."
      ],
      "metadata": {
        "id": "rEbf4r_v86WL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Q3.6 your code starts here"
      ],
      "metadata": {
        "id": "yD36kqFR9upU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Solution:"
      ],
      "metadata": {
        "id": "nyEesRXB9wMP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Q3.7: (Bonus Question) Adversarial Attacks\n",
        "**If you enjoy deeping learning, you should try this question.**\n",
        "\n",
        "Modern deep neural networks are brittle and susceptible to small\n",
        "perturbations to their inputs. This gives rise to adversarial examples, which are nearly indistinguishable\n",
        "to the human eye but somehow “fool” neural networks into making drastically wrong predictions.\n",
        "\n",
        "One algorithm to generate such examples is the untargeted fast gradient sign method (FGSM) attack,\n",
        "which can be described as follows: \n",
        "Let $x$ be an input image with label $y$, $\\c{F}$ be a neural network, and \u000f$\\epsilon$ be a small value (intuitively, an attack rate).\n",
        "\n",
        "\\begin{align}\n",
        "&\\hat{y} = \\c{F}(x) \\\\\n",
        "&\\c{L}= \\text{CrossEntropy}(\\hat{y},y) \\\\\n",
        "&x' = x+\\epsilon \\cdot \\text{sign}(\\nabla_x \\c{L})\n",
        "\\end{align}\n",
        "\n",
        "where $\\text{sign}(t) = \\frac{t}{|t|}$.  We then use $x'$ as an input to the network. Note that the calculation for $x'$\n",
        "loosely resembles gradient descent. Intuitively, we are slightly adjusting the input image so that the model is less\n",
        "likely to predict its true class.\n",
        "\n",
        "For this part, use your classifier from Q3.4 to do the following steps. As always, please provide all code\n",
        "and plots.\n",
        "\n",
        "- Select four images from the train set that have been correctly classified. Visualize them and provide\n",
        "their labels.\n",
        "\n",
        "- Implement the untargeted FGSM algorithm. Run one iteration on these images and visualize them:\n",
        "they should look like the originals.\n",
        "\n",
        "- Provide the predicted labels for your attacked images. You should have at least one image that is\n",
        "incorrectly classified. Remark: **FGSM** is a simple attack, but it’s not always effective. In order to\n",
        "generate successful adversarial examples, you may need to try different values of $\\epsilon$\u000f or even different\n",
        "images, depending on where your classifier excels.\n",
        "\n",
        "- Explain the significance of the existence of such adversarial examples.\n",
        "\n"
      ],
      "metadata": {
        "id": "Lm5m45T3LzBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Q3.5 your code starts here"
      ],
      "metadata": {
        "id": "CGQQj0jDPSY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Solution:"
      ],
      "metadata": {
        "id": "Fz6oylyDPU04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Q4: (optional) Correction to your previous homework question\n",
        "You may pick any one question in homework 1-4 that didn't perform well, and now you have the chance to correct your mistakes. If you successfully correct your mistakes, your previous grade will be replaced by the current score, e.g., say you want to correct HW4Q2:Logistic regression with Softmax , your previous score is 10/30 and after successful attempt, your score becomes 25/30. You will be awarded 15 bonus point here. \n",
        "\n",
        "**State Your question that you want to correct:**"
      ],
      "metadata": {
        "id": "kXBV7I_KPh0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your new code starts here"
      ],
      "metadata": {
        "id": "I6zGr3gxP0P4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your New Solution:"
      ],
      "metadata": {
        "id": "EpDdS-a4P1-S"
      }
    }
  ]
}