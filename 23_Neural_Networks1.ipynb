{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "23.Neural Networks1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPxjgYtG0429ZG1T3h1nE8C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yexf308/MAT592/blob/main/23_Neural_Networks1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbvOVZCdvYIw",
        "outputId": "3107f58c-da03-462c-b3ed-daea7a55b11d"
      },
      "source": [
        "%pylab inline \n",
        "import numpy.linalg as LA\n",
        "from IPython.display import Image"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQWIVWpwhxlO"
      },
      "source": [
        "## Tip of the day - Debugging with PDB (optional, for the **advanced** users):\n",
        "\n",
        "One of the main problems in working with Colab / Jupyter is the lack of a good debugger.\n",
        "\n",
        "A not so optimal solution (but sometimes better than nothing) is to use Python's built-in debugger. Python comes with a very basic command line based debugger called [PDB](https://docs.python.org/3/library/pdb.html). It has all the basic debugger capabilities but is missing a good interface.\n",
        "\n",
        "You can drop into debug mode in the middle of any Python code simply by placing the command **pdb.set_trace()** before the line which you want to debug (only after importing the **pdb** package). Once in debug mode, the debugger prompt will appear in which you can use the following commands:\n",
        "\n",
        "- **l** or **list**: to print the current line and the surrounding code.\n",
        "- **n** or **next**: to run the current line.\n",
        "- **c** or **continue**: to continue running until a breakpoint or until the end of the function.\n",
        "- **q** or **quit**: to stop the run and exit the debugger.\n",
        "- **b \\<number\\>**: Place a breakpoint in line \\<number\\>\n",
        "- **!\\<python expression\\>**: to run any python expression.\n",
        "\n",
        "(This is only a partial list of the PDB's commands. For the full list of commands you can refer to the [official documentation](https://docs.python.org/2/library/pdb.html#debugger-commands), or at [this cheat sheet](https://kapeli.com/cheat_sheets/Python_Debugger.docset/Contents/Resources/Documents/index))\n",
        "\n",
        "Let us look at an example:\n",
        "\n",
        "- Add the line **pdb.set_trace()** to the following code just before the **x **= 2**  line, and execute the cell to drop into debug mode.\n",
        "- Type **l** (followed by **Enter**) to print the current line and surrounding code.\n",
        "- Type **!print(x)** to print the value of the variable **x** (you can also omit the print command and just use **!x** in this case).\n",
        "- Type **!x=2** to change the value of **x**.\n",
        "- Type **n** execute the next line.\n",
        "- Type **l** again.\n",
        "- Type **!print(x)** again.\n",
        "- Type **b 10** to place a breakpoint on the **return x** line.\n",
        "- Type **c** to run the code until the breakpoint.\n",
        "- Type **l** again.\n",
        "- Type **!print(x)** again.\n",
        "- Type **c** or **q** to quit the debugger with or without finishing to run the code\n",
        "- After you finish playing with the debugger make sure you remove or comment out the **pdb.set_trace()** line.\n",
        "- Clear our all the debugger output. You can do so, for example, by pressing the **X** button left to the cell's output area."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0Ho2HTgh7MW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67dd39c5-4201-4036-8e51-a03dfb91d03f"
      },
      "source": [
        "import pdb\n",
        "\n",
        "def func(x):\n",
        "    x += 2\n",
        "    pdb.set_trace() \n",
        "    x *= 4\n",
        "    x -= 2\n",
        "    x /= 2\n",
        "    \n",
        "    return x\n",
        "\n",
        "print(func(3))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> <ipython-input-4-ced2f7bc05d3>(6)func()\n",
            "-> x *= 4\n",
            "(Pdb) l\n",
            "  1  \timport pdb\n",
            "  2  \t\n",
            "  3  \tdef func(x):\n",
            "  4  \t    x += 2\n",
            "  5  \t    pdb.set_trace()\n",
            "  6  ->\t    x *= 4\n",
            "  7  \t    x -= 2\n",
            "  8  \t    x /= 2\n",
            "  9  \t\n",
            " 10  \t    return x\n",
            " 11  \t\n",
            "(Pdb) !print(X)\n",
            "*** NameError: name 'X' is not defined\n",
            "(Pdb) !print(x)\n",
            "5\n",
            "(Pdb) c\n",
            "9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZCXcT4Nh-E9"
      },
      "source": [
        "### !! Important\n",
        "\n",
        "- The **pdb.set_trace()** command can be placed anywhere, but stepping through the code (using the **n** command) is only possible inside functions.\n",
        "- **You must exit the debugger** (using **c** or **q**) in order to be able to run any other cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pG8yQj9A0sK4"
      },
      "source": [
        "## Multi-class classification revisited\n",
        "- **Given**: the training dataset $\\mathcal{D}=\\{\\mathbf{x}^{(i)}, y^{(i)}\\}_{i=1}^N$, the input feature $\\mathbf{x}^{(i)}\\in \\mathbb{R}^D$ and the label $y^{(i)}\\in \\{1,2,\\dots, K\\}$, for $K$-class classification. \n",
        "\n",
        "- **Goal**:  learn a vector-valued function with tunable model parameters $\\theta$:\n",
        "$$\\vec f(\\mathbf{x}; \\theta): \\mathbb{R}^D \\rightarrow (0,1)^K $$\n",
        "  - $f_c(\\mathbf{x}^{(i)}; \\theta)$ is the **predicted probability** that input data $\\mathbf{x}^{(i)}$ belongs to Class $c$, for each $c=1, \\dots, K$. \n",
        "\n",
        "  - Tune $\\theta$ using certain optimization algorithm in the hope that, for all samples $1\\le i \\le N$, \n",
        "   $$ \\arg\\min_{c}-f_c(\\mathbf{x}^{(i)}; \\theta) = y^{(i)}$$\n",
        "\n",
        "### Logistic regression\n",
        "- $\\vec f$ is composition of linear and softmax functions. \n",
        "\n",
        "- Input data $\\mathbf{x}$ is fed into a linear function $\\mathbf{z}(\\mathbf{x})=\\mathbf{W}\\mathbf{x} + \\mathbf{b}\\in \\mathbb{R}^K$. $\\mathbf{z}$ is the pre-activation vector, $\\mathbf{W}\\in \\mathbb{R}^{K\\times D}$ is the weight matrix and $\\mathbf{b}$ is the bias vector. Note here we exclude the intercept in the feature for logistic regression.\n",
        "\n",
        "- model outputs the predicted probabilities for the $K$ classes with the help of softmax activation function $\\sigma$. \n",
        "\n",
        "$$\\vec f(\\mathbf{x};\\theta)= \\text{softmax}(\\mathbf{W}\\mathbf{x}+\\mathbf{b}) = \\text{softmax}\\left(\\begin{bmatrix}\\mathbf{w}_1 \\mathbf{x}+b_1\\\\ \\vdots  \\\\ \\mathbf{w}_K \\mathbf{x} +b_K\\end{bmatrix}\\right)=\\begin{bmatrix} \\frac{\\exp(\\mathbf{w}_1 \\mathbf{x}+b_1)}{\\sum_{j=1}^K \\exp(\\mathbf{w}_j \\mathbf{x}+b_j)} \\\\ \\vdots  \\\\ \\frac{\\exp(\\mathbf{w}_K\\mathbf{x}+b_K)}{\\sum_{j=1}^K \\exp(\\mathbf{w}_j \\mathbf{x}+b_j)} \\end{bmatrix}\\in(0,1)^K$$\n",
        "\n",
        "   where weight matrix $\\mathbf{W}\\in \\mathbb{R}^{K\\times D}$ and $\\mathbf{w}_j$ is the $j$-th row of $\\mathbf{W}$. Here $\\theta=(\\mathbf{W},\\mathbf{b})$. \n",
        "\n",
        "- **Goal**:  find parameters $\\theta$ that minimize the training loss function.\n",
        "\n",
        "   - The **cross entropy loss** for a given training sample $\\mathbf{x}^{(i)}$ with $\\vec f(\\mathbf{x}^{(i)};\\theta)\\in (0,1)^K$, $1\\le y^{(i)}\\le K$:     \n",
        "   $$\\ell(\\vec f(\\mathbf{x}^{(i)};\\theta), y^{(i)})=-\\log f_{y^{(i)}}(\\mathbf{x}^{(i)};\\theta)\\ge 0 $$\n",
        "   Because $-\\sum_{j}\\mathbf{y}_j^{(i)} \\log f_j(\\mathbf{x}^{(i)};\\theta)=-\\log f_{y^{(i)}}(\\mathbf{x}^{(i)};\\theta)$ and $\\mathbf{y}^{(i)}=\\mathbf{e}_{y^{(i)}}$ is the one-hot vector of $y^{(i)}$.\n",
        "\n",
        "\n",
        "   - For each sample, want to make $f_{y^{(i)}}f(\\mathbf{x}^{(i)};\\theta)$ (i.e., the predicted probability for Class $y^{(i)}$) as large (close to 1) as possible. \n",
        "\n",
        "   - The training loss to be minimized is the cross entropy loss on the full training set:\n",
        "   $$ \\min_{\\theta} L(\\theta; \\mathcal{D})= -\\frac{1}{N}\\sum_{i=1}^N \\log f_{y^{(i)}}(\\mathbf{x}^{(i)};\\theta)$$\n",
        "\n",
        "   - Usually add a penalty term for reducing overfitting:\n",
        "      $$ \\min_{\\theta} L(\\theta; \\mathcal{D})= -\\frac{1}{N}\\sum_{i=1}^N \\log f_{y^{(i)}}(\\mathbf{x}^{(i)};\\theta) +\\lambda ||\\mathbf{W}||_2^2$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        },
        "id": "k6hB4ogeB7IA",
        "outputId": "5837d776-4b70-4a83-9f69-5e3e36da62bc"
      },
      "source": [
        "display(Image(url='https://github.com/yexf308/MAT592/blob/main/image/MNIST_LR.png?raw=true', width=1000))\n",
        "display(Image(url='https://github.com/yexf308/MAT592/blob/main/image/comparison_LR.png?raw=true', width=500))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/MNIST_LR.png?raw=true\" width=\"1000\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/comparison_LR.png?raw=true\" width=\"500\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4v-WsX2RKgX"
      },
      "source": [
        "# Artificial neural network: Introduction\n",
        "## 1. Two-layer neural network\n",
        "The following is the structure of two layer neural network. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "YILXeqrZUPRP",
        "outputId": "370bc867-23aa-4811-afdc-e8eab56e4413"
      },
      "source": [
        "display(Image(url='https://github.com/yexf308/MAT592/blob/main/image/hidden.png?raw=true', width=400))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/hidden.png?raw=true\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s4oAYxYlcdW"
      },
      "source": [
        "### (a) Structure of neural network\n",
        "\n",
        "- The leftmost is the **inputs layer** with features $x_1, x_2, \\dots, x_D$ from the input feature $\\mathbf{x}\\in \\mathbb{R}^D$. Here we have 5 neurons. \n",
        "\n",
        "- The grey layers is **hidden neurons layers**, which process inputs from preceding layer and output results for next layer. Here we have 4 hidden neurons.\n",
        "$$\\mathbf{a}(\\mathbf{x})=g(\\underbrace{\\mathbf{W}^{(1)}\\mathbf{x}+\\mathbf{b}^{(1)}}_{= \\mathbf{z}(\\mathbf{x})})\\in \\mathbb{R}^h $$\n",
        "  Here $\\mathbf{W}^{(1)}\\in \\mathbb{R}^{h\\times D}, \\mathbf{b}^{(1)}\\in \\mathbb{R}^h$, the weight matrix and bias vector for **(dense) linear layer**; $g: \\mathbb{R}\\rightarrow \\mathbb{R}, a_j = g(z_j)$: element-wise **(non-linear) activation function**. \n",
        "\n",
        "\n",
        "- The rightmost layer is **softmax output layer**. Here we have 3 neurons. \n",
        "$$\\vec f(\\mathbf{x}; \\theta) = \\text{softmax}(\\mathbf{W}^{(2)}\\mathbf{a}(\\mathbf{x})+\\mathbf{b}^{(2)}) = \\text{softmax}(\\mathbf{W}^{(2)}g(\\mathbf{W}^{(1)}\\mathbf{x}+\\mathbf{b}^{(1)})+\\mathbf{b}^{(2)})\\in(0,1)^K $$\n",
        "  Here $\\mathbf{W}^{(2)}\\in \\mathbb{R}^{K\\times h}, \\mathbf{b}^{(2)}\\in \\mathbb{R}^K$, the weight matrix and bias vector for the second **linear layer**.\n",
        "\n",
        "- $\\theta =\\{\\mathbf{W}^{(1)}, \\mathbf{b}^{(1)},\\mathbf{W}^{(2)}, \\mathbf{b}^{(2)} \\}$  are model parameters from linear layers. \n",
        "\n",
        "- The total cross-entropy loss is  \n",
        " $$ L(\\theta; \\mathcal{D})= -\\frac{1}{N}\\sum_{i=1}^N \\log f_{y^{(i)}}(\\mathbf{x}^{(i)};\\theta)$$\n",
        "\n",
        "- **Artificial neural network** is a composition of functions. Each neuron is a function. It accepts inputs from previous\n",
        "layer and outputs for next layer. Neuron is activated (or fire) if it has high value. \n",
        "\n",
        "\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG3WFlpP9b9E"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "One advantage\n",
        "of the softmax output layer is the interpretation of its outputs as probability. \n",
        "However, one issue to use softmax function in the output layer is it is not element-wise.\n",
        " That is why some prefer to use sigmoid function with logistic loss.   \n",
        "\n",
        "The rightmost layer sometimes also uses the **sigmoid function** $\\sigma$ and **logistic loss**, \n",
        "$$\\vec f(\\mathbf{x}; \\theta) = \\sigma(\\mathbf{W}^{(2)}\\mathbf{a}(\\mathbf{x})+\\mathbf{b}^{(2)}) = \\sigma(\\mathbf{W}^{(2)}g(\\mathbf{W}^{(1)}\\mathbf{x}+\\mathbf{b}^{(1)})+\\mathbf{b}^{(2)})$$\n",
        "\n",
        "Then the logistic loss function is \n",
        "$$ L(\\theta; \\mathcal{D}) = -\\frac{1}{N}\\sum_{i=1}^N \\left(\\log f_{y^{(i)}}(\\mathbf{x}^{(i)}; \\theta) + \\sum_{j\\ne y^{(i)}}\\log (1-f_{j}(\\mathbf{x}^{(i)}; \\theta))\\right)$$\n",
        "With the help of one-hot vector $\\mathbf{y}^{(i)}=\\mathbf{e}_{y^{(i)}}$, for example, the class 1 is represented by $[1, 0,\\dots, 0]^\\top$.\n",
        "$$ L(\\theta; \\mathcal{D})= -\\frac{1}{N}\\sum_{i=1}^N \\sum_{j=1}^K \\left(\\mathbf{y}^{(i)}_j \\log f_{j}(\\mathbf{x}^{(i)}; \\theta) +(1-\\mathbf{y}^{(i)}_j)\\log (1-f_{j}(\\mathbf{x}^{(i)}; \\theta))\\right) $$\n",
        "\n",
        "**Why not using square loss here?**\n",
        "In fact, we can. \n",
        "$$L(\\theta; \\mathcal{D}) = \\frac{1}{2N}\\sum_{i=1}^N \\|\\vec f(\\mathbf{x}^{(i)};\\theta)-\\mathbf{y}^{(i)}\\|^2 $$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pwf-Wa2ryCz5"
      },
      "source": [
        "### (b) Dense linear layer\n",
        "Dense layer defines a linear mapping, specified by the weights $\\mathbf{W}$ and the bias $\\mathbf{b}$.\n",
        "\n",
        "- Both input and output of the layer are **vectors**; if input is not (such as images), then flatten it. \n",
        "\n",
        "- Each output $z_j$ is a weighted average of inputs $\\mathbf{x}$ plus a bias term. \n",
        "   $$ z_j = \\mathbf{w}_j \\mathbf{x}+b_j$$\n",
        "   where $ \\mathbf{w}_j$ is the $j$-th row of $\\mathbf{W}$.\n",
        "\n",
        "- Full connectivity between the input and output components, justifying the name _dense_; hence, also known as fully-connected layer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zot_lflw2ZRt"
      },
      "source": [
        "### (c) Connection to biological neuron\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "LrxzdqOj2hZL",
        "outputId": "ab7253a9-419d-4585-d3e0-f48879b2fe18"
      },
      "source": [
        "display(Image(url='https://github.com/yexf308/MAT592/blob/main/image/neuron.png?raw=true', width=800))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/neuron.png?raw=true\" width=\"800\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ5posqD2t1g"
      },
      "source": [
        "- Neurons (or nerve cells) are special cells that process and transmit information by electrical signaling (in brain and also spinal cord). \n",
        "\n",
        "- A neuron connects to other neurons to form a network. Each neuron cell communicates to between 1000 and 10,000 other neurons. Some group of neurons fire cause some other neuron fire.\n",
        "\n",
        "- A neuron has three components: \n",
        "   - **dendrites**: “input wires”, receive inputs from other neurons. \n",
        "\n",
        "   - **cell body**: computational unit.\n",
        "\n",
        "   - **axon**: “output wire”, sends signal to other neurons.    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB4UZYMc54HO"
      },
      "source": [
        "### (d) Element-wise activation function\n",
        "\n",
        "Element-wise activation: for any $\\mathbf{z}\\in \\mathbb{R}^h$, \n",
        "  $$ g(\\mathbf{z})_j = g(z_j),\\qquad j=1,\\dots, h$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "y7p0N5oIPy72",
        "outputId": "da45ca43-3d10-4f0d-f345-0b08c26b2652"
      },
      "source": [
        "display(Image(url='https://github.com/yexf308/MAT592/blob/main/image/activation.png?raw=true', width=900))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/activation.png?raw=true\" width=\"900\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb-uITTSRREe"
      },
      "source": [
        "- $\\text{sigm}(x)$ is the sigmoid function. \n",
        "\n",
        "- $\\tanh(x)$ is the hyperbolic tangent function. \n",
        "\n",
        "- $\\text{relu}(x)$ is the rectified linear unit function.\n",
        "\n",
        "**Blue**: activation function. **Green**: derivative of activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fejuzbmTWGd"
      },
      "source": [
        "## 2. Deep Neural Network (DNN)\n",
        " The neural network is called a **deep neural network** if it has more than two layers (otherwise, it is said to be shallow). It means we have multiple layers of hidden neurons: linear layers with each followed by element-wise non-linear activation. Parameters are collection of weights and biases from linear layers. Like two-layer NN, we could use cross entropy loss in the softmax output layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "3TrB8RxxUH2W",
        "outputId": "2544ed85-371d-4e2d-f43b-a5ab30151675"
      },
      "source": [
        "display(Image(url='https://github.com/yexf308/MAT592/blob/main/image/DNN.png?raw=true', width=900))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/DNN.png?raw=true\" width=\"900\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLBmOxWfWtyr"
      },
      "source": [
        "### Why DNN can work?\n",
        "\n",
        "In classification tasks, ML aims to learn an underlaying decision function $\\hat{f}$ that maps any data to its correct label. DNN models can approximate a decision function $\\hat{f}$ with arbitrary\n",
        "precision, given sufficient depth. This is guaranteed by the following **universal approximation theorem**. \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Theorem**: For any Lebesgue-integrable function $\\hat f: \\mathbb{R}^D\\rightarrow \\mathbb{R}$ and any $\\epsilon>0$, there exists a\n",
        "deep dense (fully-connected) ReLU network $\\mathcal{A}$ with width $\\le D+4$, , such that the function $F_{\\mathcal{A}}$ represented by the network satisfies \n",
        "$$ \\int_{\\mathbb{R}^D} |\\hat{f}(\\mathbf{x})- F_{\\mathcal{A}}(\\mathbf{x})|d\\mathbf{x}<\\epsilon $$\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "On computational side, learning (near-)optimal DNNs can be difficult due to highly non-convex optimization"
      ]
    }
  ]
}