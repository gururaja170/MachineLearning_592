{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "22_Decision_tree2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNRFR4gkN2T25fAULYr1rG8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yexf308/MAT592/blob/main/22_Decision_tree2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NWM8_rbaOpb"
      },
      "source": [
        "\n",
        "Part of the notebook is based on https://medium.com/@joachimvalente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLaeaYNlWxvx"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from IPython.display import Image\n",
        "import numpy.linalg as LA"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hpVySaUXQBp"
      },
      "source": [
        "# Decision Tree\n",
        "Advantage of Decision Tree\n",
        "\n",
        "- Simple to understand and interpret. \n",
        "\n",
        "- Able to handle both numerical and categorical data. (today's lecture)\n",
        "\n",
        "- Requires little data preparation. \n",
        "\n",
        "- Uses a white box model. \n",
        "\n",
        "- Possible to validate a model using statistical tests.\n",
        "\n",
        "- Mirrors human decision making more closely than other approaches.\n",
        "\n",
        "\n",
        "Original ID3 algorithm accepts **categorical/discrete features** only and\n",
        "bears the overfitting issue. Also ID3 only works with classification. \n",
        "\n",
        "We will introduce **C4.5** algorithm, which is the successor to ID3. C4.5 improves from ID3 mainly in three-fold:\n",
        "\n",
        "- Accept continuous features by introducing thresholding node. \n",
        "\n",
        "- Solve overfitting problem by pruning strategy.\n",
        "\n",
        "- Can handle incomplete data points with missing feature values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22Y8n01FdzB3"
      },
      "source": [
        "## C4.5 Algorithm\n",
        "- One issue to use **information gain (IG)** is it favors to the feature has a large number of distinct values. \n",
        "\n",
        "- For example, suppose that one is building a decision tree for some data describing the customers of a business. \n",
        "\n",
        "  Information gain is often used to decide which of the features are the most relevant, so they can be tested near the root of the tree. One of the input features might be the customer's **credit card number**. This features has a high IG, because it uniquely identifies each customer, but we do not want to include it in the decision tree: deciding how to treat a customer based on their credit card number is unlikely to generalize to customers we haven't seen before. It is typical **overfitting**.\n",
        "\n",
        "- C4.5 Algorithm chooses the feature with **highest information gain ratio** from among the features whose **information gain is average or higher**. This biases the decision tree against considering features with a large number of distinct values, while not giving an unfair advantage to attributes with very low information value. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYlPpfcGTeCc"
      },
      "source": [
        "### Information gain ratio \n",
        "- Recall **Information gain (IG)** of set $\\mathcal{S}$ on some feature $F$ is\n",
        "$$ \\text{IG}(\\mathcal{S}; F)= H(\\mathcal{S})- H(\\mathcal{S}|F).$$\n",
        "\n",
        "- Conditional entropy of $\\mathcal{S}$ on $F$ is computed by\n",
        "$$ H(\\mathcal{S}|F) = \\sum_{f\\in F} P(F=f)H(\\mathcal{S}_f)=\\sum_{f\\in F} \\frac{|\\mathcal{S}_f|}{|\\mathcal{S}|}H(\\mathcal{S}_f)$$\n",
        "\n",
        "- Then entropy on feature $F$ is \n",
        "$$H(F) =-\\sum_{f\\in F}\\frac{|\\mathcal{S}_f|}{|\\mathcal{S}|} \\log_2\\left(\\frac{|\\mathcal{S}_f|}{|\\mathcal{S}|}\\right) $$\n",
        "which sometimes also called intrinsic value.\n",
        "\n",
        "- **Information gain ratio** is the ratio between the information gain and the intrinsic value: \n",
        "$$ \\text{IGR}(\\mathcal{S}; F) = \\text{IG}(\\mathcal{S}; F) /H(F)$$\n",
        "\n",
        "- The strategy is to choose the highest $\\text{IGR}$ among the features whose $\\text{IG}$ is average or higher. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwCvY53zXPoB"
      },
      "source": [
        "## Classification and Regression Trees (CART)\n",
        "- It is a non-parametric decision tree learning technique that produces either classification or **regression** trees, depending on whether the target variable is categorical or numeric, respectively.\n",
        "\n",
        "- A couple issues in C4.5 is it is quite slow when the data is large since it  needs lots of logarithm calculations and it has non-binary tree. \n",
        "\n",
        "- CART uses **Gini impurity** as the metric and only uses binary tree. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsFNnKRBc6GB"
      },
      "source": [
        "### Gini impurity \n",
        "\n",
        "- Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. \n",
        "\n",
        "- Given a set of $\\mathcal{S}$ with $k$ target classes, the gini impurity is \n",
        "$$\\text{gini}(\\mathcal{S})=\\sum_{i=1}^k p_i (1-p_i) = 1-\\sum_{i=1}^k p_i^2. $$\n",
        "\n",
        "- Like entropy,  a node is pure (gini = 0) if all its samples belong to the same target class, while a node with many samples from many different target classes will have a Gini closer to 1. \n",
        "\n",
        "- The gini impurity is the first order Taylor expansion of entropy.\n",
        "Since $\\log_2(x)=\\frac{\\ln(x)}{\\ln(2)} \\approx \\frac{(-1+x+O(x))}{\\ln(2)}$\n",
        "\n",
        "$$ H(\\mathcal{S})=-\\sum_{i=1}^k p_i\\log_2(p_i) \\approx -\\frac{1}{\\ln(2)}\\sum_{i=1}^k p_i(-1+p_i)=\\frac{\\text{gini}(\\mathcal{S})}{\\ln(2)} $$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX0oJW1WXbZn"
      },
      "source": [
        "### Optimal splitting \n",
        "If the feature only has two discrete categories, then it is obvious to split into two branches. \n",
        "\n",
        "But if the feature has more than two discrete categories or even the feature is continuous variable. \n",
        "\n",
        "- The optimal splitting is that **the node is split so that the Gini impurity of the children (more specifically the average of the Gini of the children weighted by their size) is minimized.**\n",
        "\n",
        "- Mathematically, the set $\\mathcal{S}$ is splitted into two sets $\\mathcal{S}_1$ and $\\mathcal{S}_2$ based on some features and threshold $A$, i.e, $\\{\\mathcal{S_1}: \\mathbf{x}\\in A\\}$ and $\\{\\mathcal{S_2}: \\mathbf{x}\\notin A\\}$. \n",
        "we need to find the optimal feature and threshold $A$ such that \n",
        "$$\\text{gini}(\\mathcal{S},A) =  \\frac{|\\mathcal{S}_1|}{|\\mathcal{S}|}\\text{gini}(\\mathcal{S}_1) + \\frac{|\\mathcal{S}_2|}{|\\mathcal{S}|}\\text{gini}(\\mathcal{S}_2) $$\n",
        "is minimized. \n",
        "\n",
        "- **Algorithm**: \n",
        "   -  Iterate through the sorted feature values as possible thresholds. \n",
        "   - Keep track of the number of samples per class on the left and on the right.\n",
        "   -  Increment/decrement them by 1 after each threshold. From them we can easily compute Gini in constant time.. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00vuIcgSc6Xw"
      },
      "source": [
        "### For example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "_p-y7QqDc1cu",
        "outputId": "fef2a7a4-dae6-4ea6-fbcb-d5d6cba7cb14"
      },
      "source": [
        "X = '1.5,2.3,1.7,2.7,2.9'.split(',')\n",
        "y = '1,2,1,2,3'.split(',')\n",
        "dataset ={'X':X,'y':y}\n",
        "df = pd.DataFrame(dataset,columns=['X','y'])\n",
        "df"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.7</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.7</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.9</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     X  y\n",
              "0  1.5  1\n",
              "1  2.3  2\n",
              "2  1.7  1\n",
              "3  2.7  2\n",
              "4  2.9  3"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSRs3ssEdbAk",
        "outputId": "fb71ce2a-e650-4079-e7e6-c650c235e2ed"
      },
      "source": [
        "def gini(df):\n",
        "    target = df.keys()[-1] \n",
        "    classes = df[target].unique()\n",
        "    m= len(df[target])\n",
        "    num_parent = [sum(df[target] == c) for c in classes]\n",
        "    best_gini = 1.0 - sum((n / m) ** 2 for n in num_parent)\n",
        "    return best_gini\n",
        "\n",
        "\n",
        "gini(df)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6399999999999999"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqfM2u_Teo9V"
      },
      "source": [
        "def best_split(df):\n",
        "  \"\"\"Find the best split for a node.\n",
        "    \"Best\" means that the average impurity of the two children, weighted by their\n",
        "     population, is the smallest possible. Additionally it must be less than the\n",
        "     impurity of the current node.\n",
        "     To find the best split, we loop through all the features, and consider all the\n",
        "     midpoints between adjacent training samples as possible thresholds. We compute\n",
        "     the Gini impurity of the split generated by that particular feature/threshold\n",
        "     pair, and return the pair with smallest impurity.\n",
        "        Returns:\n",
        "            best_idx: Index of the feature for best split, or None if no split is found.\n",
        "            best_thr: Threshold to use for the split, or None if no split is found.\n",
        "  \"\"\"\n",
        "  features   = df.keys()[:-1]\n",
        "  target     = df.keys()[-1] \n",
        "  target_values   = df[target].unique()\n",
        "  n_target_values = len(target_values)\n",
        "  m          = len(df[target])\n",
        "  num_parent = [sum(df[target] == c) for c in target_values]\n",
        "\n",
        "  best_gini = 1.0 - sum((n / m) ** 2 for n in num_parent)\n",
        "  best_feature, best_thr = None, None\n",
        "  # Loop through all features.\n",
        "\n",
        "  for feature in features:\n",
        "    # Sort data along selected feature.\n",
        "    thresholds, classes = zip(*sorted(zip(df[feature],df[target])))\n",
        "    num_left = [0]*n_target_values\n",
        "    num_right = num_parent.copy()\n",
        "    for i in range(1, m):  \n",
        "      c = classes[i - 1]\n",
        "      for idx, val in enumerate(target_values):\n",
        "        if c==val:\n",
        "          num_left[idx]  += 1\n",
        "          num_right[idx] -= 1\n",
        "\n",
        "       \n",
        "      gini_left = 1.0 - sum((num_left[idx] / i) ** 2 for idx in range(n_target_values) )\n",
        "      gini_right = 1.0 - sum((num_right[idx] / (m-i) ) ** 2 for idx in range(n_target_values) )\n",
        "      # The Gini impurity of a split is the weighted average of the Gini\n",
        "      # impurity of the children.\n",
        "      gini = (i * gini_left + (m - i) * gini_right) / m\n",
        "      \n",
        "      if thresholds[i] == thresholds[i - 1]:\n",
        "        continue\n",
        "\n",
        "      if gini < best_gini:\n",
        "        best_gini = gini\n",
        "        best_feature = feature\n",
        "        best_thr = (float(thresholds[i]) + float(thresholds[i - 1])) / 2 \n",
        "\n",
        "  return best_feature, best_thr         \n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fewd7mPdPFSE",
        "outputId": "60590cbe-c4ca-4e1a-eea1-adb3befff3dc"
      },
      "source": [
        "best_feature, best_thr = best_split(df)\n",
        "print(best_feature)\n",
        "print(best_thr)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X\n",
            "2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8Q3JL9X5BRd"
      },
      "source": [
        "# Let's code this up on some test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "-cDb1whY3h1l",
        "outputId": "91f5a803-d219-48f6-ed9e-49356ab77f15"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
        "                     columns= iris['feature_names'] + ['target'])\n",
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <th>petal length (cm)</th>\n",
              "      <th>petal width (cm)</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal length (cm)  sepal width (cm)  ...  petal width (cm)  target\n",
              "0                5.1               3.5  ...               0.2     0.0\n",
              "1                4.9               3.0  ...               0.2     0.0\n",
              "2                4.7               3.2  ...               0.2     0.0\n",
              "3                4.6               3.1  ...               0.2     0.0\n",
              "4                5.0               3.6  ...               0.2     0.0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJwkQQVlBp0v"
      },
      "source": [
        "def get_subtable(df, node, best_thr):\n",
        "  return df[df[node] < best_thr].reset_index(drop=True), df[df[node] > best_thr].reset_index(drop=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EE2nMIB70fN"
      },
      "source": [
        "def buildTree(df,tree=None): \n",
        "  target = df.keys()[-1]  \n",
        "  features = df.keys()[:-1]\n",
        "\n",
        "  best_feature, best_thr = best_split(df)\n",
        "  if best_feature is not None:\n",
        "    node                 = best_feature\n",
        "\n",
        "    #Create an empty dictionary to create tree    \n",
        "    if tree is None:                    \n",
        "      tree={}\n",
        "      tree[node] = {}\n",
        "\n",
        "    df_left, df_right = get_subtable(df, node, best_thr)\n",
        "    clValue_left, counts_left  = np.unique(df_left[target], return_counts=True)\n",
        "    clValue_right,counts_right = np.unique(df_right[target],return_counts=True)\n",
        "    left_variable  = '<' + str(best_thr)\n",
        "    right_variable = '>' + str(best_thr)\n",
        "\n",
        "    if len(counts_left)==1:\n",
        "      tree[node][left_variable] = clValue_left[0] \n",
        "    else:\n",
        "      tree[node][left_variable] = buildTree(df_left)  \n",
        "\n",
        "    if len(counts_right)==1:\n",
        "      tree[node][right_variable] = clValue_right[0] \n",
        "    else:\n",
        "      tree[node][right_variable] = buildTree(df_right) \n",
        "\n",
        "  return tree\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b8QWyVvHbY2",
        "outputId": "dc2fc80c-5a6e-4332-e997-769a6a2de4aa"
      },
      "source": [
        "import pprint\n",
        "t=buildTree(df)\n",
        "pprint.pprint(t)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'petal length (cm)': {'<2.45': 0.0,\n",
            "                       '>2.45': {'petal width (cm)': {'<1.75': {'petal length (cm)': {'<4.95': {'petal width (cm)': {'<1.65': 1.0,\n",
            "                                                                                                                     '>1.65': 2.0}},\n",
            "                                                                                      '>4.95': {'petal width (cm)': {'<1.55': 2.0,\n",
            "                                                                                                                     '>1.55': {'sepal length (cm)': {'<6.95': 1.0,\n",
            "                                                                                                                                                     '>6.95': 2.0}}}}}},\n",
            "                                                      '>1.75': {'petal length (cm)': {'<4.85': {'sepal length (cm)': {'<5.95': 1.0,\n",
            "                                                                                                                      '>5.95': 2.0}},\n",
            "                                                                                      '>4.85': 2.0}}}}}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr0itxyoKke6"
      },
      "source": [
        "### We still need to write the prediction function. "
      ]
    }
  ]
}